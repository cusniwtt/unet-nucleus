{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESMNOyeoPOTe"
      },
      "source": [
        "# Nuclie Semantic Segmentation - UNet using Tensorflow 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e61ef2d8-f315-4f7f-b07e-1de0f4e8441a",
        "_uuid": "1677fddbb95f7545b6540e9201f3339a0fdbfc5d",
        "id": "kyyE6xeHPOTl"
      },
      "source": [
        "# Intro\n",
        "- Dataset used is from Kaggle's Data Science Bowl 2018 - Nuclei Segmentation\n",
        "- The architecture used is [U-Net](https://arxiv.org/abs/1505.04597), which is very common for image segmentation problems such as this.\n",
        "- This notebook is inspired from the great kernel [Keras U-net starter - LB 0.277](https://www.kaggle.com/keegil/keras-u-net-starter-lb-0-277) by Kjetil Åmdal-Sævik."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FtGpSvl2eq2",
        "outputId": "760da709-dffa-47f2-c0e1-8820522400d8"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFTLqTBlaXca",
        "outputId": "9e0842ce-9d22-4c47-d76a-3f017f6d748b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.9.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "#try:\n",
        "#  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "#  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "#except ValueError:\n",
        "#  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "#tf.config.experimental_connect_to_cluster(tpu)\n",
        "#tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "#tpu_strategy = tf.distribute.TPUStrategy(tpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "_cell_guid": "c332549b-8d23-4bb5-8497-e7a8eb8b21d2",
        "_uuid": "5c38504af3a84bee68c66d3cde74443c58df422f",
        "id": "GhEVrsrdPOTl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import sys\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
        "from skimage.transform import resize\n",
        "from skimage.morphology import label\n",
        "\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, add, multiply\n",
        "from keras.layers import Dropout, Lambda\n",
        "from keras.layers import Conv2D, Conv2DTranspose, BatchNormalization\n",
        "from keras.layers import MaxPooling2D, UpSampling2D\n",
        "from keras.layers import concatenate, Activation\n",
        "from keras import backend as K\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLV0HVwgZF0v"
      },
      "source": [
        "### Prediction Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NuAJQi_CZF0v"
      },
      "outputs": [],
      "source": [
        "def prediction(model_path, X_test, img_size = (256, 256), thres = 0.5, verbose = 1):\n",
        "    # Predict on train, val and test\n",
        "    model = load_model(model_path)\n",
        "    preds_test = model.predict(X_test, verbose=verbose)\n",
        "\n",
        "    # Threshold predictions\n",
        "    preds_test_t = (preds_test > thres).astype(np.uint8)\n",
        "\n",
        "    # Create list of upsampled test masks\n",
        "    preds_test_upsampled = []\n",
        "    for i in range(len(preds_test_t)):\n",
        "        preserve = np.squeeze(preds_test_t[i])\n",
        "        res = resize(preserve, img_size, mode='constant', preserve_range=True)\n",
        "        preds_test_upsampled.append(res.astype(np.uint8))\n",
        "    return preds_test_upsampled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWTeGqDRZF0v"
      },
      "source": [
        "### Watershed for split nucleus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5rxjSI6AZF0v"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import scipy.ndimage as ndi\n",
        "from skimage.segmentation import watershed, find_boundaries\n",
        "from skimage.feature import corner_peaks\n",
        "from skimage.morphology import binary_dilation, disk\n",
        "from skimage.color import label2rgb\n",
        "from skimage.measure import regionprops_table\n",
        "\n",
        "def make_boundary_image(L, A, thickness=1, color=(255,255,85), rescale_hist=True):\n",
        "    if A.ndim == 2:\n",
        "        A = np.stack((A,A,A), axis=2)\n",
        "    if rescale_hist:\n",
        "        A = np.interp(A, (np.amin(A), np.amax(A)), (0,255)).astype(np.uint8)\n",
        "    else:\n",
        "        A = A.astype(np.uint8)\n",
        "\n",
        "    mask = find_boundaries(L)\n",
        "    mask = binary_dilation(mask, footprint=disk(thickness))\n",
        "\n",
        "    R = A[:,:,0].copy()\n",
        "    G = A[:,:,1].copy()\n",
        "    B = A[:,:,2].copy()\n",
        "\n",
        "    R[mask] = color[0]\n",
        "    G[mask] = color[1]\n",
        "    B[mask] = color[2]\n",
        "\n",
        "    return np.stack((R,G,B), axis=2)\n",
        "\n",
        "def labelvis(A, L, bg_color='b'):\n",
        "    bg_color_code = {\n",
        "        'b': (0.1,0.1,0.5),\n",
        "        'g': (0.1,0.5,0.1),\n",
        "    }  \n",
        "    A = label2rgb(L, A, bg_label=0, bg_color=bg_color_code[bg_color], alpha=0.3, image_alpha=1)\n",
        "    A = np.interp(A, (0,1), (0,255)).astype(np.uint8)\n",
        "    A = make_boundary_image(L, A)\n",
        "    return A\n",
        "\n",
        "# Create predict boundary image with label\n",
        "# Input: image, label\n",
        "# Output: rgb_watershed, boundary, mask, dataframe\n",
        "def create_boundary(img, label):\n",
        "    #Threshold image to binary using OTSU. ALl thresholded pixels will be set to 255\n",
        "    ret1, thresh = cv2.threshold(label, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
        "    \n",
        "    # Borrow from imageMKS\n",
        "    # Step 9: distance transform\n",
        "    distance = ndi.distance_transform_edt(thresh)\n",
        "    \n",
        "    # Step 10: mark the maxima in the distance transform and assign labels\n",
        "    peak_markers = corner_peaks(distance, min_distance=5, indices=False)\n",
        "    peak_markers = ndi.label(peak_markers)[0]\n",
        "    \n",
        "    # Step 11: separate touching nuclei using the watershed markers\n",
        "    markers = watershed(label, peak_markers, mask=label)\n",
        "    \n",
        "    # Step 13: reassigning labels, so that they are continuously numbered\n",
        "    old_labels = np.unique(markers)\n",
        "    for i in range(len(old_labels)):\n",
        "        markers[markers == old_labels[i]] = i\n",
        "    \n",
        "    #Let us color boundaries in yellow. \n",
        "    img[markers == -1] = [0,255,255]  \n",
        "    \n",
        "    img_rgb = label2rgb(markers, bg_label=0)\n",
        "    boundary = labelvis(img, markers)\n",
        "    \n",
        "    # regionprops function in skimage measure module calculates useful parameters for each object.\n",
        "    params = ['label', 'area', 'bbox', 'equivalent_diameter', 'mean_intensity', 'solidity']\n",
        "    props = regionprops_table(markers, intensity_image=X_test[0], properties=params)\n",
        "    df = pd.DataFrame(props)\n",
        "    \n",
        "    return img_rgb, boundary, markers, df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YdGAF_woPOTm",
        "outputId": "2ee7539f-ef3b-416b-c2b6-0ed75bc0653a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/cusniwtt/Documents/GitHub/unet-nucleus'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set some parameters\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256\n",
        "IMG_CHANNELS = 3\n",
        "TRAIN_PATH = 'dataset/stage1_train/'\n",
        "TEST_PATH = 'dataset/stage1_test/'\n",
        "\n",
        "dir_path = ''\n",
        "\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='skimage')\n",
        "seed = 42\n",
        "random.seed = seed\n",
        "np.random.seed = seed\n",
        "\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "_cell_guid": "ffa0caf0-2d1b-40f2-865b-8e6db88526b6",
        "_uuid": "3fb9d6530fbbd0e22e41fc4fd9fd9fc0bff027ac",
        "id": "QeJvhGJRPOTo"
      },
      "outputs": [],
      "source": [
        "# Get train and test IDs\n",
        "train_ids = next(os.walk(TRAIN_PATH))[1]\n",
        "test_ids = next(os.walk(TEST_PATH))[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "59c4a25d-645f-4b74-9c53-145ac78cc481",
        "_uuid": "875af74f980236825de3a650825b46e25632422c",
        "id": "MWz-i4iXPOTo"
      },
      "source": [
        "# Get the data\n",
        "- Downsample both the training and test images to reduce computations\n",
        "- Retain record of the original sizes of the test images to upsample predicted masks and create correct run-length encodings "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "_cell_guid": "ca0cc34b-c26f-41ee-88d7-975aebdb634e",
        "_uuid": "9e389ba8bdb5b6fc03b231b6a6c84a8bde634053",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVXaT7BJPOTo",
        "outputId": "68a5d4a9-7158-42f6-e1c7-509ac6fc0c51",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting and resizing train images and masks ... \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:07<00:00, 13.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting and resizing test images ... \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 12/12 [00:00<00:00, 12.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Get and resize train images and masks\n",
        "X_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
        "Y_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=bool)\n",
        "print('Getting and resizing train images and masks ... ')\n",
        "sys.stdout.flush()\n",
        "for n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n",
        "    \n",
        "    #Read image files iteratively\n",
        "    path = TRAIN_PATH + id_\n",
        "    img = imread(dir_path + path + '/images/' + id_ + '.png')[:,:,:IMG_CHANNELS]\n",
        "    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
        "    \n",
        "    #Append image to numpy array for train dataset\n",
        "    X_train[n] = img\n",
        "    \n",
        "    #Read corresponding mask files iteratively\n",
        "    mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=bool)\n",
        "    \n",
        "    #Looping through masks\n",
        "    for mask_file in next(os.walk(path + '/masks/'))[2]:\n",
        "\n",
        "        # Remove .DS_Store file\n",
        "        if mask_file == '.DS_Store':\n",
        "            continue\n",
        "        \n",
        "        #Read individual masks\n",
        "        mask_ = imread(dir_path + path + '/masks/' + mask_file)\n",
        "        \n",
        "        #Expand individual mask dimensions\n",
        "        mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True), axis=-1)\n",
        "\n",
        "        #Overlay individual masks to create a final mask for corresponding image\n",
        "        try:\n",
        "            mask = np.maximum(mask, mask_)\n",
        "        except:\n",
        "            print(mask_file)\n",
        "    \n",
        "    #Append mask to numpy array for train dataset\n",
        "    Y_train[n] = mask\n",
        "\n",
        "\n",
        "# Get and resize test images\n",
        "X_test = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
        "Y_test = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=bool)\n",
        "sizes_test = []\n",
        "print('Getting and resizing test images ... ')\n",
        "sys.stdout.flush()\n",
        "for n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):\n",
        "    path = TEST_PATH + id_\n",
        "    \n",
        "    #Read images iteratively\n",
        "    img = imread(dir_path + path + '/images/' + id_ + '.png')[:,:,:IMG_CHANNELS]\n",
        "    \n",
        "    #Get test size\n",
        "    sizes_test.append([img.shape[0], img.shape[1]])\n",
        "    \n",
        "    #Resize image to match training data\n",
        "    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
        "    \n",
        "    #Append image to numpy array for test dataset\n",
        "    X_test[n] = img\n",
        "\n",
        "    #Read corresponding mask files iteratively\n",
        "    mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=bool)\n",
        "    \n",
        "    #Looping through masks\n",
        "    for mask_file in next(os.walk(path + '/masks/'))[2]:\n",
        "\n",
        "        # Remove .DS_Store file\n",
        "        if mask_file == '.DS_Store':\n",
        "            continue\n",
        "        \n",
        "        #Read individual masks\n",
        "        mask_ = imread(dir_path + path + '/masks/' + mask_file)\n",
        "        \n",
        "        #Expand individual mask dimensions\n",
        "        mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True), axis=-1)\n",
        "\n",
        "        #Overlay individual masks to create a final mask for corresponding image\n",
        "        try:\n",
        "            mask = np.maximum(mask, mask_)\n",
        "        except:\n",
        "            print(mask_file)\n",
        "    \n",
        "    #Append mask to numpy array for train dataset\n",
        "    Y_test[n] = mask\n",
        "\n",
        "print('Done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c0523b03-1fc5-4505-a1b8-eb35ee617c8a",
        "_uuid": "d4f8327802a1ec6139ce0585953986272ba62ce1",
        "id": "z3eBRkYZPOTp"
      },
      "source": [
        "## Visualize imported data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "_cell_guid": "88829b53-50ce-45d9-9540-77dd7384ad4c",
        "_uuid": "283af26f0860b7069bdfd133c746e5d20971542c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "id": "LTHVzBiqPOTp",
        "outputId": "5a818351-3ed4-4bbf-cdd3-21f93930ea10"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Check if training data looks all right\n",
        "ix = random.randint(0, len(train_ids))\n",
        "imshow(X_train[ix])\n",
        "plt.show()\n",
        "imshow(np.squeeze(Y_train[ix]))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhfN4WTnZF0y",
        "outputId": "0cabdf9b-d3d8-4489-9afe-5c710882e4ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(80, 256, 256, 3)\n",
            "(20, 256, 256, 3)\n",
            "(80, 256, 256, 1)\n",
            "(20, 256, 256, 1)\n"
          ]
        }
      ],
      "source": [
        "# Validation split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(Y_train.shape)\n",
        "print(Y_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axUYmjq1POTq"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcxxv4EnPOTq",
        "outputId": "90a77e38-167e-4c6f-a3f1-cd621d377ece"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Augmentation train images and masks ... \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 80/80 [00:00<00:00, 348.43it/s]\n",
            "100%|██████████| 80/80 [00:00<00:00, 209.65it/s]\n",
            "100%|██████████| 80/80 [00:00<00:00, 119.87it/s]\n",
            "100%|██████████| 80/80 [00:00<00:00, 83.65it/s]\n",
            "100%|██████████| 80/80 [00:01<00:00, 55.71it/s]\n",
            "100%|██████████| 80/80 [00:01<00:00, 49.78it/s]\n",
            "100%|██████████| 80/80 [00:01<00:00, 40.90it/s]\n",
            "100%|██████████| 80/80 [00:02<00:00, 33.58it/s]\n",
            "100%|██████████| 80/80 [00:02<00:00, 34.09it/s]\n",
            "100%|██████████| 80/80 [00:02<00:00, 34.37it/s]\n"
          ]
        }
      ],
      "source": [
        "import albumentations as A\n",
        "import cv2\n",
        "\n",
        "transform = A.Compose([\n",
        "    A.Resize(width=256, height=256),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.Rotate(limit=45, p=0.5),\n",
        "    A.CLAHE(p=0.5),\n",
        "])\n",
        "rounds = 10\n",
        "X_train_aug = np.zeros((0, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
        "Y_train_aug = np.zeros((0, IMG_HEIGHT, IMG_WIDTH, 1), dtype=bool)\n",
        "print('Augmentation train images and masks ... ')\n",
        "for r in range(rounds):\n",
        "    for n, id_ in tqdm(enumerate(X_train), total=X_train.shape[0]):\n",
        "        X_img = X_train[n]\n",
        "        Y_img = Y_train[n]*1\n",
        "        augmented = transform(image=X_img, mask=Y_img.astype(np.uint8))\n",
        "        if not np.any(augmented['mask']):\n",
        "            continue\n",
        "        else:\n",
        "            X_train_aug = np.append(X_train_aug, augmented['image'].reshape(1, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), axis=0)\n",
        "            Y_train_aug = np.append(Y_train_aug, augmented['mask'].astype(bool).reshape(1, IMG_HEIGHT, IMG_WIDTH, 1), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LO0hWVE6POTq",
        "outputId": "0da9bc0f-249b-4e1d-e83b-ad6acc2d2d9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(790, 256, 256, 3)\n",
            "(790, 256, 256, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train_aug.shape)\n",
        "print(Y_train_aug.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "s6qw56JPPOTr",
        "outputId": "b341cdc9-117a-45d1-a742-560db13b1a3d"
      },
      "outputs": [],
      "source": [
        "# Check if training data looks all right\n",
        "X_train = X_train_aug\n",
        "Y_train = Y_train_aug\n",
        "\n",
        "ix = random.sample(range(0, X_train.shape[0]), 16)\n",
        "fig, axs = plt.subplots(4, 8, figsize=(15, 8))\n",
        "row = 0\n",
        "col = 0\n",
        "for i in ix:\n",
        "  axs[row][col].imshow(X_train[i])\n",
        "  axs[row+1][col].imshow(np.squeeze(Y_train[i]))\n",
        "  col = col + 1\n",
        "  if col == 8:\n",
        "    col = 0\n",
        "    row = row + 2\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c3b9f148-1dba-4b6a-981b-6cdbf394fc3c",
        "_uuid": "986488a4c5223576be370e224426a30431911eb2",
        "id": "GkZA6dd9POTr"
      },
      "source": [
        "# Build and train our neural network\n",
        "Next we build our U-Net model, loosely based on [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/pdf/1505.04597.pdf) and very similar to [this repo](https://github.com/jocicmarko/ultrasound-nerve-segmentation) from the Kaggle Ultrasound Nerve Segmentation competition.\n",
        "\n",
        "<img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" alt=\"UNet\" style=\"height: 400px; width:600px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OymkzMDRPOTr"
      },
      "source": [
        "## U-Net\n",
        "from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c1dbc57c-b497-4ccb-b077-2053203ab7ed",
        "_uuid": "0aa97d66c29f45dfac9b0f45fcf74ba0e778ba5d",
        "id": "G9NqwcmpPOTs"
      },
      "outputs": [],
      "source": [
        "# Build U-Net model\n",
        "\n",
        "def down_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1, dropout=0.2):\n",
        "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n",
        "    c = BatchNormalization()(c)\n",
        "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
        "    c = BatchNormalization()(c)\n",
        "    p = MaxPooling2D((2, 2), (2, 2))(c)\n",
        "    p = Dropout(dropout)(p)\n",
        "    return c, p\n",
        "\n",
        "def bottleneck(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1, dropout=0.3):\n",
        "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n",
        "    c = BatchNormalization()(c)\n",
        "    c = Dropout(dropout)(c)\n",
        "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
        "    c = BatchNormalization()(c)\n",
        "    return c\n",
        "\n",
        "def up_block(x, skip, filters, kernel_size=(3, 3), padding=\"same\", strides=1, dropout=0.2):\n",
        "    us = UpSampling2D((2, 2))(x)\n",
        "    concat = concatenate([us, skip])\n",
        "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(concat)\n",
        "    c = BatchNormalization()(c)\n",
        "    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n",
        "    c = BatchNormalization()(c)\n",
        "    c = Dropout(dropout)(c)\n",
        "    return c\n",
        "\n",
        "def unet_model(filters=64, input=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), nn_name=\"unet\"):\n",
        "    inputs = Input(input)\n",
        "    s = Lambda(lambda x: x / 255)(inputs)\n",
        "    down1 = down_block(s, filters, kernel_size=(3, 3), padding=\"same\", strides=1, dropout=0.1)\n",
        "    down2 = down_block(down1[1], filters*2, kernel_size=(3, 3), padding=\"same\", strides=1, dropout=0.1)\n",
        "    down3 = down_block(down2[1], filters*4, kernel_size=(3, 3), padding=\"same\", strides=1, dropout=0.2)\n",
        "    down4 = down_block(down3[1], filters*8, kernel_size=(3, 3), padding=\"same\", strides=1, dropout=0.2)\n",
        "    bottleneck1 = bottleneck(down4[1], filters*16, kernel_size=(3, 3), padding=\"same\", strides=1, dropout=0.3)\n",
        "    up1 = up_block(bottleneck1, down4[0], filters*8, kernel_size=(3, 3), padding=\"same\", strides=1, dropout=0.2)\n",
        "    up2 = up_block(up1, down3[0], filters*4, kernel_size=(3, 3), padding=\"same\", strides=1, dropout=0.2)\n",
        "    up3 = up_block(up2, down2[0], filters*2, kernel_size=(3, 3), padding=\"same\", strides=1, dropout=0.1)\n",
        "    up4 = up_block(up3, down1[0], filters, kernel_size=(3, 3), padding=\"same\", strides=1, dropout=0.1)\n",
        "    outputs = Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(up4)\n",
        "    model = Model(inputs=[inputs], outputs=[outputs], name=nn_name)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EU2TB2B-ZF00",
        "outputId": "6383a571-8d2a-464e-d517-ea4baf2a49ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"unet\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 256, 256, 3)  0           ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 256, 256, 64  1792        ['lambda[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 256, 256, 64  256        ['conv2d[0][0]']                 \n",
            " alization)                     )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 256, 256, 64  36928       ['batch_normalization[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 256, 256, 64  256        ['conv2d_1[0][0]']               \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 128, 128, 64  0           ['batch_normalization_1[0][0]']  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 128, 128, 64  0           ['max_pooling2d[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 128, 128, 12  73856       ['dropout[0][0]']                \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 128, 128, 12  512        ['conv2d_2[0][0]']               \n",
            " rmalization)                   8)                                                                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 128, 128, 12  147584      ['batch_normalization_2[0][0]']  \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 128, 128, 12  512        ['conv2d_3[0][0]']               \n",
            " rmalization)                   8)                                                                \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 128)  0          ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 64, 64, 128)  0           ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 64, 64, 256)  295168      ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 64, 64, 256)  590080      ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_5[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 256)  0          ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 32, 32, 256)  0           ['max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 32, 32, 512)  1180160     ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 32, 32, 512)  2048       ['conv2d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 32, 32, 512)  2359808     ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 32, 32, 512)  2048       ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 512)  0          ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 16, 16, 512)  0           ['max_pooling2d_3[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 16, 16, 1024  4719616     ['dropout_3[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 16, 16, 1024  4096       ['conv2d_8[0][0]']               \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 16, 16, 1024  0           ['batch_normalization_8[0][0]']  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 16, 16, 1024  9438208     ['dropout_4[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 16, 16, 1024  4096       ['conv2d_9[0][0]']               \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " up_sampling2d (UpSampling2D)   (None, 32, 32, 1024  0           ['batch_normalization_9[0][0]']  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 32, 32, 1536  0           ['up_sampling2d[0][0]',          \n",
            "                                )                                 'batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 32, 32, 512)  7078400     ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 32, 32, 512)  2048       ['conv2d_10[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 32, 32, 512)  2359808     ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 32, 32, 512)  2048       ['conv2d_11[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 32, 32, 512)  0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 512)  0          ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 64, 64, 768)  0           ['up_sampling2d_1[0][0]',        \n",
            "                                                                  'batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 64, 64, 256)  1769728     ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_12[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 64, 64, 256)  590080      ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_13[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 64, 64, 256)  0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 25  0          ['dropout_6[0][0]']              \n",
            "                                6)                                                                \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 128, 128, 38  0           ['up_sampling2d_2[0][0]',        \n",
            "                                4)                                'batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 128, 128, 12  442496      ['concatenate_2[0][0]']          \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 128, 128, 12  512        ['conv2d_14[0][0]']              \n",
            " ormalization)                  8)                                                                \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 128, 128, 12  147584      ['batch_normalization_14[0][0]'] \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 128, 128, 12  512        ['conv2d_15[0][0]']              \n",
            " ormalization)                  8)                                                                \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 128, 128, 12  0           ['batch_normalization_15[0][0]'] \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 12  0          ['dropout_7[0][0]']              \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 256, 256, 19  0           ['up_sampling2d_3[0][0]',        \n",
            "                                2)                                'batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 256, 256, 64  110656      ['concatenate_3[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 256, 256, 64  256        ['conv2d_16[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 256, 256, 64  36928       ['batch_normalization_16[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 256, 256, 64  256        ['conv2d_17[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 256, 256, 64  0           ['batch_normalization_17[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 256, 256, 1)  65          ['dropout_8[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 31,402,497\n",
            "Trainable params: 31,390,721\n",
            "Non-trainable params: 11,776\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_repr = unet_model(filters=64, input=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "model_repr.summary()\n",
        "del(model_repr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydi5AeZJPOTs"
      },
      "source": [
        "### Optimizer Loss Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpcA-nThPOTs"
      },
      "outputs": [],
      "source": [
        "# Optimizer\n",
        "from keras import optimizers\n",
        "sgd = optimizers.SGD(learning_rate=0.002, decay=0.00003, momentum=0.9)\n",
        "adam = optimizers.Adam(learning_rate=0.002, decay=0.00003)\n",
        "\n",
        "# Loss function\n",
        "from keras.losses import BinaryCrossentropy\n",
        "bce = BinaryCrossentropy()\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return -dice_coef(y_true, y_pred)\n",
        "\n",
        "def jacard_coef_loss(y_true, y_pred):\n",
        "    return -jacard_coef(y_true, y_pred)\n",
        "\n",
        "# Metrics\n",
        "from keras.metrics import Recall, Precision\n",
        "recall = Recall()\n",
        "precision = Precision()\n",
        "\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2.0 * intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1.0)\n",
        "\n",
        "def jacard_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
        "\n",
        "# Fit model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
        "earlystopper = EarlyStopping(patience=15, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(factor=0.1, patience=4, min_lr=0.00001, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifBJ8x6LPOTt"
      },
      "source": [
        "### Adam, BCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rl_1B_qSPOTt",
        "outputId": "123c0ded-1bce-43bb-a0f0-09b53e0c0e7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.3382 - recall: 0.7519 - precision: 0.4984\n",
            "Epoch 1: val_loss improved from inf to 0.37536, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 92s 726ms/step - loss: 0.3382 - recall: 0.7519 - precision: 0.4984 - val_loss: 0.3754 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - lr: 0.0020\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1288 - recall: 0.6820 - precision: 0.7074\n",
            "Epoch 2: val_loss improved from 0.37536 to 0.28309, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 661ms/step - loss: 0.1288 - recall: 0.6820 - precision: 0.7074 - val_loss: 0.2831 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - lr: 0.0020\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1060 - recall: 0.6537 - precision: 0.7259\n",
            "Epoch 3: val_loss improved from 0.28309 to 0.22445, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 663ms/step - loss: 0.1060 - recall: 0.6537 - precision: 0.7259 - val_loss: 0.2244 - val_recall: 0.0010 - val_precision: 1.0000 - lr: 0.0020\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0933 - recall: 0.6714 - precision: 0.7439\n",
            "Epoch 4: val_loss improved from 0.22445 to 0.13483, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 663ms/step - loss: 0.0933 - recall: 0.6714 - precision: 0.7439 - val_loss: 0.1348 - val_recall: 0.1668 - val_precision: 0.9015 - lr: 0.0020\n",
            "Epoch 5/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0911 - recall: 0.6745 - precision: 0.7442\n",
            "Epoch 5: val_loss improved from 0.13483 to 0.08656, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 660ms/step - loss: 0.0911 - recall: 0.6745 - precision: 0.7442 - val_loss: 0.0866 - val_recall: 0.4484 - val_precision: 0.8804 - lr: 0.0020\n",
            "Epoch 6/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0896 - recall: 0.6838 - precision: 0.7462\n",
            "Epoch 6: val_loss did not improve from 0.08656\n",
            "100/100 [==============================] - 64s 645ms/step - loss: 0.0896 - recall: 0.6838 - precision: 0.7462 - val_loss: 0.1072 - val_recall: 0.3487 - val_precision: 0.9043 - lr: 0.0020\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0886 - recall: 0.6816 - precision: 0.7476\n",
            "Epoch 7: val_loss improved from 0.08656 to 0.07045, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 660ms/step - loss: 0.0886 - recall: 0.6816 - precision: 0.7476 - val_loss: 0.0705 - val_recall: 0.7379 - val_precision: 0.7489 - lr: 0.0020\n",
            "Epoch 8/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0872 - recall: 0.6907 - precision: 0.7488\n",
            "Epoch 8: val_loss did not improve from 0.07045\n",
            "100/100 [==============================] - 64s 645ms/step - loss: 0.0872 - recall: 0.6907 - precision: 0.7488 - val_loss: 0.0732 - val_recall: 0.6207 - val_precision: 0.8129 - lr: 0.0020\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0866 - recall: 0.6969 - precision: 0.7480\n",
            "Epoch 9: val_loss did not improve from 0.07045\n",
            "100/100 [==============================] - 65s 648ms/step - loss: 0.0866 - recall: 0.6969 - precision: 0.7480 - val_loss: 0.0757 - val_recall: 0.6216 - val_precision: 0.8120 - lr: 0.0020\n",
            "Epoch 10/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0855 - recall: 0.6939 - precision: 0.7540\n",
            "Epoch 10: val_loss did not improve from 0.07045\n",
            "100/100 [==============================] - 64s 645ms/step - loss: 0.0855 - recall: 0.6939 - precision: 0.7540 - val_loss: 0.0720 - val_recall: 0.7083 - val_precision: 0.7625 - lr: 0.0020\n",
            "Epoch 11/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0963 - recall: 0.6519 - precision: 0.7295\n",
            "Epoch 11: val_loss did not improve from 0.07045\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "100/100 [==============================] - 65s 647ms/step - loss: 0.0963 - recall: 0.6519 - precision: 0.7295 - val_loss: 0.0995 - val_recall: 0.3535 - val_precision: 0.8439 - lr: 0.0020\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0884 - recall: 0.6826 - precision: 0.7489\n",
            "Epoch 12: val_loss did not improve from 0.07045\n",
            "100/100 [==============================] - 65s 646ms/step - loss: 0.0884 - recall: 0.6826 - precision: 0.7489 - val_loss: 0.0708 - val_recall: 0.6822 - val_precision: 0.7768 - lr: 2.0000e-04\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0868 - recall: 0.6924 - precision: 0.7494\n",
            "Epoch 13: val_loss improved from 0.07045 to 0.06894, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 661ms/step - loss: 0.0868 - recall: 0.6924 - precision: 0.7494 - val_loss: 0.0689 - val_recall: 0.7029 - val_precision: 0.7782 - lr: 2.0000e-04\n",
            "Epoch 14/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0864 - recall: 0.7000 - precision: 0.7476\n",
            "Epoch 14: val_loss did not improve from 0.06894\n",
            "100/100 [==============================] - 65s 649ms/step - loss: 0.0864 - recall: 0.7000 - precision: 0.7476 - val_loss: 0.0691 - val_recall: 0.6902 - val_precision: 0.7881 - lr: 2.0000e-04\n",
            "Epoch 15/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0857 - recall: 0.6969 - precision: 0.7528\n",
            "Epoch 15: val_loss did not improve from 0.06894\n",
            "100/100 [==============================] - 65s 645ms/step - loss: 0.0857 - recall: 0.6969 - precision: 0.7528 - val_loss: 0.0690 - val_recall: 0.7503 - val_precision: 0.7521 - lr: 2.0000e-04\n",
            "Epoch 16/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0872 - recall: 0.6942 - precision: 0.7512\n",
            "Epoch 16: val_loss improved from 0.06894 to 0.06840, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 658ms/step - loss: 0.0872 - recall: 0.6942 - precision: 0.7512 - val_loss: 0.0684 - val_recall: 0.7284 - val_precision: 0.7680 - lr: 2.0000e-04\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0853 - recall: 0.6993 - precision: 0.7523\n",
            "Epoch 17: val_loss improved from 0.06840 to 0.06772, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 660ms/step - loss: 0.0853 - recall: 0.6993 - precision: 0.7523 - val_loss: 0.0677 - val_recall: 0.7267 - val_precision: 0.7718 - lr: 2.0000e-04\n",
            "Epoch 18/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0846 - recall: 0.7035 - precision: 0.7541\n",
            "Epoch 18: val_loss did not improve from 0.06772\n",
            "100/100 [==============================] - 65s 648ms/step - loss: 0.0846 - recall: 0.7035 - precision: 0.7541 - val_loss: 0.0678 - val_recall: 0.7139 - val_precision: 0.7835 - lr: 2.0000e-04\n",
            "Epoch 19/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0841 - recall: 0.7049 - precision: 0.7558\n",
            "Epoch 19: val_loss did not improve from 0.06772\n",
            "100/100 [==============================] - 65s 645ms/step - loss: 0.0841 - recall: 0.7049 - precision: 0.7558 - val_loss: 0.0686 - val_recall: 0.7017 - val_precision: 0.7825 - lr: 2.0000e-04\n",
            "Epoch 20/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0837 - recall: 0.7049 - precision: 0.7570\n",
            "Epoch 20: val_loss did not improve from 0.06772\n",
            "100/100 [==============================] - 65s 647ms/step - loss: 0.0837 - recall: 0.7049 - precision: 0.7570 - val_loss: 0.0695 - val_recall: 0.7907 - val_precision: 0.7292 - lr: 2.0000e-04\n",
            "Epoch 21/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0835 - recall: 0.7055 - precision: 0.7570\n",
            "Epoch 21: val_loss improved from 0.06772 to 0.06732, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 659ms/step - loss: 0.0835 - recall: 0.7055 - precision: 0.7570 - val_loss: 0.0673 - val_recall: 0.7269 - val_precision: 0.7783 - lr: 2.0000e-04\n",
            "Epoch 22/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0831 - recall: 0.7131 - precision: 0.7553\n",
            "Epoch 22: val_loss did not improve from 0.06732\n",
            "100/100 [==============================] - 65s 647ms/step - loss: 0.0831 - recall: 0.7131 - precision: 0.7553 - val_loss: 0.0676 - val_recall: 0.6810 - val_precision: 0.8076 - lr: 2.0000e-04\n",
            "Epoch 23/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0833 - recall: 0.7044 - precision: 0.7587\n",
            "Epoch 23: val_loss did not improve from 0.06732\n",
            "100/100 [==============================] - 65s 646ms/step - loss: 0.0833 - recall: 0.7044 - precision: 0.7587 - val_loss: 0.0687 - val_recall: 0.7554 - val_precision: 0.7556 - lr: 2.0000e-04\n",
            "Epoch 24/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0822 - recall: 0.7104 - precision: 0.7611\n",
            "Epoch 24: val_loss did not improve from 0.06732\n",
            "100/100 [==============================] - 65s 647ms/step - loss: 0.0822 - recall: 0.7104 - precision: 0.7611 - val_loss: 0.0682 - val_recall: 0.7508 - val_precision: 0.7576 - lr: 2.0000e-04\n",
            "Epoch 25/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0822 - recall: 0.7101 - precision: 0.7612\n",
            "Epoch 25: val_loss did not improve from 0.06732\n",
            "\n",
            "Epoch 25: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
            "100/100 [==============================] - 65s 646ms/step - loss: 0.0822 - recall: 0.7101 - precision: 0.7612 - val_loss: 0.0691 - val_recall: 0.8071 - val_precision: 0.7257 - lr: 2.0000e-04\n",
            "Epoch 26/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0815 - recall: 0.7266 - precision: 0.7574\n",
            "Epoch 26: val_loss improved from 0.06732 to 0.06638, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 658ms/step - loss: 0.0815 - recall: 0.7266 - precision: 0.7574 - val_loss: 0.0664 - val_recall: 0.7323 - val_precision: 0.7789 - lr: 2.0000e-05\n",
            "Epoch 27/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0813 - recall: 0.7135 - precision: 0.7645\n",
            "Epoch 27: val_loss improved from 0.06638 to 0.06620, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 662ms/step - loss: 0.0813 - recall: 0.7135 - precision: 0.7645 - val_loss: 0.0662 - val_recall: 0.7515 - val_precision: 0.7693 - lr: 2.0000e-05\n",
            "Epoch 28/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0809 - recall: 0.7154 - precision: 0.7662\n",
            "Epoch 28: val_loss improved from 0.06620 to 0.06614, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 663ms/step - loss: 0.0809 - recall: 0.7154 - precision: 0.7662 - val_loss: 0.0661 - val_recall: 0.7488 - val_precision: 0.7709 - lr: 2.0000e-05\n",
            "Epoch 29/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0808 - recall: 0.7178 - precision: 0.7645\n",
            "Epoch 29: val_loss did not improve from 0.06614\n",
            "100/100 [==============================] - 65s 647ms/step - loss: 0.0808 - recall: 0.7178 - precision: 0.7645 - val_loss: 0.0667 - val_recall: 0.7203 - val_precision: 0.7868 - lr: 2.0000e-05\n",
            "Epoch 30/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0810 - recall: 0.7197 - precision: 0.7634\n",
            "Epoch 30: val_loss did not improve from 0.06614\n",
            "100/100 [==============================] - 65s 646ms/step - loss: 0.0810 - recall: 0.7197 - precision: 0.7634 - val_loss: 0.0664 - val_recall: 0.7298 - val_precision: 0.7807 - lr: 2.0000e-05\n",
            "Epoch 31/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0808 - recall: 0.7147 - precision: 0.7666\n",
            "Epoch 31: val_loss did not improve from 0.06614\n",
            "\n",
            "Epoch 31: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "100/100 [==============================] - 64s 645ms/step - loss: 0.0808 - recall: 0.7147 - precision: 0.7666 - val_loss: 0.0661 - val_recall: 0.7410 - val_precision: 0.7745 - lr: 2.0000e-05\n",
            "Epoch 32/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0808 - recall: 0.7206 - precision: 0.7631\n",
            "Epoch 32: val_loss did not improve from 0.06614\n",
            "100/100 [==============================] - 65s 648ms/step - loss: 0.0808 - recall: 0.7206 - precision: 0.7631 - val_loss: 0.0663 - val_recall: 0.7428 - val_precision: 0.7734 - lr: 1.0000e-05\n",
            "Epoch 33/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0806 - recall: 0.7185 - precision: 0.7653\n",
            "Epoch 33: val_loss did not improve from 0.06614\n",
            "100/100 [==============================] - 65s 647ms/step - loss: 0.0806 - recall: 0.7185 - precision: 0.7653 - val_loss: 0.0662 - val_recall: 0.7391 - val_precision: 0.7746 - lr: 1.0000e-05\n",
            "Epoch 34/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0806 - recall: 0.7164 - precision: 0.7669\n",
            "Epoch 34: val_loss improved from 0.06614 to 0.06598, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 661ms/step - loss: 0.0806 - recall: 0.7164 - precision: 0.7669 - val_loss: 0.0660 - val_recall: 0.7385 - val_precision: 0.7756 - lr: 1.0000e-05\n",
            "Epoch 35/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0803 - recall: 0.7194 - precision: 0.7668\n",
            "Epoch 35: val_loss improved from 0.06598 to 0.06596, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 660ms/step - loss: 0.0803 - recall: 0.7194 - precision: 0.7668 - val_loss: 0.0660 - val_recall: 0.7474 - val_precision: 0.7705 - lr: 1.0000e-05\n",
            "Epoch 36/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0804 - recall: 0.7222 - precision: 0.7643\n",
            "Epoch 36: val_loss improved from 0.06596 to 0.06588, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 659ms/step - loss: 0.0804 - recall: 0.7222 - precision: 0.7643 - val_loss: 0.0659 - val_recall: 0.7479 - val_precision: 0.7709 - lr: 1.0000e-05\n",
            "Epoch 37/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0806 - recall: 0.7228 - precision: 0.7642\n",
            "Epoch 37: val_loss did not improve from 0.06588\n",
            "100/100 [==============================] - 65s 648ms/step - loss: 0.0806 - recall: 0.7228 - precision: 0.7642 - val_loss: 0.0661 - val_recall: 0.7459 - val_precision: 0.7720 - lr: 1.0000e-05\n",
            "Epoch 38/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0804 - recall: 0.7223 - precision: 0.7643\n",
            "Epoch 38: val_loss did not improve from 0.06588\n",
            "100/100 [==============================] - 65s 646ms/step - loss: 0.0804 - recall: 0.7223 - precision: 0.7643 - val_loss: 0.0660 - val_recall: 0.7414 - val_precision: 0.7753 - lr: 1.0000e-05\n",
            "Epoch 39/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0806 - recall: 0.7259 - precision: 0.7625\n",
            "Epoch 39: val_loss improved from 0.06588 to 0.06587, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 661ms/step - loss: 0.0806 - recall: 0.7259 - precision: 0.7625 - val_loss: 0.0659 - val_recall: 0.7363 - val_precision: 0.7788 - lr: 1.0000e-05\n",
            "Epoch 40/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0805 - recall: 0.7137 - precision: 0.7687\n",
            "Epoch 40: val_loss improved from 0.06587 to 0.06585, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 659ms/step - loss: 0.0805 - recall: 0.7137 - precision: 0.7687 - val_loss: 0.0658 - val_recall: 0.7406 - val_precision: 0.7754 - lr: 1.0000e-05\n",
            "Epoch 41/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0802 - recall: 0.7211 - precision: 0.7662\n",
            "Epoch 41: val_loss improved from 0.06585 to 0.06570, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 659ms/step - loss: 0.0802 - recall: 0.7211 - precision: 0.7662 - val_loss: 0.0657 - val_recall: 0.7459 - val_precision: 0.7720 - lr: 1.0000e-05\n",
            "Epoch 42/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0805 - recall: 0.7209 - precision: 0.7652\n",
            "Epoch 42: val_loss did not improve from 0.06570\n",
            "100/100 [==============================] - 64s 645ms/step - loss: 0.0805 - recall: 0.7209 - precision: 0.7652 - val_loss: 0.0658 - val_recall: 0.7377 - val_precision: 0.7766 - lr: 1.0000e-05\n",
            "Epoch 43/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0801 - recall: 0.7204 - precision: 0.7678\n",
            "Epoch 43: val_loss improved from 0.06570 to 0.06560, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 659ms/step - loss: 0.0801 - recall: 0.7204 - precision: 0.7678 - val_loss: 0.0656 - val_recall: 0.7507 - val_precision: 0.7698 - lr: 1.0000e-05\n",
            "Epoch 44/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0803 - recall: 0.7231 - precision: 0.7651\n",
            "Epoch 44: val_loss did not improve from 0.06560\n",
            "100/100 [==============================] - 65s 648ms/step - loss: 0.0803 - recall: 0.7231 - precision: 0.7651 - val_loss: 0.0657 - val_recall: 0.7397 - val_precision: 0.7763 - lr: 1.0000e-05\n",
            "Epoch 45/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0802 - recall: 0.7199 - precision: 0.7679\n",
            "Epoch 45: val_loss did not improve from 0.06560\n",
            "100/100 [==============================] - 65s 647ms/step - loss: 0.0802 - recall: 0.7199 - precision: 0.7679 - val_loss: 0.0659 - val_recall: 0.7423 - val_precision: 0.7745 - lr: 1.0000e-05\n",
            "Epoch 46/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0800 - recall: 0.7249 - precision: 0.7654\n",
            "Epoch 46: val_loss did not improve from 0.06560\n",
            "100/100 [==============================] - 65s 645ms/step - loss: 0.0800 - recall: 0.7249 - precision: 0.7654 - val_loss: 0.0656 - val_recall: 0.7339 - val_precision: 0.7796 - lr: 1.0000e-05\n",
            "Epoch 47/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0801 - recall: 0.7235 - precision: 0.7660\n",
            "Epoch 47: val_loss did not improve from 0.06560\n",
            "100/100 [==============================] - 65s 646ms/step - loss: 0.0801 - recall: 0.7235 - precision: 0.7660 - val_loss: 0.0657 - val_recall: 0.7303 - val_precision: 0.7821 - lr: 1.0000e-05\n",
            "Epoch 48/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0803 - recall: 0.7176 - precision: 0.7691\n",
            "Epoch 48: val_loss did not improve from 0.06560\n",
            "100/100 [==============================] - 65s 647ms/step - loss: 0.0803 - recall: 0.7176 - precision: 0.7691 - val_loss: 0.0657 - val_recall: 0.7665 - val_precision: 0.7589 - lr: 1.0000e-05\n",
            "Epoch 49/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0800 - recall: 0.7262 - precision: 0.7650\n",
            "Epoch 49: val_loss improved from 0.06560 to 0.06547, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 660ms/step - loss: 0.0800 - recall: 0.7262 - precision: 0.7650 - val_loss: 0.0655 - val_recall: 0.7420 - val_precision: 0.7748 - lr: 1.0000e-05\n",
            "Epoch 50/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0798 - recall: 0.7263 - precision: 0.7665\n",
            "Epoch 50: val_loss did not improve from 0.06547\n",
            "100/100 [==============================] - 65s 647ms/step - loss: 0.0798 - recall: 0.7263 - precision: 0.7665 - val_loss: 0.0659 - val_recall: 0.7503 - val_precision: 0.7698 - lr: 1.0000e-05\n",
            "Epoch 51/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0798 - recall: 0.7214 - precision: 0.7676\n",
            "Epoch 51: val_loss did not improve from 0.06547\n",
            "100/100 [==============================] - 65s 648ms/step - loss: 0.0798 - recall: 0.7214 - precision: 0.7676 - val_loss: 0.0659 - val_recall: 0.7389 - val_precision: 0.7773 - lr: 1.0000e-05\n",
            "Epoch 52/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0798 - recall: 0.7261 - precision: 0.7663\n",
            "Epoch 52: val_loss did not improve from 0.06547\n",
            "100/100 [==============================] - 65s 647ms/step - loss: 0.0798 - recall: 0.7261 - precision: 0.7663 - val_loss: 0.0657 - val_recall: 0.7463 - val_precision: 0.7728 - lr: 1.0000e-05\n",
            "Epoch 53/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0799 - recall: 0.7243 - precision: 0.7665\n",
            "Epoch 53: val_loss did not improve from 0.06547\n",
            "100/100 [==============================] - 65s 645ms/step - loss: 0.0799 - recall: 0.7243 - precision: 0.7665 - val_loss: 0.0656 - val_recall: 0.7370 - val_precision: 0.7799 - lr: 1.0000e-05\n",
            "Epoch 54/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0797 - recall: 0.7228 - precision: 0.7685\n",
            "Epoch 54: val_loss did not improve from 0.06547\n",
            "100/100 [==============================] - 65s 649ms/step - loss: 0.0797 - recall: 0.7228 - precision: 0.7685 - val_loss: 0.0657 - val_recall: 0.7517 - val_precision: 0.7694 - lr: 1.0000e-05\n",
            "Epoch 55/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0794 - recall: 0.7291 - precision: 0.7660\n",
            "Epoch 55: val_loss improved from 0.06547 to 0.06545, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 664ms/step - loss: 0.0794 - recall: 0.7291 - precision: 0.7660 - val_loss: 0.0654 - val_recall: 0.7402 - val_precision: 0.7773 - lr: 1.0000e-05\n",
            "Epoch 56/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0796 - recall: 0.7193 - precision: 0.7710\n",
            "Epoch 56: val_loss did not improve from 0.06545\n",
            "100/100 [==============================] - 65s 648ms/step - loss: 0.0796 - recall: 0.7193 - precision: 0.7710 - val_loss: 0.0655 - val_recall: 0.7624 - val_precision: 0.7641 - lr: 1.0000e-05\n",
            "Epoch 57/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0798 - recall: 0.7261 - precision: 0.7662\n",
            "Epoch 57: val_loss did not improve from 0.06545\n",
            "100/100 [==============================] - 65s 647ms/step - loss: 0.0798 - recall: 0.7261 - precision: 0.7662 - val_loss: 0.0657 - val_recall: 0.7564 - val_precision: 0.7676 - lr: 1.0000e-05\n",
            "Epoch 58/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0796 - recall: 0.7254 - precision: 0.7670\n",
            "Epoch 58: val_loss improved from 0.06545 to 0.06539, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 661ms/step - loss: 0.0796 - recall: 0.7254 - precision: 0.7670 - val_loss: 0.0654 - val_recall: 0.7455 - val_precision: 0.7751 - lr: 1.0000e-05\n",
            "Epoch 59/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0795 - recall: 0.7237 - precision: 0.7690\n",
            "Epoch 59: val_loss did not improve from 0.06539\n",
            "100/100 [==============================] - 65s 645ms/step - loss: 0.0795 - recall: 0.7237 - precision: 0.7690 - val_loss: 0.0655 - val_recall: 0.7519 - val_precision: 0.7707 - lr: 1.0000e-05\n",
            "Epoch 60/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0795 - recall: 0.7286 - precision: 0.7666\n",
            "Epoch 60: val_loss improved from 0.06539 to 0.06533, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 661ms/step - loss: 0.0795 - recall: 0.7286 - precision: 0.7666 - val_loss: 0.0653 - val_recall: 0.7486 - val_precision: 0.7729 - lr: 1.0000e-05\n",
            "Epoch 61/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0795 - recall: 0.7218 - precision: 0.7707\n",
            "Epoch 61: val_loss did not improve from 0.06533\n",
            "100/100 [==============================] - 65s 647ms/step - loss: 0.0795 - recall: 0.7218 - precision: 0.7707 - val_loss: 0.0654 - val_recall: 0.7611 - val_precision: 0.7650 - lr: 1.0000e-05\n",
            "Epoch 62/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0792 - recall: 0.7261 - precision: 0.7698\n",
            "Epoch 62: val_loss did not improve from 0.06533\n",
            "100/100 [==============================] - 65s 648ms/step - loss: 0.0792 - recall: 0.7261 - precision: 0.7698 - val_loss: 0.0655 - val_recall: 0.7497 - val_precision: 0.7710 - lr: 1.0000e-05\n",
            "Epoch 63/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0793 - recall: 0.7266 - precision: 0.7693\n",
            "Epoch 63: val_loss did not improve from 0.06533\n",
            "100/100 [==============================] - 65s 647ms/step - loss: 0.0793 - recall: 0.7266 - precision: 0.7693 - val_loss: 0.0654 - val_recall: 0.7568 - val_precision: 0.7682 - lr: 1.0000e-05\n",
            "Epoch 64/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0794 - recall: 0.7267 - precision: 0.7679\n",
            "Epoch 64: val_loss did not improve from 0.06533\n",
            "100/100 [==============================] - 65s 647ms/step - loss: 0.0794 - recall: 0.7267 - precision: 0.7679 - val_loss: 0.0654 - val_recall: 0.7396 - val_precision: 0.7782 - lr: 1.0000e-05\n",
            "Epoch 65/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0795 - recall: 0.7217 - precision: 0.7700\n",
            "Epoch 65: val_loss improved from 0.06533 to 0.06529, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 663ms/step - loss: 0.0795 - recall: 0.7217 - precision: 0.7700 - val_loss: 0.0653 - val_recall: 0.7524 - val_precision: 0.7703 - lr: 1.0000e-05\n",
            "Epoch 66/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0792 - recall: 0.7285 - precision: 0.7686\n",
            "Epoch 66: val_loss improved from 0.06529 to 0.06520, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 661ms/step - loss: 0.0792 - recall: 0.7285 - precision: 0.7686 - val_loss: 0.0652 - val_recall: 0.7551 - val_precision: 0.7694 - lr: 1.0000e-05\n",
            "Epoch 67/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0790 - recall: 0.7268 - precision: 0.7705\n",
            "Epoch 67: val_loss improved from 0.06520 to 0.06493, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 661ms/step - loss: 0.0790 - recall: 0.7268 - precision: 0.7705 - val_loss: 0.0649 - val_recall: 0.7516 - val_precision: 0.7718 - lr: 1.0000e-05\n",
            "Epoch 68/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0788 - recall: 0.7271 - precision: 0.7705\n",
            "Epoch 68: val_loss did not improve from 0.06493\n",
            "100/100 [==============================] - 65s 649ms/step - loss: 0.0788 - recall: 0.7271 - precision: 0.7705 - val_loss: 0.0650 - val_recall: 0.7559 - val_precision: 0.7693 - lr: 1.0000e-05\n",
            "Epoch 69/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0789 - recall: 0.7258 - precision: 0.7718\n",
            "Epoch 69: val_loss did not improve from 0.06493\n",
            "100/100 [==============================] - 65s 647ms/step - loss: 0.0789 - recall: 0.7258 - precision: 0.7718 - val_loss: 0.0651 - val_recall: 0.7604 - val_precision: 0.7671 - lr: 1.0000e-05\n",
            "Epoch 70/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0792 - recall: 0.7278 - precision: 0.7686\n",
            "Epoch 70: val_loss did not improve from 0.06493\n",
            "100/100 [==============================] - 65s 649ms/step - loss: 0.0792 - recall: 0.7278 - precision: 0.7686 - val_loss: 0.0652 - val_recall: 0.7418 - val_precision: 0.7787 - lr: 1.0000e-05\n",
            "Epoch 71/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0789 - recall: 0.7237 - precision: 0.7720\n",
            "Epoch 71: val_loss did not improve from 0.06493\n",
            "100/100 [==============================] - 65s 649ms/step - loss: 0.0789 - recall: 0.7237 - precision: 0.7720 - val_loss: 0.0650 - val_recall: 0.7456 - val_precision: 0.7763 - lr: 1.0000e-05\n",
            "Epoch 72/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0788 - recall: 0.7314 - precision: 0.7692\n",
            "Epoch 72: val_loss did not improve from 0.06493\n",
            "100/100 [==============================] - 65s 648ms/step - loss: 0.0788 - recall: 0.7314 - precision: 0.7692 - val_loss: 0.0650 - val_recall: 0.7500 - val_precision: 0.7734 - lr: 1.0000e-05\n",
            "Epoch 73/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0789 - recall: 0.7229 - precision: 0.7727\n",
            "Epoch 73: val_loss did not improve from 0.06493\n",
            "100/100 [==============================] - 65s 646ms/step - loss: 0.0789 - recall: 0.7229 - precision: 0.7727 - val_loss: 0.0650 - val_recall: 0.7640 - val_precision: 0.7651 - lr: 1.0000e-05\n",
            "Epoch 74/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0788 - recall: 0.7303 - precision: 0.7696\n",
            "Epoch 74: val_loss did not improve from 0.06493\n",
            "100/100 [==============================] - 65s 646ms/step - loss: 0.0788 - recall: 0.7303 - precision: 0.7696 - val_loss: 0.0650 - val_recall: 0.7585 - val_precision: 0.7675 - lr: 1.0000e-05\n",
            "Epoch 75/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0785 - recall: 0.7316 - precision: 0.7701\n",
            "Epoch 75: val_loss improved from 0.06493 to 0.06485, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 662ms/step - loss: 0.0785 - recall: 0.7316 - precision: 0.7701 - val_loss: 0.0648 - val_recall: 0.7449 - val_precision: 0.7772 - lr: 1.0000e-05\n",
            "Epoch 76/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0787 - recall: 0.7283 - precision: 0.7711\n",
            "Epoch 76: val_loss did not improve from 0.06485\n",
            "100/100 [==============================] - 65s 647ms/step - loss: 0.0787 - recall: 0.7283 - precision: 0.7711 - val_loss: 0.0649 - val_recall: 0.7453 - val_precision: 0.7766 - lr: 1.0000e-05\n",
            "Epoch 77/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0786 - recall: 0.7292 - precision: 0.7712\n",
            "Epoch 77: val_loss did not improve from 0.06485\n",
            "100/100 [==============================] - 65s 647ms/step - loss: 0.0786 - recall: 0.7292 - precision: 0.7712 - val_loss: 0.0650 - val_recall: 0.7602 - val_precision: 0.7684 - lr: 1.0000e-05\n",
            "Epoch 78/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0787 - recall: 0.7313 - precision: 0.7701\n",
            "Epoch 78: val_loss improved from 0.06485 to 0.06484, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 659ms/step - loss: 0.0787 - recall: 0.7313 - precision: 0.7701 - val_loss: 0.0648 - val_recall: 0.7534 - val_precision: 0.7727 - lr: 1.0000e-05\n",
            "Epoch 79/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0784 - recall: 0.7256 - precision: 0.7744\n",
            "Epoch 79: val_loss improved from 0.06484 to 0.06459, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 660ms/step - loss: 0.0784 - recall: 0.7256 - precision: 0.7744 - val_loss: 0.0646 - val_recall: 0.7490 - val_precision: 0.7762 - lr: 1.0000e-05\n",
            "Epoch 80/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0787 - recall: 0.7317 - precision: 0.7695\n",
            "Epoch 80: val_loss did not improve from 0.06459\n",
            "100/100 [==============================] - 65s 647ms/step - loss: 0.0787 - recall: 0.7317 - precision: 0.7695 - val_loss: 0.0647 - val_recall: 0.7493 - val_precision: 0.7750 - lr: 1.0000e-05\n",
            "Epoch 81/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0783 - recall: 0.7252 - precision: 0.7751\n",
            "Epoch 81: val_loss improved from 0.06459 to 0.06442, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 659ms/step - loss: 0.0783 - recall: 0.7252 - precision: 0.7751 - val_loss: 0.0644 - val_recall: 0.7431 - val_precision: 0.7800 - lr: 1.0000e-05\n",
            "Epoch 82/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0785 - recall: 0.7268 - precision: 0.7735\n",
            "Epoch 82: val_loss did not improve from 0.06442\n",
            "100/100 [==============================] - 65s 648ms/step - loss: 0.0785 - recall: 0.7268 - precision: 0.7735 - val_loss: 0.0648 - val_recall: 0.7580 - val_precision: 0.7694 - lr: 1.0000e-05\n",
            "Epoch 83/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0784 - recall: 0.7311 - precision: 0.7712\n",
            "Epoch 83: val_loss did not improve from 0.06442\n",
            "100/100 [==============================] - 65s 646ms/step - loss: 0.0784 - recall: 0.7311 - precision: 0.7712 - val_loss: 0.0649 - val_recall: 0.7615 - val_precision: 0.7668 - lr: 1.0000e-05\n",
            "Epoch 84/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0781 - recall: 0.7334 - precision: 0.7721\n",
            "Epoch 84: val_loss did not improve from 0.06442\n",
            "100/100 [==============================] - 65s 646ms/step - loss: 0.0781 - recall: 0.7334 - precision: 0.7721 - val_loss: 0.0652 - val_recall: 0.7735 - val_precision: 0.7596 - lr: 1.0000e-05\n",
            "Epoch 85/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0782 - recall: 0.7291 - precision: 0.7729\n",
            "Epoch 85: val_loss did not improve from 0.06442\n",
            "100/100 [==============================] - 65s 647ms/step - loss: 0.0782 - recall: 0.7291 - precision: 0.7729 - val_loss: 0.0646 - val_recall: 0.7510 - val_precision: 0.7756 - lr: 1.0000e-05\n",
            "Epoch 86/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0786 - recall: 0.7304 - precision: 0.7716\n",
            "Epoch 86: val_loss did not improve from 0.06442\n",
            "100/100 [==============================] - 65s 646ms/step - loss: 0.0786 - recall: 0.7304 - precision: 0.7716 - val_loss: 0.0649 - val_recall: 0.7505 - val_precision: 0.7735 - lr: 1.0000e-05\n",
            "Epoch 87/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0781 - recall: 0.7323 - precision: 0.7726\n",
            "Epoch 87: val_loss did not improve from 0.06442\n",
            "100/100 [==============================] - 65s 647ms/step - loss: 0.0781 - recall: 0.7323 - precision: 0.7726 - val_loss: 0.0647 - val_recall: 0.7624 - val_precision: 0.7670 - lr: 1.0000e-05\n",
            "Epoch 88/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0780 - recall: 0.7309 - precision: 0.7738\n",
            "Epoch 88: val_loss improved from 0.06442 to 0.06429, saving model to weights/unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 66s 662ms/step - loss: 0.0780 - recall: 0.7309 - precision: 0.7738 - val_loss: 0.0643 - val_recall: 0.7518 - val_precision: 0.7763 - lr: 1.0000e-05\n",
            "Epoch 89/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0778 - recall: 0.7301 - precision: 0.7753\n",
            "Epoch 89: val_loss did not improve from 0.06429\n",
            "100/100 [==============================] - 65s 647ms/step - loss: 0.0778 - recall: 0.7301 - precision: 0.7753 - val_loss: 0.0649 - val_recall: 0.7723 - val_precision: 0.7618 - lr: 1.0000e-05\n",
            "Epoch 90/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0777 - recall: 0.7342 - precision: 0.7723\n",
            "Epoch 90: val_loss did not improve from 0.06429\n",
            "100/100 [==============================] - 65s 648ms/step - loss: 0.0777 - recall: 0.7342 - precision: 0.7723 - val_loss: 0.0646 - val_recall: 0.7496 - val_precision: 0.7764 - lr: 1.0000e-05\n",
            "Epoch 91/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0779 - recall: 0.7313 - precision: 0.7740\n",
            "Epoch 91: val_loss did not improve from 0.06429\n",
            "100/100 [==============================] - 65s 648ms/step - loss: 0.0779 - recall: 0.7313 - precision: 0.7740 - val_loss: 0.0643 - val_recall: 0.7379 - val_precision: 0.7853 - lr: 1.0000e-05\n",
            "Epoch 92/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0777 - recall: 0.7303 - precision: 0.7753\n",
            "Epoch 92: val_loss did not improve from 0.06429\n",
            "100/100 [==============================] - 64s 645ms/step - loss: 0.0777 - recall: 0.7303 - precision: 0.7753 - val_loss: 0.0646 - val_recall: 0.7660 - val_precision: 0.7671 - lr: 1.0000e-05\n",
            "Epoch 93/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0778 - recall: 0.7351 - precision: 0.7729\n",
            "Epoch 93: val_loss did not improve from 0.06429\n",
            "100/100 [==============================] - 65s 646ms/step - loss: 0.0778 - recall: 0.7351 - precision: 0.7729 - val_loss: 0.0649 - val_recall: 0.7706 - val_precision: 0.7621 - lr: 1.0000e-05\n",
            "Epoch 94/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0777 - recall: 0.7360 - precision: 0.7721\n",
            "Epoch 94: val_loss did not improve from 0.06429\n",
            "100/100 [==============================] - 65s 649ms/step - loss: 0.0777 - recall: 0.7360 - precision: 0.7721 - val_loss: 0.0644 - val_recall: 0.7522 - val_precision: 0.7751 - lr: 1.0000e-05\n",
            "Epoch 95/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0778 - recall: 0.7331 - precision: 0.7736\n",
            "Epoch 95: val_loss did not improve from 0.06429\n",
            "100/100 [==============================] - 65s 648ms/step - loss: 0.0778 - recall: 0.7331 - precision: 0.7736 - val_loss: 0.0646 - val_recall: 0.7673 - val_precision: 0.7652 - lr: 1.0000e-05\n",
            "Epoch 96/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0777 - recall: 0.7341 - precision: 0.7742\n",
            "Epoch 96: val_loss did not improve from 0.06429\n",
            "100/100 [==============================] - 65s 646ms/step - loss: 0.0777 - recall: 0.7341 - precision: 0.7742 - val_loss: 0.0644 - val_recall: 0.7585 - val_precision: 0.7711 - lr: 1.0000e-05\n",
            "Epoch 97/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0779 - recall: 0.7336 - precision: 0.7732\n",
            "Epoch 97: val_loss did not improve from 0.06429\n",
            "100/100 [==============================] - 65s 647ms/step - loss: 0.0779 - recall: 0.7336 - precision: 0.7732 - val_loss: 0.0654 - val_recall: 0.7727 - val_precision: 0.7613 - lr: 1.0000e-05\n",
            "Epoch 98/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0777 - recall: 0.7313 - precision: 0.7753\n",
            "Epoch 98: val_loss did not improve from 0.06429\n",
            "100/100 [==============================] - 65s 648ms/step - loss: 0.0777 - recall: 0.7313 - precision: 0.7753 - val_loss: 0.0649 - val_recall: 0.7591 - val_precision: 0.7701 - lr: 1.0000e-05\n",
            "Epoch 99/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0775 - recall: 0.7380 - precision: 0.7720\n",
            "Epoch 99: val_loss did not improve from 0.06429\n",
            "100/100 [==============================] - 65s 647ms/step - loss: 0.0775 - recall: 0.7380 - precision: 0.7720 - val_loss: 0.0648 - val_recall: 0.7529 - val_precision: 0.7729 - lr: 1.0000e-05\n",
            "Epoch 100/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0774 - recall: 0.7313 - precision: 0.7770\n",
            "Epoch 100: val_loss did not improve from 0.06429\n",
            "100/100 [==============================] - 65s 649ms/step - loss: 0.0774 - recall: 0.7313 - precision: 0.7770 - val_loss: 0.0648 - val_recall: 0.7606 - val_precision: 0.7684 - lr: 1.0000e-05\n"
          ]
        }
      ],
      "source": [
        "# Adam, BCE, batch_size=8\n",
        "logs_p = 'logs/unet_64_adam_bce_8.csv'\n",
        "weights_p = 'weights/unet_64_adam_bce_8.h5'\n",
        "\n",
        "checkpointer = ModelCheckpoint(weights_p, verbose=1, save_best_only=True)\n",
        "callbacks_f = [earlystopper, checkpointer, reduce_lr, CSVLogger(logs_p, append=True, separator=';')]\n",
        "metrics_f = [recall, precision,]\n",
        "#batch_s = 8 * tpu_strategy.num_replicas_in_sync # with TPU\n",
        "batch_s = 8\n",
        "epochs_s = 100\n",
        "\n",
        "#with tpu_strategy.scope():  # with TPU\n",
        "#  model = unet_model(input=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),)\n",
        "#  adam = optimizers.Adam(learning_rate=0.002, decay=0.00003)\n",
        "#  bce = BinaryCrossentropy()\n",
        "#  model.compile(optimizer=adam, loss=bce, metrics=[Recall(), Precision()])\n",
        "model = unet_model(input=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),)\n",
        "model.compile(optimizer=adam, loss=bce, metrics=metrics_f)\n",
        "\n",
        "results = model.fit(x=X_train, y=Y_train, validation_data=(X_val, Y_val), batch_size=batch_s, \n",
        "                    epochs=epochs_s, callbacks=callbacks_f, verbose=1,)\n",
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBmYx-QVcs63"
      },
      "source": [
        "### SGD, BCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anOiZsocPOTt",
        "outputId": "a4c55864-7eb2-4f9e-d4f5-c3c5331ec38b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.4377 - recall: 0.8303 - precision: 0.3453\n",
            "Epoch 1: val_loss improved from inf to 0.26633, saving model to weights/unet_64_sgd_bce_8.h5\n",
            "100/100 [==============================] - 70s 672ms/step - loss: 0.4377 - recall: 0.8303 - precision: 0.3453 - val_loss: 0.2663 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - lr: 0.0020\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.2078 - recall: 0.7159 - precision: 0.6564\n",
            "Epoch 2: val_loss improved from 0.26633 to 0.23037, saving model to weights/unet_64_sgd_bce_8.h5\n",
            "100/100 [==============================] - 66s 661ms/step - loss: 0.2078 - recall: 0.7159 - precision: 0.6564 - val_loss: 0.2304 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00 - lr: 0.0020\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1561 - recall: 0.6774 - precision: 0.6936\n",
            "Epoch 3: val_loss improved from 0.23037 to 0.19464, saving model to weights/unet_64_sgd_bce_8.h5\n",
            "100/100 [==============================] - 66s 660ms/step - loss: 0.1561 - recall: 0.6774 - precision: 0.6936 - val_loss: 0.1946 - val_recall: 6.6538e-04 - val_precision: 1.0000 - lr: 0.0020\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1365 - recall: 0.6566 - precision: 0.7086\n",
            "Epoch 4: val_loss improved from 0.19464 to 0.14866, saving model to weights/unet_64_sgd_bce_8.h5\n",
            "100/100 [==============================] - 66s 660ms/step - loss: 0.1365 - recall: 0.6566 - precision: 0.7086 - val_loss: 0.1487 - val_recall: 0.0887 - val_precision: 0.9145 - lr: 0.0020\n",
            "Epoch 5/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1258 - recall: 0.6540 - precision: 0.7163\n",
            "Epoch 5: val_loss improved from 0.14866 to 0.10848, saving model to weights/unet_64_sgd_bce_8.h5\n",
            "100/100 [==============================] - 66s 662ms/step - loss: 0.1258 - recall: 0.6540 - precision: 0.7163 - val_loss: 0.1085 - val_recall: 0.4042 - val_precision: 0.8107 - lr: 0.0020\n",
            "Epoch 6/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1188 - recall: 0.6568 - precision: 0.7208\n",
            "Epoch 6: val_loss improved from 0.10848 to 0.09211, saving model to weights/unet_64_sgd_bce_8.h5\n",
            "100/100 [==============================] - 66s 662ms/step - loss: 0.1188 - recall: 0.6568 - precision: 0.7208 - val_loss: 0.0921 - val_recall: 0.6089 - val_precision: 0.7707 - lr: 0.0020\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1139 - recall: 0.6557 - precision: 0.7275\n",
            "Epoch 7: val_loss improved from 0.09211 to 0.08522, saving model to weights/unet_64_sgd_bce_8.h5\n",
            "100/100 [==============================] - 66s 660ms/step - loss: 0.1139 - recall: 0.6557 - precision: 0.7275 - val_loss: 0.0852 - val_recall: 0.7789 - val_precision: 0.7012 - lr: 0.0020\n",
            "Epoch 8/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1107 - recall: 0.6556 - precision: 0.7297\n",
            "Epoch 8: val_loss improved from 0.08522 to 0.08377, saving model to weights/unet_64_sgd_bce_8.h5\n",
            "100/100 [==============================] - 66s 664ms/step - loss: 0.1107 - recall: 0.6556 - precision: 0.7297 - val_loss: 0.0838 - val_recall: 0.7981 - val_precision: 0.6981 - lr: 0.0020\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1089 - recall: 0.6497 - precision: 0.7312\n",
            "Epoch 9: val_loss did not improve from 0.08377\n",
            "100/100 [==============================] - 65s 655ms/step - loss: 0.1089 - recall: 0.6497 - precision: 0.7312 - val_loss: 0.0859 - val_recall: 0.8451 - val_precision: 0.6711 - lr: 0.0020\n",
            "Epoch 10/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1059 - recall: 0.6599 - precision: 0.7329\n",
            "Epoch 10: val_loss improved from 0.08377 to 0.08227, saving model to weights/unet_64_sgd_bce_8.h5\n",
            "100/100 [==============================] - 66s 662ms/step - loss: 0.1059 - recall: 0.6599 - precision: 0.7329 - val_loss: 0.0823 - val_recall: 0.8327 - val_precision: 0.6819 - lr: 0.0020\n",
            "Epoch 11/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1052 - recall: 0.6551 - precision: 0.7338\n",
            "Epoch 11: val_loss improved from 0.08227 to 0.07711, saving model to weights/unet_64_sgd_bce_8.h5\n",
            "100/100 [==============================] - 66s 661ms/step - loss: 0.1052 - recall: 0.6551 - precision: 0.7338 - val_loss: 0.0771 - val_recall: 0.7887 - val_precision: 0.7152 - lr: 0.0020\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1027 - recall: 0.6622 - precision: 0.7360\n",
            "Epoch 12: val_loss did not improve from 0.07711\n",
            "100/100 [==============================] - 66s 657ms/step - loss: 0.1027 - recall: 0.6622 - precision: 0.7360 - val_loss: 0.0809 - val_recall: 0.8318 - val_precision: 0.6860 - lr: 0.0020\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.1019 - recall: 0.6555 - precision: 0.7399\n",
            "Epoch 13: val_loss did not improve from 0.07711\n",
            "100/100 [==============================] - 66s 656ms/step - loss: 0.1019 - recall: 0.6555 - precision: 0.7399 - val_loss: 0.0783 - val_recall: 0.8097 - val_precision: 0.7010 - lr: 0.0020\n",
            "Epoch 14/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0994 - recall: 0.6638 - precision: 0.7435\n",
            "Epoch 14: val_loss did not improve from 0.07711\n",
            "100/100 [==============================] - 65s 654ms/step - loss: 0.0994 - recall: 0.6638 - precision: 0.7435 - val_loss: 0.0787 - val_recall: 0.8274 - val_precision: 0.6889 - lr: 0.0020\n",
            "Epoch 15/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0991 - recall: 0.6725 - precision: 0.7397\n",
            "Epoch 15: val_loss improved from 0.07711 to 0.07686, saving model to weights/unet_64_sgd_bce_8.h5\n",
            "100/100 [==============================] - 66s 662ms/step - loss: 0.0991 - recall: 0.6725 - precision: 0.7397 - val_loss: 0.0769 - val_recall: 0.8198 - val_precision: 0.6964 - lr: 0.0020\n",
            "Epoch 16/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0978 - recall: 0.6650 - precision: 0.7459\n",
            "Epoch 16: val_loss improved from 0.07686 to 0.07532, saving model to weights/unet_64_sgd_bce_8.h5\n",
            "100/100 [==============================] - 66s 664ms/step - loss: 0.0978 - recall: 0.6650 - precision: 0.7459 - val_loss: 0.0753 - val_recall: 0.7789 - val_precision: 0.7246 - lr: 0.0020\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0973 - recall: 0.6685 - precision: 0.7444\n",
            "Epoch 17: val_loss did not improve from 0.07532\n",
            "100/100 [==============================] - 66s 656ms/step - loss: 0.0973 - recall: 0.6685 - precision: 0.7444 - val_loss: 0.0791 - val_recall: 0.8348 - val_precision: 0.6832 - lr: 0.0020\n",
            "Epoch 18/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0968 - recall: 0.6715 - precision: 0.7429\n",
            "Epoch 18: val_loss improved from 0.07532 to 0.07447, saving model to weights/unet_64_sgd_bce_8.h5\n",
            "100/100 [==============================] - 66s 663ms/step - loss: 0.0968 - recall: 0.6715 - precision: 0.7429 - val_loss: 0.0745 - val_recall: 0.8078 - val_precision: 0.7084 - lr: 0.0020\n",
            "Epoch 19/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0963 - recall: 0.6707 - precision: 0.7461\n",
            "Epoch 19: val_loss did not improve from 0.07447\n",
            "100/100 [==============================] - 65s 653ms/step - loss: 0.0963 - recall: 0.6707 - precision: 0.7461 - val_loss: 0.0778 - val_recall: 0.8296 - val_precision: 0.6885 - lr: 0.0020\n",
            "Epoch 20/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0953 - recall: 0.6708 - precision: 0.7481\n",
            "Epoch 20: val_loss did not improve from 0.07447\n",
            "100/100 [==============================] - 66s 657ms/step - loss: 0.0953 - recall: 0.6708 - precision: 0.7481 - val_loss: 0.0758 - val_recall: 0.8155 - val_precision: 0.7020 - lr: 0.0020\n",
            "Epoch 21/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0946 - recall: 0.6710 - precision: 0.7492\n",
            "Epoch 21: val_loss did not improve from 0.07447\n",
            "100/100 [==============================] - 65s 655ms/step - loss: 0.0946 - recall: 0.6710 - precision: 0.7492 - val_loss: 0.0751 - val_recall: 0.8118 - val_precision: 0.7022 - lr: 0.0020\n",
            "Epoch 22/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0938 - recall: 0.6731 - precision: 0.7518\n",
            "Epoch 22: val_loss improved from 0.07447 to 0.07402, saving model to weights/unet_64_sgd_bce_8.h5\n",
            "100/100 [==============================] - 66s 660ms/step - loss: 0.0938 - recall: 0.6731 - precision: 0.7518 - val_loss: 0.0740 - val_recall: 0.7896 - val_precision: 0.7164 - lr: 0.0020\n",
            "Epoch 23/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0933 - recall: 0.6759 - precision: 0.7503\n",
            "Epoch 23: val_loss did not improve from 0.07402\n",
            "100/100 [==============================] - 66s 656ms/step - loss: 0.0933 - recall: 0.6759 - precision: 0.7503 - val_loss: 0.0767 - val_recall: 0.8260 - val_precision: 0.6916 - lr: 0.0020\n",
            "Epoch 24/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0931 - recall: 0.6771 - precision: 0.7507\n",
            "Epoch 24: val_loss improved from 0.07402 to 0.07324, saving model to weights/unet_64_sgd_bce_8.h5\n",
            "100/100 [==============================] - 66s 661ms/step - loss: 0.0931 - recall: 0.6771 - precision: 0.7507 - val_loss: 0.0732 - val_recall: 0.8014 - val_precision: 0.7137 - lr: 0.0020\n",
            "Epoch 25/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0925 - recall: 0.6832 - precision: 0.7479\n",
            "Epoch 25: val_loss improved from 0.07324 to 0.07234, saving model to weights/unet_64_sgd_bce_8.h5\n",
            "100/100 [==============================] - 66s 661ms/step - loss: 0.0925 - recall: 0.6832 - precision: 0.7479 - val_loss: 0.0723 - val_recall: 0.7781 - val_precision: 0.7295 - lr: 0.0020\n",
            "Epoch 26/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0922 - recall: 0.6762 - precision: 0.7541\n",
            "Epoch 26: val_loss did not improve from 0.07234\n",
            "100/100 [==============================] - 66s 655ms/step - loss: 0.0922 - recall: 0.6762 - precision: 0.7541 - val_loss: 0.0741 - val_recall: 0.7957 - val_precision: 0.7146 - lr: 0.0020\n",
            "Epoch 27/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0918 - recall: 0.6882 - precision: 0.7485\n",
            "Epoch 27: val_loss improved from 0.07234 to 0.07130, saving model to weights/unet_64_sgd_bce_8.h5\n",
            "100/100 [==============================] - 66s 663ms/step - loss: 0.0918 - recall: 0.6882 - precision: 0.7485 - val_loss: 0.0713 - val_recall: 0.7728 - val_precision: 0.7323 - lr: 0.0020\n",
            "Epoch 28/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0909 - recall: 0.6807 - precision: 0.7555\n",
            "Epoch 28: val_loss did not improve from 0.07130\n",
            "100/100 [==============================] - 65s 654ms/step - loss: 0.0909 - recall: 0.6807 - precision: 0.7555 - val_loss: 0.0763 - val_recall: 0.8318 - val_precision: 0.6858 - lr: 0.0020\n",
            "Epoch 29/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0907 - recall: 0.6899 - precision: 0.7499\n",
            "Epoch 29: val_loss did not improve from 0.07130\n",
            "100/100 [==============================] - 65s 654ms/step - loss: 0.0907 - recall: 0.6899 - precision: 0.7499 - val_loss: 0.0730 - val_recall: 0.7624 - val_precision: 0.7293 - lr: 0.0020\n",
            "Epoch 30/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0906 - recall: 0.6843 - precision: 0.7538\n",
            "Epoch 30: val_loss did not improve from 0.07130\n",
            "100/100 [==============================] - 65s 653ms/step - loss: 0.0906 - recall: 0.6843 - precision: 0.7538 - val_loss: 0.0760 - val_recall: 0.8305 - val_precision: 0.6876 - lr: 0.0020\n",
            "Epoch 31/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0902 - recall: 0.6892 - precision: 0.7526\n",
            "Epoch 31: val_loss did not improve from 0.07130\n",
            "\n",
            "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "100/100 [==============================] - 65s 654ms/step - loss: 0.0902 - recall: 0.6892 - precision: 0.7526 - val_loss: 0.0728 - val_recall: 0.8061 - val_precision: 0.7084 - lr: 0.0020\n",
            "Epoch 32/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0898 - recall: 0.6906 - precision: 0.7524\n",
            "Epoch 32: val_loss did not improve from 0.07130\n",
            "100/100 [==============================] - 65s 654ms/step - loss: 0.0898 - recall: 0.6906 - precision: 0.7524 - val_loss: 0.0729 - val_recall: 0.7961 - val_precision: 0.7146 - lr: 2.0000e-04\n",
            "Epoch 33/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0895 - recall: 0.6893 - precision: 0.7541\n",
            "Epoch 33: val_loss did not improve from 0.07130\n",
            "100/100 [==============================] - 65s 653ms/step - loss: 0.0895 - recall: 0.6893 - precision: 0.7541 - val_loss: 0.0725 - val_recall: 0.7840 - val_precision: 0.7194 - lr: 2.0000e-04\n",
            "Epoch 34/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0893 - recall: 0.6873 - precision: 0.7569\n",
            "Epoch 34: val_loss did not improve from 0.07130\n",
            "100/100 [==============================] - 65s 652ms/step - loss: 0.0893 - recall: 0.6873 - precision: 0.7569 - val_loss: 0.0727 - val_recall: 0.7851 - val_precision: 0.7194 - lr: 2.0000e-04\n",
            "Epoch 35/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0892 - recall: 0.6938 - precision: 0.7535\n",
            "Epoch 35: val_loss did not improve from 0.07130\n",
            "\n",
            "Epoch 35: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
            "100/100 [==============================] - 65s 652ms/step - loss: 0.0892 - recall: 0.6938 - precision: 0.7535 - val_loss: 0.0729 - val_recall: 0.7840 - val_precision: 0.7187 - lr: 2.0000e-04\n",
            "Epoch 36/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0891 - recall: 0.6818 - precision: 0.7601\n",
            "Epoch 36: val_loss did not improve from 0.07130\n",
            "100/100 [==============================] - 65s 652ms/step - loss: 0.0891 - recall: 0.6818 - precision: 0.7601 - val_loss: 0.0724 - val_recall: 0.7771 - val_precision: 0.7241 - lr: 2.0000e-05\n",
            "Epoch 37/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0893 - recall: 0.6798 - precision: 0.7612\n",
            "Epoch 37: val_loss did not improve from 0.07130\n",
            "100/100 [==============================] - 65s 652ms/step - loss: 0.0893 - recall: 0.6798 - precision: 0.7612 - val_loss: 0.0725 - val_recall: 0.7799 - val_precision: 0.7227 - lr: 2.0000e-05\n",
            "Epoch 38/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0895 - recall: 0.6851 - precision: 0.7566\n",
            "Epoch 38: val_loss did not improve from 0.07130\n",
            "100/100 [==============================] - 65s 653ms/step - loss: 0.0895 - recall: 0.6851 - precision: 0.7566 - val_loss: 0.0726 - val_recall: 0.7851 - val_precision: 0.7196 - lr: 2.0000e-05\n",
            "Epoch 39/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0892 - recall: 0.6861 - precision: 0.7584\n",
            "Epoch 39: val_loss did not improve from 0.07130\n",
            "\n",
            "Epoch 39: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "100/100 [==============================] - 66s 655ms/step - loss: 0.0892 - recall: 0.6861 - precision: 0.7584 - val_loss: 0.0727 - val_recall: 0.7874 - val_precision: 0.7186 - lr: 2.0000e-05\n",
            "Epoch 40/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0892 - recall: 0.6855 - precision: 0.7573\n",
            "Epoch 40: val_loss did not improve from 0.07130\n",
            "100/100 [==============================] - 66s 656ms/step - loss: 0.0892 - recall: 0.6855 - precision: 0.7573 - val_loss: 0.0726 - val_recall: 0.7856 - val_precision: 0.7195 - lr: 1.0000e-05\n",
            "Epoch 41/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0891 - recall: 0.6868 - precision: 0.7578\n",
            "Epoch 41: val_loss did not improve from 0.07130\n",
            "100/100 [==============================] - 65s 653ms/step - loss: 0.0891 - recall: 0.6868 - precision: 0.7578 - val_loss: 0.0726 - val_recall: 0.7841 - val_precision: 0.7200 - lr: 1.0000e-05\n",
            "Epoch 42/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.0892 - recall: 0.6874 - precision: 0.7573\n",
            "Epoch 42: val_loss did not improve from 0.07130\n",
            "100/100 [==============================] - 65s 653ms/step - loss: 0.0892 - recall: 0.6874 - precision: 0.7573 - val_loss: 0.0723 - val_recall: 0.7806 - val_precision: 0.7224 - lr: 1.0000e-05\n",
            "Epoch 42: early stopping\n"
          ]
        }
      ],
      "source": [
        "# SGD, BCE, batch_size=8\n",
        "logs_p = 'logs/unet_64_sgd_bce_8.csv'\n",
        "weights_p = 'weights/unet_64_sgd_bce_8.h5'\n",
        "\n",
        "checkpointer = ModelCheckpoint(weights_p, verbose=1, save_best_only=True)\n",
        "callbacks_f = [earlystopper, checkpointer, reduce_lr, CSVLogger(logs_p, append=True, separator=';')]\n",
        "optimizer_f = sgd\n",
        "loss_f = bce\n",
        "metrics_f = [recall, precision,]\n",
        "#batch_s = 8 * tpu_strategy.num_replicas_in_sync # with TPU\n",
        "batch_s = 8\n",
        "epochs_s = 100\n",
        "\n",
        "#with tpu_strategy.scope():  # with TPU\n",
        "#  model = unet_model(input=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),)\n",
        "#  sgd = optimizers.SGD(learning_rate=0.002, decay=0.00003, momentum=0.9)\n",
        "#  bce = BinaryCrossentropy()\n",
        "#  model.compile(optimizer=sgd, loss=bce, metrics=[Recall(), Precision(),])\n",
        "model = unet_model(input=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),)\n",
        "model.compile(optimizer=sgd, loss=bce, metrics=[recall, precision,])\n",
        "\n",
        "results = model.fit(x=X_train, y=Y_train, validation_data=(X_val, Y_val), batch_size=batch_s, \n",
        "                    epochs=epochs_s, callbacks=callbacks_f, verbose=1,)\n",
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwLdhAT94nL1"
      },
      "source": [
        "### Adam, DICE + Jaccard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQFic6s99RyI",
        "outputId": "367dedb1-3a22-4e03-8cf0-f3f78fdb5972"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.4226 - dice_coef: 0.4214 - jacard_coef: 0.2719\n",
            "Epoch 1: val_loss improved from inf to -0.12810, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 72s 691ms/step - loss: -0.4226 - dice_coef: 0.4214 - jacard_coef: 0.2719 - val_loss: -0.1281 - val_dice_coef: 0.1252 - val_jacard_coef: 0.0668 - lr: 1.0000e-05\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.5155 - dice_coef: 0.5130 - jacard_coef: 0.3478\n",
            "Epoch 2: val_loss did not improve from -0.12810\n",
            "100/100 [==============================] - 65s 652ms/step - loss: -0.5155 - dice_coef: 0.5130 - jacard_coef: 0.3478 - val_loss: -0.1121 - val_dice_coef: 0.1104 - val_jacard_coef: 0.0584 - lr: 1.0000e-05\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.5393 - dice_coef: 0.5358 - jacard_coef: 0.3693\n",
            "Epoch 3: val_loss did not improve from -0.12810\n",
            "100/100 [==============================] - 65s 649ms/step - loss: -0.5393 - dice_coef: 0.5358 - jacard_coef: 0.3693 - val_loss: -0.0478 - val_dice_coef: 0.0480 - val_jacard_coef: 0.0246 - lr: 1.0000e-05\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.5576 - dice_coef: 0.5551 - jacard_coef: 0.3871\n",
            "Epoch 4: val_loss improved from -0.12810 to -0.20023, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 68s 680ms/step - loss: -0.5576 - dice_coef: 0.5551 - jacard_coef: 0.3871 - val_loss: -0.2002 - val_dice_coef: 0.1871 - val_jacard_coef: 0.1059 - lr: 1.0000e-05\n",
            "Epoch 5/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.5695 - dice_coef: 0.5666 - jacard_coef: 0.3987\n",
            "Epoch 5: val_loss improved from -0.20023 to -0.41122, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 68s 680ms/step - loss: -0.5695 - dice_coef: 0.5666 - jacard_coef: 0.3987 - val_loss: -0.4112 - val_dice_coef: 0.4292 - val_jacard_coef: 0.2762 - lr: 1.0000e-05\n",
            "Epoch 6/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.5830 - dice_coef: 0.5831 - jacard_coef: 0.4138\n",
            "Epoch 6: val_loss improved from -0.41122 to -0.61476, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 67s 674ms/step - loss: -0.5830 - dice_coef: 0.5831 - jacard_coef: 0.4138 - val_loss: -0.6148 - val_dice_coef: 0.6216 - val_jacard_coef: 0.4515 - lr: 1.0000e-05\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.5917 - dice_coef: 0.5881 - jacard_coef: 0.4202\n",
            "Epoch 7: val_loss improved from -0.61476 to -0.62884, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 68s 677ms/step - loss: -0.5917 - dice_coef: 0.5881 - jacard_coef: 0.4202 - val_loss: -0.6288 - val_dice_coef: 0.6255 - val_jacard_coef: 0.4566 - lr: 1.0000e-05\n",
            "Epoch 8/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.6041 - dice_coef: 0.6049 - jacard_coef: 0.4363\n",
            "Epoch 8: val_loss improved from -0.62884 to -0.64009, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 67s 674ms/step - loss: -0.6041 - dice_coef: 0.6049 - jacard_coef: 0.4363 - val_loss: -0.6401 - val_dice_coef: 0.6374 - val_jacard_coef: 0.4692 - lr: 1.0000e-05\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.6129 - dice_coef: 0.6128 - jacard_coef: 0.4440\n",
            "Epoch 9: val_loss improved from -0.64009 to -0.65529, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 68s 676ms/step - loss: -0.6129 - dice_coef: 0.6128 - jacard_coef: 0.4440 - val_loss: -0.6553 - val_dice_coef: 0.6533 - val_jacard_coef: 0.4862 - lr: 1.0000e-05\n",
            "Epoch 10/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.6223 - dice_coef: 0.6215 - jacard_coef: 0.4529\n",
            "Epoch 10: val_loss did not improve from -0.65529\n",
            "100/100 [==============================] - 65s 651ms/step - loss: -0.6223 - dice_coef: 0.6215 - jacard_coef: 0.4529 - val_loss: -0.6508 - val_dice_coef: 0.6490 - val_jacard_coef: 0.4817 - lr: 1.0000e-05\n",
            "Epoch 11/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.6250 - dice_coef: 0.6247 - jacard_coef: 0.4569\n",
            "Epoch 11: val_loss did not improve from -0.65529\n",
            "100/100 [==============================] - 65s 649ms/step - loss: -0.6250 - dice_coef: 0.6247 - jacard_coef: 0.4569 - val_loss: -0.6508 - val_dice_coef: 0.6488 - val_jacard_coef: 0.4818 - lr: 1.0000e-05\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.6338 - dice_coef: 0.6297 - jacard_coef: 0.4630\n",
            "Epoch 12: val_loss did not improve from -0.65529\n",
            "100/100 [==============================] - 65s 648ms/step - loss: -0.6338 - dice_coef: 0.6297 - jacard_coef: 0.4630 - val_loss: -0.6495 - val_dice_coef: 0.6467 - val_jacard_coef: 0.4794 - lr: 1.0000e-05\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.6375 - dice_coef: 0.6373 - jacard_coef: 0.4705\n",
            "Epoch 13: val_loss did not improve from -0.65529\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.6375 - dice_coef: 0.6373 - jacard_coef: 0.4705 - val_loss: -0.6502 - val_dice_coef: 0.6474 - val_jacard_coef: 0.4802 - lr: 1.0000e-05\n",
            "Epoch 14/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.6412 - dice_coef: 0.6410 - jacard_coef: 0.4747\n",
            "Epoch 14: val_loss did not improve from -0.65529\n",
            "100/100 [==============================] - 65s 652ms/step - loss: -0.6412 - dice_coef: 0.6410 - jacard_coef: 0.4747 - val_loss: -0.6477 - val_dice_coef: 0.6447 - val_jacard_coef: 0.4773 - lr: 1.0000e-05\n",
            "Epoch 15/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.6465 - dice_coef: 0.6473 - jacard_coef: 0.4810\n",
            "Epoch 15: val_loss did not improve from -0.65529\n",
            "100/100 [==============================] - 65s 651ms/step - loss: -0.6465 - dice_coef: 0.6473 - jacard_coef: 0.4810 - val_loss: -0.6518 - val_dice_coef: 0.6500 - val_jacard_coef: 0.4831 - lr: 1.0000e-05\n",
            "Epoch 16/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.6497 - dice_coef: 0.6479 - jacard_coef: 0.4822\n",
            "Epoch 16: val_loss improved from -0.65529 to -0.67107, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 68s 678ms/step - loss: -0.6497 - dice_coef: 0.6479 - jacard_coef: 0.4822 - val_loss: -0.6711 - val_dice_coef: 0.6700 - val_jacard_coef: 0.5052 - lr: 1.0000e-05\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.6549 - dice_coef: 0.6505 - jacard_coef: 0.4858\n",
            "Epoch 17: val_loss did not improve from -0.67107\n",
            "100/100 [==============================] - 65s 651ms/step - loss: -0.6549 - dice_coef: 0.6505 - jacard_coef: 0.4858 - val_loss: -0.6692 - val_dice_coef: 0.6682 - val_jacard_coef: 0.5031 - lr: 1.0000e-05\n",
            "Epoch 18/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.6577 - dice_coef: 0.6575 - jacard_coef: 0.4921\n",
            "Epoch 18: val_loss did not improve from -0.67107\n",
            "100/100 [==============================] - 65s 648ms/step - loss: -0.6577 - dice_coef: 0.6575 - jacard_coef: 0.4921 - val_loss: -0.6617 - val_dice_coef: 0.6602 - val_jacard_coef: 0.4946 - lr: 1.0000e-05\n",
            "Epoch 19/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.6634 - dice_coef: 0.6618 - jacard_coef: 0.4968\n",
            "Epoch 19: val_loss improved from -0.67107 to -0.67210, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 68s 680ms/step - loss: -0.6634 - dice_coef: 0.6618 - jacard_coef: 0.4968 - val_loss: -0.6721 - val_dice_coef: 0.6728 - val_jacard_coef: 0.5083 - lr: 1.0000e-05\n",
            "Epoch 20/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.6662 - dice_coef: 0.6660 - jacard_coef: 0.5015\n",
            "Epoch 20: val_loss improved from -0.67210 to -0.67312, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 68s 680ms/step - loss: -0.6662 - dice_coef: 0.6660 - jacard_coef: 0.5015 - val_loss: -0.6731 - val_dice_coef: 0.6740 - val_jacard_coef: 0.5094 - lr: 1.0000e-05\n",
            "Epoch 21/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.6669 - dice_coef: 0.6677 - jacard_coef: 0.5042\n",
            "Epoch 21: val_loss did not improve from -0.67312\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.6669 - dice_coef: 0.6677 - jacard_coef: 0.5042 - val_loss: -0.6693 - val_dice_coef: 0.6694 - val_jacard_coef: 0.5050 - lr: 1.0000e-05\n",
            "Epoch 22/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.6728 - dice_coef: 0.6726 - jacard_coef: 0.5085\n",
            "Epoch 22: val_loss did not improve from -0.67312\n",
            "100/100 [==============================] - 65s 650ms/step - loss: -0.6728 - dice_coef: 0.6726 - jacard_coef: 0.5085 - val_loss: -0.6725 - val_dice_coef: 0.6725 - val_jacard_coef: 0.5081 - lr: 1.0000e-05\n",
            "Epoch 23/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.6748 - dice_coef: 0.6706 - jacard_coef: 0.5085\n",
            "Epoch 23: val_loss improved from -0.67312 to -0.68447, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 68s 678ms/step - loss: -0.6748 - dice_coef: 0.6706 - jacard_coef: 0.5085 - val_loss: -0.6845 - val_dice_coef: 0.6851 - val_jacard_coef: 0.5221 - lr: 1.0000e-05\n",
            "Epoch 24/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.6792 - dice_coef: 0.6794 - jacard_coef: 0.5167\n",
            "Epoch 24: val_loss did not improve from -0.68447\n",
            "100/100 [==============================] - 65s 652ms/step - loss: -0.6792 - dice_coef: 0.6794 - jacard_coef: 0.5167 - val_loss: -0.6730 - val_dice_coef: 0.6730 - val_jacard_coef: 0.5087 - lr: 1.0000e-05\n",
            "Epoch 25/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.6862 - dice_coef: 0.6860 - jacard_coef: 0.5237\n",
            "Epoch 25: val_loss did not improve from -0.68447\n",
            "100/100 [==============================] - 65s 650ms/step - loss: -0.6862 - dice_coef: 0.6860 - jacard_coef: 0.5237 - val_loss: -0.6666 - val_dice_coef: 0.6675 - val_jacard_coef: 0.5023 - lr: 1.0000e-05\n",
            "Epoch 26/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.6867 - dice_coef: 0.6871 - jacard_coef: 0.5252\n",
            "Epoch 26: val_loss did not improve from -0.68447\n",
            "100/100 [==============================] - 65s 652ms/step - loss: -0.6867 - dice_coef: 0.6871 - jacard_coef: 0.5252 - val_loss: -0.6800 - val_dice_coef: 0.6817 - val_jacard_coef: 0.5182 - lr: 1.0000e-05\n",
            "Epoch 27/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.6888 - dice_coef: 0.6829 - jacard_coef: 0.5234\n",
            "Epoch 27: val_loss improved from -0.68447 to -0.68459, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 67s 674ms/step - loss: -0.6888 - dice_coef: 0.6829 - jacard_coef: 0.5234 - val_loss: -0.6846 - val_dice_coef: 0.6862 - val_jacard_coef: 0.5232 - lr: 1.0000e-05\n",
            "Epoch 28/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.6916 - dice_coef: 0.6922 - jacard_coef: 0.5315\n",
            "Epoch 28: val_loss did not improve from -0.68459\n",
            "100/100 [==============================] - 65s 650ms/step - loss: -0.6916 - dice_coef: 0.6922 - jacard_coef: 0.5315 - val_loss: -0.6804 - val_dice_coef: 0.6824 - val_jacard_coef: 0.5189 - lr: 1.0000e-05\n",
            "Epoch 29/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.6941 - dice_coef: 0.6936 - jacard_coef: 0.5331\n",
            "Epoch 29: val_loss did not improve from -0.68459\n",
            "100/100 [==============================] - 65s 650ms/step - loss: -0.6941 - dice_coef: 0.6936 - jacard_coef: 0.5331 - val_loss: -0.6798 - val_dice_coef: 0.6801 - val_jacard_coef: 0.5166 - lr: 1.0000e-05\n",
            "Epoch 30/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.6972 - dice_coef: 0.6978 - jacard_coef: 0.5380\n",
            "Epoch 30: val_loss did not improve from -0.68459\n",
            "100/100 [==============================] - 65s 651ms/step - loss: -0.6972 - dice_coef: 0.6978 - jacard_coef: 0.5380 - val_loss: -0.6749 - val_dice_coef: 0.6759 - val_jacard_coef: 0.5121 - lr: 1.0000e-05\n",
            "Epoch 31/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7010 - dice_coef: 0.7007 - jacard_coef: 0.5412\n",
            "Epoch 31: val_loss did not improve from -0.68459\n",
            "100/100 [==============================] - 65s 652ms/step - loss: -0.7010 - dice_coef: 0.7007 - jacard_coef: 0.5412 - val_loss: -0.6830 - val_dice_coef: 0.6851 - val_jacard_coef: 0.5223 - lr: 1.0000e-05\n",
            "Epoch 32/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7033 - dice_coef: 0.6974 - jacard_coef: 0.5398\n",
            "Epoch 32: val_loss did not improve from -0.68459\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.7033 - dice_coef: 0.6974 - jacard_coef: 0.5398 - val_loss: -0.6769 - val_dice_coef: 0.6784 - val_jacard_coef: 0.5149 - lr: 1.0000e-05\n",
            "Epoch 33/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7044 - dice_coef: 0.7044 - jacard_coef: 0.5464\n",
            "Epoch 33: val_loss did not improve from -0.68459\n",
            "100/100 [==============================] - 65s 652ms/step - loss: -0.7044 - dice_coef: 0.7044 - jacard_coef: 0.5464 - val_loss: -0.6835 - val_dice_coef: 0.6856 - val_jacard_coef: 0.5228 - lr: 1.0000e-05\n",
            "Epoch 34/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7119 - dice_coef: 0.7120 - jacard_coef: 0.5544\n",
            "Epoch 34: val_loss improved from -0.68459 to -0.68778, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 67s 675ms/step - loss: -0.7119 - dice_coef: 0.7120 - jacard_coef: 0.5544 - val_loss: -0.6878 - val_dice_coef: 0.6909 - val_jacard_coef: 0.5289 - lr: 1.0000e-05\n",
            "Epoch 35/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7141 - dice_coef: 0.7142 - jacard_coef: 0.5569\n",
            "Epoch 35: val_loss improved from -0.68778 to -0.68801, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 68s 675ms/step - loss: -0.7141 - dice_coef: 0.7142 - jacard_coef: 0.5569 - val_loss: -0.6880 - val_dice_coef: 0.6904 - val_jacard_coef: 0.5285 - lr: 1.0000e-05\n",
            "Epoch 36/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7157 - dice_coef: 0.7166 - jacard_coef: 0.5602\n",
            "Epoch 36: val_loss improved from -0.68801 to -0.68860, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 68s 677ms/step - loss: -0.7157 - dice_coef: 0.7166 - jacard_coef: 0.5602 - val_loss: -0.6886 - val_dice_coef: 0.6913 - val_jacard_coef: 0.5296 - lr: 1.0000e-05\n",
            "Epoch 37/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7131 - dice_coef: 0.7133 - jacard_coef: 0.5575\n",
            "Epoch 37: val_loss did not improve from -0.68860\n",
            "100/100 [==============================] - 65s 652ms/step - loss: -0.7131 - dice_coef: 0.7133 - jacard_coef: 0.5575 - val_loss: -0.6842 - val_dice_coef: 0.6862 - val_jacard_coef: 0.5237 - lr: 1.0000e-05\n",
            "Epoch 38/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7170 - dice_coef: 0.7169 - jacard_coef: 0.5614\n",
            "Epoch 38: val_loss did not improve from -0.68860\n",
            "100/100 [==============================] - 65s 651ms/step - loss: -0.7170 - dice_coef: 0.7169 - jacard_coef: 0.5614 - val_loss: -0.6850 - val_dice_coef: 0.6869 - val_jacard_coef: 0.5244 - lr: 1.0000e-05\n",
            "Epoch 39/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7231 - dice_coef: 0.7199 - jacard_coef: 0.5651\n",
            "Epoch 39: val_loss improved from -0.68860 to -0.69173, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 68s 676ms/step - loss: -0.7231 - dice_coef: 0.7199 - jacard_coef: 0.5651 - val_loss: -0.6917 - val_dice_coef: 0.6942 - val_jacard_coef: 0.5329 - lr: 1.0000e-05\n",
            "Epoch 40/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7242 - dice_coef: 0.7246 - jacard_coef: 0.5703\n",
            "Epoch 40: val_loss did not improve from -0.69173\n",
            "100/100 [==============================] - 65s 654ms/step - loss: -0.7242 - dice_coef: 0.7246 - jacard_coef: 0.5703 - val_loss: -0.6877 - val_dice_coef: 0.6915 - val_jacard_coef: 0.5298 - lr: 1.0000e-05\n",
            "Epoch 41/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7271 - dice_coef: 0.7267 - jacard_coef: 0.5725\n",
            "Epoch 41: val_loss improved from -0.69173 to -0.69389, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 68s 676ms/step - loss: -0.7271 - dice_coef: 0.7267 - jacard_coef: 0.5725 - val_loss: -0.6939 - val_dice_coef: 0.6971 - val_jacard_coef: 0.5365 - lr: 1.0000e-05\n",
            "Epoch 42/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7318 - dice_coef: 0.7308 - jacard_coef: 0.5773\n",
            "Epoch 42: val_loss did not improve from -0.69389\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.7318 - dice_coef: 0.7308 - jacard_coef: 0.5773 - val_loss: -0.6892 - val_dice_coef: 0.6930 - val_jacard_coef: 0.5312 - lr: 1.0000e-05\n",
            "Epoch 43/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7260 - dice_coef: 0.7268 - jacard_coef: 0.5737\n",
            "Epoch 43: val_loss did not improve from -0.69389\n",
            "100/100 [==============================] - 65s 651ms/step - loss: -0.7260 - dice_coef: 0.7268 - jacard_coef: 0.5737 - val_loss: -0.6862 - val_dice_coef: 0.6898 - val_jacard_coef: 0.5276 - lr: 1.0000e-05\n",
            "Epoch 44/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7300 - dice_coef: 0.7297 - jacard_coef: 0.5773\n",
            "Epoch 44: val_loss did not improve from -0.69389\n",
            "100/100 [==============================] - 65s 651ms/step - loss: -0.7300 - dice_coef: 0.7297 - jacard_coef: 0.5773 - val_loss: -0.6936 - val_dice_coef: 0.6971 - val_jacard_coef: 0.5363 - lr: 1.0000e-05\n",
            "Epoch 45/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7343 - dice_coef: 0.7312 - jacard_coef: 0.5790\n",
            "Epoch 45: val_loss improved from -0.69389 to -0.69887, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 68s 676ms/step - loss: -0.7343 - dice_coef: 0.7312 - jacard_coef: 0.5790 - val_loss: -0.6989 - val_dice_coef: 0.7033 - val_jacard_coef: 0.5437 - lr: 1.0000e-05\n",
            "Epoch 46/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7375 - dice_coef: 0.7379 - jacard_coef: 0.5865\n",
            "Epoch 46: val_loss did not improve from -0.69887\n",
            "100/100 [==============================] - 65s 651ms/step - loss: -0.7375 - dice_coef: 0.7379 - jacard_coef: 0.5865 - val_loss: -0.6914 - val_dice_coef: 0.6958 - val_jacard_coef: 0.5349 - lr: 1.0000e-05\n",
            "Epoch 47/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7410 - dice_coef: 0.7414 - jacard_coef: 0.5904\n",
            "Epoch 47: val_loss did not improve from -0.69887\n",
            "100/100 [==============================] - 65s 651ms/step - loss: -0.7410 - dice_coef: 0.7414 - jacard_coef: 0.5904 - val_loss: -0.6940 - val_dice_coef: 0.6981 - val_jacard_coef: 0.5374 - lr: 1.0000e-05\n",
            "Epoch 48/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7409 - dice_coef: 0.7379 - jacard_coef: 0.5872\n",
            "Epoch 48: val_loss did not improve from -0.69887\n",
            "100/100 [==============================] - 65s 652ms/step - loss: -0.7409 - dice_coef: 0.7379 - jacard_coef: 0.5872 - val_loss: -0.6877 - val_dice_coef: 0.6911 - val_jacard_coef: 0.5293 - lr: 1.0000e-05\n",
            "Epoch 49/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7424 - dice_coef: 0.7427 - jacard_coef: 0.5923\n",
            "Epoch 49: val_loss did not improve from -0.69887\n",
            "100/100 [==============================] - 65s 651ms/step - loss: -0.7424 - dice_coef: 0.7427 - jacard_coef: 0.5923 - val_loss: -0.6917 - val_dice_coef: 0.6965 - val_jacard_coef: 0.5355 - lr: 1.0000e-05\n",
            "Epoch 50/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7443 - dice_coef: 0.7403 - jacard_coef: 0.5908\n",
            "Epoch 50: val_loss did not improve from -0.69887\n",
            "100/100 [==============================] - 65s 652ms/step - loss: -0.7443 - dice_coef: 0.7403 - jacard_coef: 0.5908 - val_loss: -0.6873 - val_dice_coef: 0.6930 - val_jacard_coef: 0.5312 - lr: 1.0000e-05\n",
            "Epoch 51/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7443 - dice_coef: 0.7446 - jacard_coef: 0.5955\n",
            "Epoch 51: val_loss did not improve from -0.69887\n",
            "100/100 [==============================] - 65s 652ms/step - loss: -0.7443 - dice_coef: 0.7446 - jacard_coef: 0.5955 - val_loss: -0.6953 - val_dice_coef: 0.6995 - val_jacard_coef: 0.5390 - lr: 1.0000e-05\n",
            "Epoch 52/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7493 - dice_coef: 0.7456 - jacard_coef: 0.5974\n",
            "Epoch 52: val_loss did not improve from -0.69887\n",
            "100/100 [==============================] - 65s 648ms/step - loss: -0.7493 - dice_coef: 0.7456 - jacard_coef: 0.5974 - val_loss: -0.6976 - val_dice_coef: 0.7019 - val_jacard_coef: 0.5421 - lr: 1.0000e-05\n",
            "Epoch 53/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7489 - dice_coef: 0.7478 - jacard_coef: 0.5992\n",
            "Epoch 53: val_loss did not improve from -0.69887\n",
            "100/100 [==============================] - 65s 651ms/step - loss: -0.7489 - dice_coef: 0.7478 - jacard_coef: 0.5992 - val_loss: -0.6972 - val_dice_coef: 0.7022 - val_jacard_coef: 0.5421 - lr: 1.0000e-05\n",
            "Epoch 54/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7495 - dice_coef: 0.7499 - jacard_coef: 0.6022\n",
            "Epoch 54: val_loss improved from -0.69887 to -0.69893, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 68s 677ms/step - loss: -0.7495 - dice_coef: 0.7499 - jacard_coef: 0.6022 - val_loss: -0.6989 - val_dice_coef: 0.7039 - val_jacard_coef: 0.5443 - lr: 1.0000e-05\n",
            "Epoch 55/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7562 - dice_coef: 0.7560 - jacard_coef: 0.6090\n",
            "Epoch 55: val_loss improved from -0.69893 to -0.69912, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 68s 678ms/step - loss: -0.7562 - dice_coef: 0.7560 - jacard_coef: 0.6090 - val_loss: -0.6991 - val_dice_coef: 0.7043 - val_jacard_coef: 0.5446 - lr: 1.0000e-05\n",
            "Epoch 56/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7560 - dice_coef: 0.7563 - jacard_coef: 0.6098\n",
            "Epoch 56: val_loss did not improve from -0.69912\n",
            "100/100 [==============================] - 65s 652ms/step - loss: -0.7560 - dice_coef: 0.7563 - jacard_coef: 0.6098 - val_loss: -0.6982 - val_dice_coef: 0.7038 - val_jacard_coef: 0.5442 - lr: 1.0000e-05\n",
            "Epoch 57/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7584 - dice_coef: 0.7585 - jacard_coef: 0.6121\n",
            "Epoch 57: val_loss improved from -0.69912 to -0.69924, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 68s 676ms/step - loss: -0.7584 - dice_coef: 0.7585 - jacard_coef: 0.6121 - val_loss: -0.6992 - val_dice_coef: 0.7046 - val_jacard_coef: 0.5450 - lr: 1.0000e-05\n",
            "Epoch 58/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7622 - dice_coef: 0.7577 - jacard_coef: 0.6130\n",
            "Epoch 58: val_loss improved from -0.69924 to -0.70003, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 68s 679ms/step - loss: -0.7622 - dice_coef: 0.7577 - jacard_coef: 0.6130 - val_loss: -0.7000 - val_dice_coef: 0.7052 - val_jacard_coef: 0.5456 - lr: 1.0000e-05\n",
            "Epoch 59/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7598 - dice_coef: 0.7536 - jacard_coef: 0.6093\n",
            "Epoch 59: val_loss did not improve from -0.70003\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.7598 - dice_coef: 0.7536 - jacard_coef: 0.6093 - val_loss: -0.6995 - val_dice_coef: 0.7040 - val_jacard_coef: 0.5446 - lr: 1.0000e-05\n",
            "Epoch 60/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7657 - dice_coef: 0.7659 - jacard_coef: 0.6217\n",
            "Epoch 60: val_loss improved from -0.70003 to -0.70283, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 68s 678ms/step - loss: -0.7657 - dice_coef: 0.7659 - jacard_coef: 0.6217 - val_loss: -0.7028 - val_dice_coef: 0.7080 - val_jacard_coef: 0.5493 - lr: 1.0000e-05\n",
            "Epoch 61/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7638 - dice_coef: 0.7619 - jacard_coef: 0.6176\n",
            "Epoch 61: val_loss improved from -0.70283 to -0.70422, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 68s 681ms/step - loss: -0.7638 - dice_coef: 0.7619 - jacard_coef: 0.6176 - val_loss: -0.7042 - val_dice_coef: 0.7098 - val_jacard_coef: 0.5514 - lr: 1.0000e-05\n",
            "Epoch 62/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7679 - dice_coef: 0.7634 - jacard_coef: 0.6203\n",
            "Epoch 62: val_loss did not improve from -0.70422\n",
            "100/100 [==============================] - 65s 652ms/step - loss: -0.7679 - dice_coef: 0.7634 - jacard_coef: 0.6203 - val_loss: -0.6980 - val_dice_coef: 0.7030 - val_jacard_coef: 0.5432 - lr: 1.0000e-05\n",
            "Epoch 63/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7701 - dice_coef: 0.7677 - jacard_coef: 0.6247\n",
            "Epoch 63: val_loss did not improve from -0.70422\n",
            "100/100 [==============================] - 65s 651ms/step - loss: -0.7701 - dice_coef: 0.7677 - jacard_coef: 0.6247 - val_loss: -0.7033 - val_dice_coef: 0.7093 - val_jacard_coef: 0.5510 - lr: 1.0000e-05\n",
            "Epoch 64/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7685 - dice_coef: 0.7683 - jacard_coef: 0.6259\n",
            "Epoch 64: val_loss did not improve from -0.70422\n",
            "100/100 [==============================] - 65s 652ms/step - loss: -0.7685 - dice_coef: 0.7683 - jacard_coef: 0.6259 - val_loss: -0.7029 - val_dice_coef: 0.7098 - val_jacard_coef: 0.5514 - lr: 1.0000e-05\n",
            "Epoch 65/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7741 - dice_coef: 0.7749 - jacard_coef: 0.6338\n",
            "Epoch 65: val_loss improved from -0.70422 to -0.70574, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 68s 676ms/step - loss: -0.7741 - dice_coef: 0.7749 - jacard_coef: 0.6338 - val_loss: -0.7057 - val_dice_coef: 0.7121 - val_jacard_coef: 0.5544 - lr: 1.0000e-05\n",
            "Epoch 66/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7753 - dice_coef: 0.7750 - jacard_coef: 0.6337\n",
            "Epoch 66: val_loss did not improve from -0.70574\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.7753 - dice_coef: 0.7750 - jacard_coef: 0.6337 - val_loss: -0.7024 - val_dice_coef: 0.7097 - val_jacard_coef: 0.5514 - lr: 1.0000e-05\n",
            "Epoch 67/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7763 - dice_coef: 0.7722 - jacard_coef: 0.6315\n",
            "Epoch 67: val_loss did not improve from -0.70574\n",
            "100/100 [==============================] - 65s 651ms/step - loss: -0.7763 - dice_coef: 0.7722 - jacard_coef: 0.6315 - val_loss: -0.7049 - val_dice_coef: 0.7107 - val_jacard_coef: 0.5528 - lr: 1.0000e-05\n",
            "Epoch 68/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7736 - dice_coef: 0.7738 - jacard_coef: 0.6327\n",
            "Epoch 68: val_loss did not improve from -0.70574\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.7736 - dice_coef: 0.7738 - jacard_coef: 0.6327 - val_loss: -0.7033 - val_dice_coef: 0.7099 - val_jacard_coef: 0.5516 - lr: 1.0000e-05\n",
            "Epoch 69/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7751 - dice_coef: 0.7756 - jacard_coef: 0.6354\n",
            "Epoch 69: val_loss improved from -0.70574 to -0.70620, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 68s 680ms/step - loss: -0.7751 - dice_coef: 0.7756 - jacard_coef: 0.6354 - val_loss: -0.7062 - val_dice_coef: 0.7128 - val_jacard_coef: 0.5550 - lr: 1.0000e-05\n",
            "Epoch 70/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7774 - dice_coef: 0.7760 - jacard_coef: 0.6356\n",
            "Epoch 70: val_loss improved from -0.70620 to -0.70653, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 68s 676ms/step - loss: -0.7774 - dice_coef: 0.7760 - jacard_coef: 0.6356 - val_loss: -0.7065 - val_dice_coef: 0.7123 - val_jacard_coef: 0.5545 - lr: 1.0000e-05\n",
            "Epoch 71/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7800 - dice_coef: 0.7770 - jacard_coef: 0.6376\n",
            "Epoch 71: val_loss improved from -0.70653 to -0.70766, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 68s 678ms/step - loss: -0.7800 - dice_coef: 0.7770 - jacard_coef: 0.6376 - val_loss: -0.7077 - val_dice_coef: 0.7131 - val_jacard_coef: 0.5555 - lr: 1.0000e-05\n",
            "Epoch 72/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7786 - dice_coef: 0.7743 - jacard_coef: 0.6356\n",
            "Epoch 72: val_loss did not improve from -0.70766\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.7786 - dice_coef: 0.7743 - jacard_coef: 0.6356 - val_loss: -0.7071 - val_dice_coef: 0.7130 - val_jacard_coef: 0.5553 - lr: 1.0000e-05\n",
            "Epoch 73/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7829 - dice_coef: 0.7833 - jacard_coef: 0.6452\n",
            "Epoch 73: val_loss did not improve from -0.70766\n",
            "100/100 [==============================] - 65s 649ms/step - loss: -0.7829 - dice_coef: 0.7833 - jacard_coef: 0.6452 - val_loss: -0.7013 - val_dice_coef: 0.7077 - val_jacard_coef: 0.5487 - lr: 1.0000e-05\n",
            "Epoch 74/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7820 - dice_coef: 0.7820 - jacard_coef: 0.6436\n",
            "Epoch 74: val_loss did not improve from -0.70766\n",
            "100/100 [==============================] - 65s 650ms/step - loss: -0.7820 - dice_coef: 0.7820 - jacard_coef: 0.6436 - val_loss: -0.7021 - val_dice_coef: 0.7089 - val_jacard_coef: 0.5504 - lr: 1.0000e-05\n",
            "Epoch 75/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7838 - dice_coef: 0.7805 - jacard_coef: 0.6427\n",
            "Epoch 75: val_loss did not improve from -0.70766\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.7838 - dice_coef: 0.7805 - jacard_coef: 0.6427 - val_loss: -0.7054 - val_dice_coef: 0.7120 - val_jacard_coef: 0.5542 - lr: 1.0000e-05\n",
            "Epoch 76/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7856 - dice_coef: 0.7829 - jacard_coef: 0.6454\n",
            "Epoch 76: val_loss did not improve from -0.70766\n",
            "100/100 [==============================] - 65s 654ms/step - loss: -0.7856 - dice_coef: 0.7829 - jacard_coef: 0.6454 - val_loss: -0.7049 - val_dice_coef: 0.7119 - val_jacard_coef: 0.5540 - lr: 1.0000e-05\n",
            "Epoch 77/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7881 - dice_coef: 0.7882 - jacard_coef: 0.6517\n",
            "Epoch 77: val_loss did not improve from -0.70766\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.7881 - dice_coef: 0.7882 - jacard_coef: 0.6517 - val_loss: -0.7012 - val_dice_coef: 0.7088 - val_jacard_coef: 0.5505 - lr: 1.0000e-05\n",
            "Epoch 78/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7862 - dice_coef: 0.7864 - jacard_coef: 0.6496\n",
            "Epoch 78: val_loss did not improve from -0.70766\n",
            "100/100 [==============================] - 65s 652ms/step - loss: -0.7862 - dice_coef: 0.7864 - jacard_coef: 0.6496 - val_loss: -0.6988 - val_dice_coef: 0.7065 - val_jacard_coef: 0.5476 - lr: 1.0000e-05\n",
            "Epoch 79/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7902 - dice_coef: 0.7890 - jacard_coef: 0.6529\n",
            "Epoch 79: val_loss did not improve from -0.70766\n",
            "100/100 [==============================] - 65s 651ms/step - loss: -0.7902 - dice_coef: 0.7890 - jacard_coef: 0.6529 - val_loss: -0.7039 - val_dice_coef: 0.7109 - val_jacard_coef: 0.5528 - lr: 1.0000e-05\n",
            "Epoch 80/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7922 - dice_coef: 0.7901 - jacard_coef: 0.6546\n",
            "Epoch 80: val_loss did not improve from -0.70766\n",
            "100/100 [==============================] - 65s 649ms/step - loss: -0.7922 - dice_coef: 0.7901 - jacard_coef: 0.6546 - val_loss: -0.7021 - val_dice_coef: 0.7090 - val_jacard_coef: 0.5507 - lr: 1.0000e-05\n",
            "Epoch 81/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7945 - dice_coef: 0.7932 - jacard_coef: 0.6585\n",
            "Epoch 81: val_loss did not improve from -0.70766\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.7945 - dice_coef: 0.7932 - jacard_coef: 0.6585 - val_loss: -0.7051 - val_dice_coef: 0.7125 - val_jacard_coef: 0.5547 - lr: 1.0000e-05\n",
            "Epoch 82/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7939 - dice_coef: 0.7943 - jacard_coef: 0.6604\n",
            "Epoch 82: val_loss did not improve from -0.70766\n",
            "100/100 [==============================] - 65s 652ms/step - loss: -0.7939 - dice_coef: 0.7943 - jacard_coef: 0.6604 - val_loss: -0.7066 - val_dice_coef: 0.7133 - val_jacard_coef: 0.5556 - lr: 1.0000e-05\n",
            "Epoch 83/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7981 - dice_coef: 0.7975 - jacard_coef: 0.6640\n",
            "Epoch 83: val_loss improved from -0.70766 to -0.70971, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 68s 677ms/step - loss: -0.7981 - dice_coef: 0.7975 - jacard_coef: 0.6640 - val_loss: -0.7097 - val_dice_coef: 0.7172 - val_jacard_coef: 0.5606 - lr: 1.0000e-05\n",
            "Epoch 84/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7959 - dice_coef: 0.7961 - jacard_coef: 0.6629\n",
            "Epoch 84: val_loss did not improve from -0.70971\n",
            "100/100 [==============================] - 66s 655ms/step - loss: -0.7959 - dice_coef: 0.7961 - jacard_coef: 0.6629 - val_loss: -0.7077 - val_dice_coef: 0.7135 - val_jacard_coef: 0.5558 - lr: 1.0000e-05\n",
            "Epoch 85/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.8003 - dice_coef: 0.8002 - jacard_coef: 0.6678\n",
            "Epoch 85: val_loss did not improve from -0.70971\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.8003 - dice_coef: 0.8002 - jacard_coef: 0.6678 - val_loss: -0.7080 - val_dice_coef: 0.7147 - val_jacard_coef: 0.5574 - lr: 1.0000e-05\n",
            "Epoch 86/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7998 - dice_coef: 0.7995 - jacard_coef: 0.6672\n",
            "Epoch 86: val_loss did not improve from -0.70971\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.7998 - dice_coef: 0.7995 - jacard_coef: 0.6672 - val_loss: -0.7091 - val_dice_coef: 0.7160 - val_jacard_coef: 0.5591 - lr: 1.0000e-05\n",
            "Epoch 87/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7993 - dice_coef: 0.7979 - jacard_coef: 0.6653\n",
            "Epoch 87: val_loss did not improve from -0.70971\n",
            "100/100 [==============================] - 65s 652ms/step - loss: -0.7993 - dice_coef: 0.7979 - jacard_coef: 0.6653 - val_loss: -0.7078 - val_dice_coef: 0.7146 - val_jacard_coef: 0.5573 - lr: 1.0000e-05\n",
            "Epoch 88/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.8034 - dice_coef: 0.7968 - jacard_coef: 0.6672\n",
            "Epoch 88: val_loss improved from -0.70971 to -0.71124, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 68s 679ms/step - loss: -0.8034 - dice_coef: 0.7968 - jacard_coef: 0.6672 - val_loss: -0.7112 - val_dice_coef: 0.7185 - val_jacard_coef: 0.5620 - lr: 1.0000e-05\n",
            "Epoch 89/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.8041 - dice_coef: 0.8036 - jacard_coef: 0.6725\n",
            "Epoch 89: val_loss did not improve from -0.71124\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.8041 - dice_coef: 0.8036 - jacard_coef: 0.6725 - val_loss: -0.7070 - val_dice_coef: 0.7138 - val_jacard_coef: 0.5562 - lr: 1.0000e-05\n",
            "Epoch 90/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.8015 - dice_coef: 0.8016 - jacard_coef: 0.6702\n",
            "Epoch 90: val_loss did not improve from -0.71124\n",
            "100/100 [==============================] - 65s 650ms/step - loss: -0.8015 - dice_coef: 0.8016 - jacard_coef: 0.6702 - val_loss: -0.7030 - val_dice_coef: 0.7114 - val_jacard_coef: 0.5536 - lr: 1.0000e-05\n",
            "Epoch 91/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.8036 - dice_coef: 0.8003 - jacard_coef: 0.6696\n",
            "Epoch 91: val_loss improved from -0.71124 to -0.71263, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 68s 680ms/step - loss: -0.8036 - dice_coef: 0.8003 - jacard_coef: 0.6696 - val_loss: -0.7126 - val_dice_coef: 0.7200 - val_jacard_coef: 0.5639 - lr: 1.0000e-05\n",
            "Epoch 92/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.8037 - dice_coef: 0.7977 - jacard_coef: 0.6683\n",
            "Epoch 92: val_loss did not improve from -0.71263\n",
            "100/100 [==============================] - 65s 654ms/step - loss: -0.8037 - dice_coef: 0.7977 - jacard_coef: 0.6683 - val_loss: -0.7039 - val_dice_coef: 0.7121 - val_jacard_coef: 0.5543 - lr: 1.0000e-05\n",
            "Epoch 93/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.8077 - dice_coef: 0.8045 - jacard_coef: 0.6752\n",
            "Epoch 93: val_loss did not improve from -0.71263\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.8077 - dice_coef: 0.8045 - jacard_coef: 0.6752 - val_loss: -0.7056 - val_dice_coef: 0.7131 - val_jacard_coef: 0.5557 - lr: 1.0000e-05\n",
            "Epoch 94/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.8084 - dice_coef: 0.8089 - jacard_coef: 0.6802\n",
            "Epoch 94: val_loss did not improve from -0.71263\n",
            "100/100 [==============================] - 65s 651ms/step - loss: -0.8084 - dice_coef: 0.8089 - jacard_coef: 0.6802 - val_loss: -0.7004 - val_dice_coef: 0.7084 - val_jacard_coef: 0.5498 - lr: 1.0000e-05\n",
            "Epoch 95/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.8088 - dice_coef: 0.8058 - jacard_coef: 0.6768\n",
            "Epoch 95: val_loss did not improve from -0.71263\n",
            "100/100 [==============================] - 65s 652ms/step - loss: -0.8088 - dice_coef: 0.8058 - jacard_coef: 0.6768 - val_loss: -0.7109 - val_dice_coef: 0.7182 - val_jacard_coef: 0.5616 - lr: 1.0000e-05\n",
            "Epoch 96/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.8102 - dice_coef: 0.8103 - jacard_coef: 0.6823\n",
            "Epoch 96: val_loss did not improve from -0.71263\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.8102 - dice_coef: 0.8103 - jacard_coef: 0.6823 - val_loss: -0.7122 - val_dice_coef: 0.7199 - val_jacard_coef: 0.5638 - lr: 1.0000e-05\n",
            "Epoch 97/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.8113 - dice_coef: 0.8111 - jacard_coef: 0.6839\n",
            "Epoch 97: val_loss did not improve from -0.71263\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.8113 - dice_coef: 0.8111 - jacard_coef: 0.6839 - val_loss: -0.7037 - val_dice_coef: 0.7123 - val_jacard_coef: 0.5545 - lr: 1.0000e-05\n",
            "Epoch 98/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.8134 - dice_coef: 0.8131 - jacard_coef: 0.6860\n",
            "Epoch 98: val_loss did not improve from -0.71263\n",
            "100/100 [==============================] - 65s 651ms/step - loss: -0.8134 - dice_coef: 0.8131 - jacard_coef: 0.6860 - val_loss: -0.7048 - val_dice_coef: 0.7129 - val_jacard_coef: 0.5555 - lr: 1.0000e-05\n",
            "Epoch 99/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.8154 - dice_coef: 0.8136 - jacard_coef: 0.6868\n",
            "Epoch 99: val_loss improved from -0.71263 to -0.71330, saving model to weights/unet_64_adam_dice+jacard_8.h5\n",
            "100/100 [==============================] - 68s 677ms/step - loss: -0.8154 - dice_coef: 0.8136 - jacard_coef: 0.6868 - val_loss: -0.7133 - val_dice_coef: 0.7200 - val_jacard_coef: 0.5639 - lr: 1.0000e-05\n",
            "Epoch 100/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.8165 - dice_coef: 0.8158 - jacard_coef: 0.6898\n",
            "Epoch 100: val_loss did not improve from -0.71330\n",
            "100/100 [==============================] - 66s 655ms/step - loss: -0.8165 - dice_coef: 0.8158 - jacard_coef: 0.6898 - val_loss: -0.7094 - val_dice_coef: 0.7161 - val_jacard_coef: 0.5592 - lr: 1.0000e-05\n"
          ]
        }
      ],
      "source": [
        "# Adam, Dice+Jacard, batch_size=8\n",
        "logs_p = 'logs/unet_64_adam_dice+jacard_8.csv'\n",
        "weights_p = 'weights/unet_64_adam_dice+jacard_8.h5'\n",
        "\n",
        "checkpointer = ModelCheckpoint(weights_p, verbose=1, save_best_only=True)\n",
        "callbacks_f = [earlystopper, checkpointer, reduce_lr, CSVLogger(logs_p, append=True, separator=';')]\n",
        "optimizer_f = adam\n",
        "loss_f = [dice_coef_loss, jacard_coef_loss]\n",
        "metrics_f = [dice_coef, jacard_coef,]\n",
        "#batch_s = 8 * tpu_strategy.num_replicas_in_sync # with TPU\n",
        "batch_s = 8\n",
        "epochs_s = 100\n",
        "\n",
        "#with tpu_strategy.scope():  # with TPU\n",
        "#  model = unet_model(input=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),)\n",
        "model = unet_model(input=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),)\n",
        "model.compile(optimizer=optimizer_f, loss=loss_f, metrics=metrics_f)\n",
        "\n",
        "results = model.fit(x=X_train, y=Y_train.astype(np.float32), validation_data=(X_val, Y_val.astype(np.float32)), batch_size=batch_s, \n",
        "                    epochs=epochs_s, callbacks=callbacks_f, verbose=1,)\n",
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZYV3v7Jcs64"
      },
      "source": [
        "### SGD, DICE + Jaccard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYRFYAKAZF01",
        "outputId": "8457395f-cfd8-4eca-d135-2c13f8f2eb18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.3829 - dice_coef: 0.3849 - jacard_coef: 0.2434\n",
            "Epoch 1: val_loss improved from inf to -0.11259, saving model to weights/unet_64_sgd_dice+jacard_8.h5\n",
            "100/100 [==============================] - 88s 756ms/step - loss: -0.3829 - dice_coef: 0.3849 - jacard_coef: 0.2434 - val_loss: -0.1126 - val_dice_coef: 0.1108 - val_jacard_coef: 0.0587 - lr: 0.0020\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.5501 - dice_coef: 0.5513 - jacard_coef: 0.3828\n",
            "Epoch 2: val_loss did not improve from -0.11259\n",
            "100/100 [==============================] - 66s 655ms/step - loss: -0.5501 - dice_coef: 0.5513 - jacard_coef: 0.3828 - val_loss: -0.0894 - val_dice_coef: 0.0890 - val_jacard_coef: 0.0466 - lr: 0.0020\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.6204 - dice_coef: 0.6210 - jacard_coef: 0.4519\n",
            "Epoch 3: val_loss improved from -0.11259 to -0.44308, saving model to weights/unet_64_sgd_dice+jacard_8.h5\n",
            "100/100 [==============================] - 66s 660ms/step - loss: -0.6204 - dice_coef: 0.6210 - jacard_coef: 0.4519 - val_loss: -0.4431 - val_dice_coef: 0.4491 - val_jacard_coef: 0.2903 - lr: 0.0020\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.6598 - dice_coef: 0.6604 - jacard_coef: 0.4943\n",
            "Epoch 4: val_loss improved from -0.44308 to -0.67106, saving model to weights/unet_64_sgd_dice+jacard_8.h5\n",
            "100/100 [==============================] - 66s 663ms/step - loss: -0.6598 - dice_coef: 0.6604 - jacard_coef: 0.4943 - val_loss: -0.6711 - val_dice_coef: 0.6816 - val_jacard_coef: 0.5187 - lr: 0.0020\n",
            "Epoch 5/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.6906 - dice_coef: 0.6895 - jacard_coef: 0.5273\n",
            "Epoch 5: val_loss improved from -0.67106 to -0.70775, saving model to weights/unet_64_sgd_dice+jacard_8.h5\n",
            "100/100 [==============================] - 66s 663ms/step - loss: -0.6906 - dice_coef: 0.6895 - jacard_coef: 0.5273 - val_loss: -0.7078 - val_dice_coef: 0.7150 - val_jacard_coef: 0.5584 - lr: 0.0020\n",
            "Epoch 6/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7063 - dice_coef: 0.7058 - jacard_coef: 0.5463\n",
            "Epoch 6: val_loss improved from -0.70775 to -0.72486, saving model to weights/unet_64_sgd_dice+jacard_8.h5\n",
            "100/100 [==============================] - 66s 664ms/step - loss: -0.7063 - dice_coef: 0.7058 - jacard_coef: 0.5463 - val_loss: -0.7249 - val_dice_coef: 0.7316 - val_jacard_coef: 0.5790 - lr: 0.0020\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7178 - dice_coef: 0.7121 - jacard_coef: 0.5563\n",
            "Epoch 7: val_loss improved from -0.72486 to -0.73119, saving model to weights/unet_64_sgd_dice+jacard_8.h5\n",
            "100/100 [==============================] - 67s 665ms/step - loss: -0.7178 - dice_coef: 0.7121 - jacard_coef: 0.5563 - val_loss: -0.7312 - val_dice_coef: 0.7385 - val_jacard_coef: 0.5873 - lr: 0.0020\n",
            "Epoch 8/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7240 - dice_coef: 0.7240 - jacard_coef: 0.5683\n",
            "Epoch 8: val_loss improved from -0.73119 to -0.73768, saving model to weights/unet_64_sgd_dice+jacard_8.h5\n",
            "100/100 [==============================] - 66s 662ms/step - loss: -0.7240 - dice_coef: 0.7240 - jacard_coef: 0.5683 - val_loss: -0.7377 - val_dice_coef: 0.7445 - val_jacard_coef: 0.5944 - lr: 0.0020\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7294 - dice_coef: 0.7303 - jacard_coef: 0.5759\n",
            "Epoch 9: val_loss improved from -0.73768 to -0.74293, saving model to weights/unet_64_sgd_dice+jacard_8.h5\n",
            "100/100 [==============================] - 66s 662ms/step - loss: -0.7294 - dice_coef: 0.7303 - jacard_coef: 0.5759 - val_loss: -0.7429 - val_dice_coef: 0.7485 - val_jacard_coef: 0.5996 - lr: 0.0020\n",
            "Epoch 10/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7344 - dice_coef: 0.7336 - jacard_coef: 0.5798\n",
            "Epoch 10: val_loss did not improve from -0.74293\n",
            "100/100 [==============================] - 66s 656ms/step - loss: -0.7344 - dice_coef: 0.7336 - jacard_coef: 0.5798 - val_loss: -0.7375 - val_dice_coef: 0.7404 - val_jacard_coef: 0.5895 - lr: 0.0020\n",
            "Epoch 11/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7377 - dice_coef: 0.7385 - jacard_coef: 0.5862\n",
            "Epoch 11: val_loss improved from -0.74293 to -0.74330, saving model to weights/unet_64_sgd_dice+jacard_8.h5\n",
            "100/100 [==============================] - 66s 663ms/step - loss: -0.7377 - dice_coef: 0.7385 - jacard_coef: 0.5862 - val_loss: -0.7433 - val_dice_coef: 0.7492 - val_jacard_coef: 0.6003 - lr: 0.0020\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7400 - dice_coef: 0.7404 - jacard_coef: 0.5884\n",
            "Epoch 12: val_loss improved from -0.74330 to -0.75164, saving model to weights/unet_64_sgd_dice+jacard_8.h5\n",
            "100/100 [==============================] - 66s 662ms/step - loss: -0.7400 - dice_coef: 0.7404 - jacard_coef: 0.5884 - val_loss: -0.7516 - val_dice_coef: 0.7563 - val_jacard_coef: 0.6098 - lr: 0.0020\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7392 - dice_coef: 0.7398 - jacard_coef: 0.5876\n",
            "Epoch 13: val_loss did not improve from -0.75164\n",
            "100/100 [==============================] - 66s 655ms/step - loss: -0.7392 - dice_coef: 0.7398 - jacard_coef: 0.5876 - val_loss: -0.7502 - val_dice_coef: 0.7577 - val_jacard_coef: 0.6112 - lr: 0.0020\n",
            "Epoch 14/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7426 - dice_coef: 0.7426 - jacard_coef: 0.5911\n",
            "Epoch 14: val_loss did not improve from -0.75164\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.7426 - dice_coef: 0.7426 - jacard_coef: 0.5911 - val_loss: -0.7453 - val_dice_coef: 0.7502 - val_jacard_coef: 0.6018 - lr: 0.0020\n",
            "Epoch 15/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7439 - dice_coef: 0.7444 - jacard_coef: 0.5933\n",
            "Epoch 15: val_loss improved from -0.75164 to -0.75316, saving model to weights/unet_64_sgd_dice+jacard_8.h5\n",
            "100/100 [==============================] - 66s 663ms/step - loss: -0.7439 - dice_coef: 0.7444 - jacard_coef: 0.5933 - val_loss: -0.7532 - val_dice_coef: 0.7612 - val_jacard_coef: 0.6164 - lr: 0.0020\n",
            "Epoch 16/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7453 - dice_coef: 0.7453 - jacard_coef: 0.5946\n",
            "Epoch 16: val_loss did not improve from -0.75316\n",
            "100/100 [==============================] - 65s 655ms/step - loss: -0.7453 - dice_coef: 0.7453 - jacard_coef: 0.5946 - val_loss: -0.7480 - val_dice_coef: 0.7558 - val_jacard_coef: 0.6091 - lr: 0.0020\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7478 - dice_coef: 0.7480 - jacard_coef: 0.5979\n",
            "Epoch 17: val_loss did not improve from -0.75316\n",
            "100/100 [==============================] - 65s 655ms/step - loss: -0.7478 - dice_coef: 0.7480 - jacard_coef: 0.5979 - val_loss: -0.7466 - val_dice_coef: 0.7556 - val_jacard_coef: 0.6092 - lr: 0.0020\n",
            "Epoch 18/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7491 - dice_coef: 0.7493 - jacard_coef: 0.5996\n",
            "Epoch 18: val_loss did not improve from -0.75316\n",
            "100/100 [==============================] - 65s 654ms/step - loss: -0.7491 - dice_coef: 0.7493 - jacard_coef: 0.5996 - val_loss: -0.7528 - val_dice_coef: 0.7594 - val_jacard_coef: 0.6137 - lr: 0.0020\n",
            "Epoch 19/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7499 - dice_coef: 0.7500 - jacard_coef: 0.6005\n",
            "Epoch 19: val_loss did not improve from -0.75316\n",
            "\n",
            "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "100/100 [==============================] - 65s 651ms/step - loss: -0.7499 - dice_coef: 0.7500 - jacard_coef: 0.6005 - val_loss: -0.7528 - val_dice_coef: 0.7600 - val_jacard_coef: 0.6142 - lr: 0.0020\n",
            "Epoch 20/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7505 - dice_coef: 0.7513 - jacard_coef: 0.6021\n",
            "Epoch 20: val_loss improved from -0.75316 to -0.75338, saving model to weights/unet_64_sgd_dice+jacard_8.h5\n",
            "100/100 [==============================] - 66s 659ms/step - loss: -0.7505 - dice_coef: 0.7513 - jacard_coef: 0.6021 - val_loss: -0.7534 - val_dice_coef: 0.7598 - val_jacard_coef: 0.6141 - lr: 2.0000e-04\n",
            "Epoch 21/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7509 - dice_coef: 0.7510 - jacard_coef: 0.6019\n",
            "Epoch 21: val_loss improved from -0.75338 to -0.75475, saving model to weights/unet_64_sgd_dice+jacard_8.h5\n",
            "100/100 [==============================] - 66s 660ms/step - loss: -0.7509 - dice_coef: 0.7510 - jacard_coef: 0.6019 - val_loss: -0.7547 - val_dice_coef: 0.7610 - val_jacard_coef: 0.6160 - lr: 2.0000e-04\n",
            "Epoch 22/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7511 - dice_coef: 0.7493 - jacard_coef: 0.5998\n",
            "Epoch 22: val_loss did not improve from -0.75475\n",
            "100/100 [==============================] - 66s 655ms/step - loss: -0.7511 - dice_coef: 0.7493 - jacard_coef: 0.5998 - val_loss: -0.7541 - val_dice_coef: 0.7615 - val_jacard_coef: 0.6165 - lr: 2.0000e-04\n",
            "Epoch 23/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7520 - dice_coef: 0.7503 - jacard_coef: 0.6012\n",
            "Epoch 23: val_loss did not improve from -0.75475\n",
            "100/100 [==============================] - 65s 654ms/step - loss: -0.7520 - dice_coef: 0.7503 - jacard_coef: 0.6012 - val_loss: -0.7541 - val_dice_coef: 0.7615 - val_jacard_coef: 0.6165 - lr: 2.0000e-04\n",
            "Epoch 24/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7517 - dice_coef: 0.7517 - jacard_coef: 0.6025\n",
            "Epoch 24: val_loss did not improve from -0.75475\n",
            "100/100 [==============================] - 65s 650ms/step - loss: -0.7517 - dice_coef: 0.7517 - jacard_coef: 0.6025 - val_loss: -0.7536 - val_dice_coef: 0.7612 - val_jacard_coef: 0.6160 - lr: 2.0000e-04\n",
            "Epoch 25/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7522 - dice_coef: 0.7521 - jacard_coef: 0.6032\n",
            "Epoch 25: val_loss did not improve from -0.75475\n",
            "\n",
            "Epoch 25: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
            "100/100 [==============================] - 65s 652ms/step - loss: -0.7522 - dice_coef: 0.7521 - jacard_coef: 0.6032 - val_loss: -0.7531 - val_dice_coef: 0.7607 - val_jacard_coef: 0.6155 - lr: 2.0000e-04\n",
            "Epoch 26/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7536 - dice_coef: 0.7535 - jacard_coef: 0.6049\n",
            "Epoch 26: val_loss did not improve from -0.75475\n",
            "100/100 [==============================] - 65s 652ms/step - loss: -0.7536 - dice_coef: 0.7535 - jacard_coef: 0.6049 - val_loss: -0.7530 - val_dice_coef: 0.7607 - val_jacard_coef: 0.6154 - lr: 2.0000e-05\n",
            "Epoch 27/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7522 - dice_coef: 0.7520 - jacard_coef: 0.6031\n",
            "Epoch 27: val_loss did not improve from -0.75475\n",
            "100/100 [==============================] - 65s 654ms/step - loss: -0.7522 - dice_coef: 0.7520 - jacard_coef: 0.6031 - val_loss: -0.7538 - val_dice_coef: 0.7613 - val_jacard_coef: 0.6162 - lr: 2.0000e-05\n",
            "Epoch 28/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7516 - dice_coef: 0.7521 - jacard_coef: 0.6032\n",
            "Epoch 28: val_loss did not improve from -0.75475\n",
            "100/100 [==============================] - 65s 655ms/step - loss: -0.7516 - dice_coef: 0.7521 - jacard_coef: 0.6032 - val_loss: -0.7542 - val_dice_coef: 0.7617 - val_jacard_coef: 0.6166 - lr: 2.0000e-05\n",
            "Epoch 29/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7527 - dice_coef: 0.7526 - jacard_coef: 0.6037\n",
            "Epoch 29: val_loss did not improve from -0.75475\n",
            "\n",
            "Epoch 29: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.7527 - dice_coef: 0.7526 - jacard_coef: 0.6037 - val_loss: -0.7545 - val_dice_coef: 0.7619 - val_jacard_coef: 0.6169 - lr: 2.0000e-05\n",
            "Epoch 30/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7527 - dice_coef: 0.7524 - jacard_coef: 0.6037\n",
            "Epoch 30: val_loss did not improve from -0.75475\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.7527 - dice_coef: 0.7524 - jacard_coef: 0.6037 - val_loss: -0.7546 - val_dice_coef: 0.7619 - val_jacard_coef: 0.6170 - lr: 1.0000e-05\n",
            "Epoch 31/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7525 - dice_coef: 0.7522 - jacard_coef: 0.6034\n",
            "Epoch 31: val_loss did not improve from -0.75475\n",
            "100/100 [==============================] - 65s 654ms/step - loss: -0.7525 - dice_coef: 0.7522 - jacard_coef: 0.6034 - val_loss: -0.7547 - val_dice_coef: 0.7621 - val_jacard_coef: 0.6172 - lr: 1.0000e-05\n",
            "Epoch 32/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7534 - dice_coef: 0.7538 - jacard_coef: 0.6053\n",
            "Epoch 32: val_loss did not improve from -0.75475\n",
            "100/100 [==============================] - 65s 655ms/step - loss: -0.7534 - dice_coef: 0.7538 - jacard_coef: 0.6053 - val_loss: -0.7546 - val_dice_coef: 0.7620 - val_jacard_coef: 0.6171 - lr: 1.0000e-05\n",
            "Epoch 33/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7517 - dice_coef: 0.7519 - jacard_coef: 0.6030\n",
            "Epoch 33: val_loss did not improve from -0.75475\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.7517 - dice_coef: 0.7519 - jacard_coef: 0.6030 - val_loss: -0.7544 - val_dice_coef: 0.7617 - val_jacard_coef: 0.6167 - lr: 1.0000e-05\n",
            "Epoch 34/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7525 - dice_coef: 0.7525 - jacard_coef: 0.6037\n",
            "Epoch 34: val_loss improved from -0.75475 to -0.75479, saving model to weights/unet_64_sgd_dice+jacard_8.h5\n",
            "100/100 [==============================] - 67s 667ms/step - loss: -0.7525 - dice_coef: 0.7525 - jacard_coef: 0.6037 - val_loss: -0.7548 - val_dice_coef: 0.7621 - val_jacard_coef: 0.6172 - lr: 1.0000e-05\n",
            "Epoch 35/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7533 - dice_coef: 0.7533 - jacard_coef: 0.6047\n",
            "Epoch 35: val_loss did not improve from -0.75479\n",
            "100/100 [==============================] - 65s 652ms/step - loss: -0.7533 - dice_coef: 0.7533 - jacard_coef: 0.6047 - val_loss: -0.7546 - val_dice_coef: 0.7619 - val_jacard_coef: 0.6170 - lr: 1.0000e-05\n",
            "Epoch 36/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7536 - dice_coef: 0.7539 - jacard_coef: 0.6053\n",
            "Epoch 36: val_loss did not improve from -0.75479\n",
            "100/100 [==============================] - 65s 654ms/step - loss: -0.7536 - dice_coef: 0.7539 - jacard_coef: 0.6053 - val_loss: -0.7545 - val_dice_coef: 0.7618 - val_jacard_coef: 0.6168 - lr: 1.0000e-05\n",
            "Epoch 37/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7532 - dice_coef: 0.7536 - jacard_coef: 0.6050\n",
            "Epoch 37: val_loss did not improve from -0.75479\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.7532 - dice_coef: 0.7536 - jacard_coef: 0.6050 - val_loss: -0.7547 - val_dice_coef: 0.7620 - val_jacard_coef: 0.6170 - lr: 1.0000e-05\n",
            "Epoch 38/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7529 - dice_coef: 0.7528 - jacard_coef: 0.6040\n",
            "Epoch 38: val_loss improved from -0.75479 to -0.75481, saving model to weights/unet_64_sgd_dice+jacard_8.h5\n",
            "100/100 [==============================] - 66s 663ms/step - loss: -0.7529 - dice_coef: 0.7528 - jacard_coef: 0.6040 - val_loss: -0.7548 - val_dice_coef: 0.7621 - val_jacard_coef: 0.6172 - lr: 1.0000e-05\n",
            "Epoch 39/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7529 - dice_coef: 0.7529 - jacard_coef: 0.6041\n",
            "Epoch 39: val_loss did not improve from -0.75481\n",
            "100/100 [==============================] - 65s 654ms/step - loss: -0.7529 - dice_coef: 0.7529 - jacard_coef: 0.6041 - val_loss: -0.7546 - val_dice_coef: 0.7619 - val_jacard_coef: 0.6169 - lr: 1.0000e-05\n",
            "Epoch 40/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7533 - dice_coef: 0.7534 - jacard_coef: 0.6048\n",
            "Epoch 40: val_loss improved from -0.75481 to -0.75483, saving model to weights/unet_64_sgd_dice+jacard_8.h5\n",
            "100/100 [==============================] - 66s 661ms/step - loss: -0.7533 - dice_coef: 0.7534 - jacard_coef: 0.6048 - val_loss: -0.7548 - val_dice_coef: 0.7621 - val_jacard_coef: 0.6172 - lr: 1.0000e-05\n",
            "Epoch 41/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7525 - dice_coef: 0.7527 - jacard_coef: 0.6039\n",
            "Epoch 41: val_loss improved from -0.75483 to -0.75493, saving model to weights/unet_64_sgd_dice+jacard_8.h5\n",
            "100/100 [==============================] - 66s 663ms/step - loss: -0.7525 - dice_coef: 0.7527 - jacard_coef: 0.6039 - val_loss: -0.7549 - val_dice_coef: 0.7622 - val_jacard_coef: 0.6173 - lr: 1.0000e-05\n",
            "Epoch 42/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7536 - dice_coef: 0.7529 - jacard_coef: 0.6042\n",
            "Epoch 42: val_loss improved from -0.75493 to -0.75511, saving model to weights/unet_64_sgd_dice+jacard_8.h5\n",
            "100/100 [==============================] - 66s 661ms/step - loss: -0.7536 - dice_coef: 0.7529 - jacard_coef: 0.6042 - val_loss: -0.7551 - val_dice_coef: 0.7624 - val_jacard_coef: 0.6176 - lr: 1.0000e-05\n",
            "Epoch 43/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7531 - dice_coef: 0.7529 - jacard_coef: 0.6041\n",
            "Epoch 43: val_loss did not improve from -0.75511\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.7531 - dice_coef: 0.7529 - jacard_coef: 0.6041 - val_loss: -0.7550 - val_dice_coef: 0.7622 - val_jacard_coef: 0.6174 - lr: 1.0000e-05\n",
            "Epoch 44/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7512 - dice_coef: 0.7509 - jacard_coef: 0.6017\n",
            "Epoch 44: val_loss did not improve from -0.75511\n",
            "100/100 [==============================] - 65s 652ms/step - loss: -0.7512 - dice_coef: 0.7509 - jacard_coef: 0.6017 - val_loss: -0.7551 - val_dice_coef: 0.7623 - val_jacard_coef: 0.6175 - lr: 1.0000e-05\n",
            "Epoch 45/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7520 - dice_coef: 0.7501 - jacard_coef: 0.6010\n",
            "Epoch 45: val_loss improved from -0.75511 to -0.75533, saving model to weights/unet_64_sgd_dice+jacard_8.h5\n",
            "100/100 [==============================] - 66s 660ms/step - loss: -0.7520 - dice_coef: 0.7501 - jacard_coef: 0.6010 - val_loss: -0.7553 - val_dice_coef: 0.7625 - val_jacard_coef: 0.6178 - lr: 1.0000e-05\n",
            "Epoch 46/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7523 - dice_coef: 0.7501 - jacard_coef: 0.6011\n",
            "Epoch 46: val_loss improved from -0.75533 to -0.75550, saving model to weights/unet_64_sgd_dice+jacard_8.h5\n",
            "100/100 [==============================] - 66s 662ms/step - loss: -0.7523 - dice_coef: 0.7501 - jacard_coef: 0.6011 - val_loss: -0.7555 - val_dice_coef: 0.7627 - val_jacard_coef: 0.6180 - lr: 1.0000e-05\n",
            "Epoch 47/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7529 - dice_coef: 0.7528 - jacard_coef: 0.6041\n",
            "Epoch 47: val_loss did not improve from -0.75550\n",
            "100/100 [==============================] - 66s 656ms/step - loss: -0.7529 - dice_coef: 0.7528 - jacard_coef: 0.6041 - val_loss: -0.7552 - val_dice_coef: 0.7624 - val_jacard_coef: 0.6176 - lr: 1.0000e-05\n",
            "Epoch 48/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7526 - dice_coef: 0.7496 - jacard_coef: 0.6008\n",
            "Epoch 48: val_loss did not improve from -0.75550\n",
            "100/100 [==============================] - 66s 655ms/step - loss: -0.7526 - dice_coef: 0.7496 - jacard_coef: 0.6008 - val_loss: -0.7552 - val_dice_coef: 0.7625 - val_jacard_coef: 0.6177 - lr: 1.0000e-05\n",
            "Epoch 49/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7530 - dice_coef: 0.7529 - jacard_coef: 0.6042\n",
            "Epoch 49: val_loss did not improve from -0.75550\n",
            "100/100 [==============================] - 65s 654ms/step - loss: -0.7530 - dice_coef: 0.7529 - jacard_coef: 0.6042 - val_loss: -0.7551 - val_dice_coef: 0.7624 - val_jacard_coef: 0.6176 - lr: 1.0000e-05\n",
            "Epoch 50/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7529 - dice_coef: 0.7534 - jacard_coef: 0.6049\n",
            "Epoch 50: val_loss did not improve from -0.75550\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.7529 - dice_coef: 0.7534 - jacard_coef: 0.6049 - val_loss: -0.7551 - val_dice_coef: 0.7623 - val_jacard_coef: 0.6175 - lr: 1.0000e-05\n",
            "Epoch 51/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7517 - dice_coef: 0.7468 - jacard_coef: 0.5986\n",
            "Epoch 51: val_loss did not improve from -0.75550\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.7517 - dice_coef: 0.7468 - jacard_coef: 0.5986 - val_loss: -0.7555 - val_dice_coef: 0.7627 - val_jacard_coef: 0.6179 - lr: 1.0000e-05\n",
            "Epoch 52/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7524 - dice_coef: 0.7528 - jacard_coef: 0.6040\n",
            "Epoch 52: val_loss did not improve from -0.75550\n",
            "100/100 [==============================] - 65s 655ms/step - loss: -0.7524 - dice_coef: 0.7528 - jacard_coef: 0.6040 - val_loss: -0.7553 - val_dice_coef: 0.7625 - val_jacard_coef: 0.6177 - lr: 1.0000e-05\n",
            "Epoch 53/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7524 - dice_coef: 0.7522 - jacard_coef: 0.6033\n",
            "Epoch 53: val_loss did not improve from -0.75550\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.7524 - dice_coef: 0.7522 - jacard_coef: 0.6033 - val_loss: -0.7553 - val_dice_coef: 0.7626 - val_jacard_coef: 0.6178 - lr: 1.0000e-05\n",
            "Epoch 54/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7532 - dice_coef: 0.7527 - jacard_coef: 0.6039\n",
            "Epoch 54: val_loss did not improve from -0.75550\n",
            "100/100 [==============================] - 66s 655ms/step - loss: -0.7532 - dice_coef: 0.7527 - jacard_coef: 0.6039 - val_loss: -0.7553 - val_dice_coef: 0.7626 - val_jacard_coef: 0.6178 - lr: 1.0000e-05\n",
            "Epoch 55/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7530 - dice_coef: 0.7539 - jacard_coef: 0.6055\n",
            "Epoch 55: val_loss did not improve from -0.75550\n",
            "100/100 [==============================] - 65s 655ms/step - loss: -0.7530 - dice_coef: 0.7539 - jacard_coef: 0.6055 - val_loss: -0.7553 - val_dice_coef: 0.7625 - val_jacard_coef: 0.6177 - lr: 1.0000e-05\n",
            "Epoch 56/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7517 - dice_coef: 0.7514 - jacard_coef: 0.6024\n",
            "Epoch 56: val_loss did not improve from -0.75550\n",
            "100/100 [==============================] - 65s 652ms/step - loss: -0.7517 - dice_coef: 0.7514 - jacard_coef: 0.6024 - val_loss: -0.7551 - val_dice_coef: 0.7624 - val_jacard_coef: 0.6176 - lr: 1.0000e-05\n",
            "Epoch 57/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7526 - dice_coef: 0.7525 - jacard_coef: 0.6037\n",
            "Epoch 57: val_loss did not improve from -0.75550\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.7526 - dice_coef: 0.7525 - jacard_coef: 0.6037 - val_loss: -0.7552 - val_dice_coef: 0.7624 - val_jacard_coef: 0.6176 - lr: 1.0000e-05\n",
            "Epoch 58/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7537 - dice_coef: 0.7542 - jacard_coef: 0.6059\n",
            "Epoch 58: val_loss did not improve from -0.75550\n",
            "100/100 [==============================] - 65s 655ms/step - loss: -0.7537 - dice_coef: 0.7542 - jacard_coef: 0.6059 - val_loss: -0.7551 - val_dice_coef: 0.7624 - val_jacard_coef: 0.6176 - lr: 1.0000e-05\n",
            "Epoch 59/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7539 - dice_coef: 0.7541 - jacard_coef: 0.6057\n",
            "Epoch 59: val_loss improved from -0.75550 to -0.75560, saving model to weights/unet_64_sgd_dice+jacard_8.h5\n",
            "100/100 [==============================] - 66s 661ms/step - loss: -0.7539 - dice_coef: 0.7541 - jacard_coef: 0.6057 - val_loss: -0.7556 - val_dice_coef: 0.7628 - val_jacard_coef: 0.6181 - lr: 1.0000e-05\n",
            "Epoch 60/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7529 - dice_coef: 0.7528 - jacard_coef: 0.6040\n",
            "Epoch 60: val_loss did not improve from -0.75560\n",
            "100/100 [==============================] - 65s 654ms/step - loss: -0.7529 - dice_coef: 0.7528 - jacard_coef: 0.6040 - val_loss: -0.7555 - val_dice_coef: 0.7627 - val_jacard_coef: 0.6180 - lr: 1.0000e-05\n",
            "Epoch 61/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7530 - dice_coef: 0.7507 - jacard_coef: 0.6019\n",
            "Epoch 61: val_loss did not improve from -0.75560\n",
            "100/100 [==============================] - 65s 654ms/step - loss: -0.7530 - dice_coef: 0.7507 - jacard_coef: 0.6019 - val_loss: -0.7553 - val_dice_coef: 0.7626 - val_jacard_coef: 0.6178 - lr: 1.0000e-05\n",
            "Epoch 62/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7521 - dice_coef: 0.7520 - jacard_coef: 0.6030\n",
            "Epoch 62: val_loss did not improve from -0.75560\n",
            "100/100 [==============================] - 66s 655ms/step - loss: -0.7521 - dice_coef: 0.7520 - jacard_coef: 0.6030 - val_loss: -0.7550 - val_dice_coef: 0.7623 - val_jacard_coef: 0.6174 - lr: 1.0000e-05\n",
            "Epoch 63/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7523 - dice_coef: 0.7518 - jacard_coef: 0.6029\n",
            "Epoch 63: val_loss did not improve from -0.75560\n",
            "100/100 [==============================] - 65s 652ms/step - loss: -0.7523 - dice_coef: 0.7518 - jacard_coef: 0.6029 - val_loss: -0.7553 - val_dice_coef: 0.7625 - val_jacard_coef: 0.6177 - lr: 1.0000e-05\n",
            "Epoch 64/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7532 - dice_coef: 0.7537 - jacard_coef: 0.6053\n",
            "Epoch 64: val_loss did not improve from -0.75560\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.7532 - dice_coef: 0.7537 - jacard_coef: 0.6053 - val_loss: -0.7552 - val_dice_coef: 0.7624 - val_jacard_coef: 0.6176 - lr: 1.0000e-05\n",
            "Epoch 65/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7535 - dice_coef: 0.7534 - jacard_coef: 0.6047\n",
            "Epoch 65: val_loss improved from -0.75560 to -0.75572, saving model to weights/unet_64_sgd_dice+jacard_8.h5\n",
            "100/100 [==============================] - 66s 661ms/step - loss: -0.7535 - dice_coef: 0.7534 - jacard_coef: 0.6047 - val_loss: -0.7557 - val_dice_coef: 0.7629 - val_jacard_coef: 0.6183 - lr: 1.0000e-05\n",
            "Epoch 66/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7520 - dice_coef: 0.7520 - jacard_coef: 0.6031\n",
            "Epoch 66: val_loss improved from -0.75572 to -0.75579, saving model to weights/unet_64_sgd_dice+jacard_8.h5\n",
            "100/100 [==============================] - 66s 663ms/step - loss: -0.7520 - dice_coef: 0.7520 - jacard_coef: 0.6031 - val_loss: -0.7558 - val_dice_coef: 0.7629 - val_jacard_coef: 0.6183 - lr: 1.0000e-05\n",
            "Epoch 67/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7528 - dice_coef: 0.7528 - jacard_coef: 0.6040\n",
            "Epoch 67: val_loss did not improve from -0.75579\n",
            "100/100 [==============================] - 66s 655ms/step - loss: -0.7528 - dice_coef: 0.7528 - jacard_coef: 0.6040 - val_loss: -0.7557 - val_dice_coef: 0.7629 - val_jacard_coef: 0.6182 - lr: 1.0000e-05\n",
            "Epoch 68/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7535 - dice_coef: 0.7535 - jacard_coef: 0.6048\n",
            "Epoch 68: val_loss did not improve from -0.75579\n",
            "100/100 [==============================] - 65s 652ms/step - loss: -0.7535 - dice_coef: 0.7535 - jacard_coef: 0.6048 - val_loss: -0.7555 - val_dice_coef: 0.7627 - val_jacard_coef: 0.6180 - lr: 1.0000e-05\n",
            "Epoch 69/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7533 - dice_coef: 0.7534 - jacard_coef: 0.6048\n",
            "Epoch 69: val_loss did not improve from -0.75579\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.7533 - dice_coef: 0.7534 - jacard_coef: 0.6048 - val_loss: -0.7557 - val_dice_coef: 0.7629 - val_jacard_coef: 0.6183 - lr: 1.0000e-05\n",
            "Epoch 70/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7521 - dice_coef: 0.7521 - jacard_coef: 0.6031\n",
            "Epoch 70: val_loss did not improve from -0.75579\n",
            "100/100 [==============================] - 66s 656ms/step - loss: -0.7521 - dice_coef: 0.7521 - jacard_coef: 0.6031 - val_loss: -0.7555 - val_dice_coef: 0.7627 - val_jacard_coef: 0.6180 - lr: 1.0000e-05\n",
            "Epoch 71/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7530 - dice_coef: 0.7535 - jacard_coef: 0.6050\n",
            "Epoch 71: val_loss did not improve from -0.75579\n",
            "100/100 [==============================] - 65s 655ms/step - loss: -0.7530 - dice_coef: 0.7535 - jacard_coef: 0.6050 - val_loss: -0.7552 - val_dice_coef: 0.7625 - val_jacard_coef: 0.6177 - lr: 1.0000e-05\n",
            "Epoch 72/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7524 - dice_coef: 0.7524 - jacard_coef: 0.6035\n",
            "Epoch 72: val_loss did not improve from -0.75579\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.7524 - dice_coef: 0.7524 - jacard_coef: 0.6035 - val_loss: -0.7555 - val_dice_coef: 0.7627 - val_jacard_coef: 0.6179 - lr: 1.0000e-05\n",
            "Epoch 73/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7521 - dice_coef: 0.7521 - jacard_coef: 0.6031\n",
            "Epoch 73: val_loss did not improve from -0.75579\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.7521 - dice_coef: 0.7521 - jacard_coef: 0.6031 - val_loss: -0.7555 - val_dice_coef: 0.7627 - val_jacard_coef: 0.6180 - lr: 1.0000e-05\n",
            "Epoch 74/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7525 - dice_coef: 0.7526 - jacard_coef: 0.6039\n",
            "Epoch 74: val_loss did not improve from -0.75579\n",
            "100/100 [==============================] - 65s 654ms/step - loss: -0.7525 - dice_coef: 0.7526 - jacard_coef: 0.6039 - val_loss: -0.7553 - val_dice_coef: 0.7625 - val_jacard_coef: 0.6178 - lr: 1.0000e-05\n",
            "Epoch 75/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7525 - dice_coef: 0.7522 - jacard_coef: 0.6033\n",
            "Epoch 75: val_loss did not improve from -0.75579\n",
            "100/100 [==============================] - 65s 654ms/step - loss: -0.7525 - dice_coef: 0.7522 - jacard_coef: 0.6033 - val_loss: -0.7558 - val_dice_coef: 0.7630 - val_jacard_coef: 0.6184 - lr: 1.0000e-05\n",
            "Epoch 76/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7527 - dice_coef: 0.7529 - jacard_coef: 0.6043\n",
            "Epoch 76: val_loss did not improve from -0.75579\n",
            "100/100 [==============================] - 66s 656ms/step - loss: -0.7527 - dice_coef: 0.7529 - jacard_coef: 0.6043 - val_loss: -0.7555 - val_dice_coef: 0.7628 - val_jacard_coef: 0.6181 - lr: 1.0000e-05\n",
            "Epoch 77/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7533 - dice_coef: 0.7534 - jacard_coef: 0.6048\n",
            "Epoch 77: val_loss did not improve from -0.75579\n",
            "100/100 [==============================] - 65s 655ms/step - loss: -0.7533 - dice_coef: 0.7534 - jacard_coef: 0.6048 - val_loss: -0.7556 - val_dice_coef: 0.7629 - val_jacard_coef: 0.6182 - lr: 1.0000e-05\n",
            "Epoch 78/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7527 - dice_coef: 0.7524 - jacard_coef: 0.6034\n",
            "Epoch 78: val_loss did not improve from -0.75579\n",
            "100/100 [==============================] - 65s 654ms/step - loss: -0.7527 - dice_coef: 0.7524 - jacard_coef: 0.6034 - val_loss: -0.7557 - val_dice_coef: 0.7630 - val_jacard_coef: 0.6183 - lr: 1.0000e-05\n",
            "Epoch 79/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7515 - dice_coef: 0.7493 - jacard_coef: 0.6002\n",
            "Epoch 79: val_loss improved from -0.75579 to -0.75591, saving model to weights/unet_64_sgd_dice+jacard_8.h5\n",
            "100/100 [==============================] - 66s 660ms/step - loss: -0.7515 - dice_coef: 0.7493 - jacard_coef: 0.6002 - val_loss: -0.7559 - val_dice_coef: 0.7631 - val_jacard_coef: 0.6185 - lr: 1.0000e-05\n",
            "Epoch 80/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7543 - dice_coef: 0.7547 - jacard_coef: 0.6064\n",
            "Epoch 80: val_loss did not improve from -0.75591\n",
            "100/100 [==============================] - 66s 656ms/step - loss: -0.7543 - dice_coef: 0.7547 - jacard_coef: 0.6064 - val_loss: -0.7557 - val_dice_coef: 0.7629 - val_jacard_coef: 0.6183 - lr: 1.0000e-05\n",
            "Epoch 81/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7525 - dice_coef: 0.7524 - jacard_coef: 0.6035\n",
            "Epoch 81: val_loss did not improve from -0.75591\n",
            "100/100 [==============================] - 65s 654ms/step - loss: -0.7525 - dice_coef: 0.7524 - jacard_coef: 0.6035 - val_loss: -0.7554 - val_dice_coef: 0.7627 - val_jacard_coef: 0.6180 - lr: 1.0000e-05\n",
            "Epoch 82/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7531 - dice_coef: 0.7530 - jacard_coef: 0.6043\n",
            "Epoch 82: val_loss did not improve from -0.75591\n",
            "100/100 [==============================] - 65s 652ms/step - loss: -0.7531 - dice_coef: 0.7530 - jacard_coef: 0.6043 - val_loss: -0.7552 - val_dice_coef: 0.7625 - val_jacard_coef: 0.6177 - lr: 1.0000e-05\n",
            "Epoch 83/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7533 - dice_coef: 0.7535 - jacard_coef: 0.6049\n",
            "Epoch 83: val_loss did not improve from -0.75591\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.7533 - dice_coef: 0.7535 - jacard_coef: 0.6049 - val_loss: -0.7549 - val_dice_coef: 0.7623 - val_jacard_coef: 0.6174 - lr: 1.0000e-05\n",
            "Epoch 84/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7535 - dice_coef: 0.7533 - jacard_coef: 0.6047\n",
            "Epoch 84: val_loss did not improve from -0.75591\n",
            "100/100 [==============================] - 65s 652ms/step - loss: -0.7535 - dice_coef: 0.7533 - jacard_coef: 0.6047 - val_loss: -0.7551 - val_dice_coef: 0.7624 - val_jacard_coef: 0.6177 - lr: 1.0000e-05\n",
            "Epoch 85/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7532 - dice_coef: 0.7533 - jacard_coef: 0.6047\n",
            "Epoch 85: val_loss did not improve from -0.75591\n",
            "100/100 [==============================] - 65s 655ms/step - loss: -0.7532 - dice_coef: 0.7533 - jacard_coef: 0.6047 - val_loss: -0.7555 - val_dice_coef: 0.7628 - val_jacard_coef: 0.6181 - lr: 1.0000e-05\n",
            "Epoch 86/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7537 - dice_coef: 0.7515 - jacard_coef: 0.6030\n",
            "Epoch 86: val_loss did not improve from -0.75591\n",
            "100/100 [==============================] - 65s 651ms/step - loss: -0.7537 - dice_coef: 0.7515 - jacard_coef: 0.6030 - val_loss: -0.7555 - val_dice_coef: 0.7628 - val_jacard_coef: 0.6182 - lr: 1.0000e-05\n",
            "Epoch 87/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7522 - dice_coef: 0.7505 - jacard_coef: 0.6014\n",
            "Epoch 87: val_loss did not improve from -0.75591\n",
            "100/100 [==============================] - 65s 651ms/step - loss: -0.7522 - dice_coef: 0.7505 - jacard_coef: 0.6014 - val_loss: -0.7557 - val_dice_coef: 0.7629 - val_jacard_coef: 0.6183 - lr: 1.0000e-05\n",
            "Epoch 88/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7523 - dice_coef: 0.7496 - jacard_coef: 0.6007\n",
            "Epoch 88: val_loss did not improve from -0.75591\n",
            "100/100 [==============================] - 65s 655ms/step - loss: -0.7523 - dice_coef: 0.7496 - jacard_coef: 0.6007 - val_loss: -0.7559 - val_dice_coef: 0.7630 - val_jacard_coef: 0.6185 - lr: 1.0000e-05\n",
            "Epoch 89/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7527 - dice_coef: 0.7512 - jacard_coef: 0.6023\n",
            "Epoch 89: val_loss did not improve from -0.75591\n",
            "100/100 [==============================] - 66s 655ms/step - loss: -0.7527 - dice_coef: 0.7512 - jacard_coef: 0.6023 - val_loss: -0.7557 - val_dice_coef: 0.7629 - val_jacard_coef: 0.6182 - lr: 1.0000e-05\n",
            "Epoch 90/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7527 - dice_coef: 0.7521 - jacard_coef: 0.6033\n",
            "Epoch 90: val_loss did not improve from -0.75591\n",
            "100/100 [==============================] - 65s 655ms/step - loss: -0.7527 - dice_coef: 0.7521 - jacard_coef: 0.6033 - val_loss: -0.7557 - val_dice_coef: 0.7629 - val_jacard_coef: 0.6183 - lr: 1.0000e-05\n",
            "Epoch 91/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7527 - dice_coef: 0.7531 - jacard_coef: 0.6046\n",
            "Epoch 91: val_loss did not improve from -0.75591\n",
            "100/100 [==============================] - 65s 653ms/step - loss: -0.7527 - dice_coef: 0.7531 - jacard_coef: 0.6046 - val_loss: -0.7557 - val_dice_coef: 0.7629 - val_jacard_coef: 0.6182 - lr: 1.0000e-05\n",
            "Epoch 92/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7539 - dice_coef: 0.7522 - jacard_coef: 0.6036\n",
            "Epoch 92: val_loss did not improve from -0.75591\n",
            "100/100 [==============================] - 66s 656ms/step - loss: -0.7539 - dice_coef: 0.7522 - jacard_coef: 0.6036 - val_loss: -0.7554 - val_dice_coef: 0.7627 - val_jacard_coef: 0.6179 - lr: 1.0000e-05\n",
            "Epoch 93/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7536 - dice_coef: 0.7538 - jacard_coef: 0.6054\n",
            "Epoch 93: val_loss did not improve from -0.75591\n",
            "100/100 [==============================] - 65s 654ms/step - loss: -0.7536 - dice_coef: 0.7538 - jacard_coef: 0.6054 - val_loss: -0.7554 - val_dice_coef: 0.7626 - val_jacard_coef: 0.6179 - lr: 1.0000e-05\n",
            "Epoch 94/100\n",
            "100/100 [==============================] - ETA: 0s - loss: -0.7536 - dice_coef: 0.7539 - jacard_coef: 0.6054\n",
            "Epoch 94: val_loss did not improve from -0.75591\n",
            "100/100 [==============================] - 65s 650ms/step - loss: -0.7536 - dice_coef: 0.7539 - jacard_coef: 0.6054 - val_loss: -0.7555 - val_dice_coef: 0.7627 - val_jacard_coef: 0.6181 - lr: 1.0000e-05\n",
            "Epoch 94: early stopping\n"
          ]
        }
      ],
      "source": [
        "# SGD, Dice+Jacard, batch_size=8\n",
        "logs_p = 'logs/unet_64_sgd_dice+jacard_8.csv'\n",
        "weights_p = 'weights/unet_64_sgd_dice+jacard_8.h5'\n",
        "\n",
        "checkpointer = ModelCheckpoint(weights_p, verbose=1, save_best_only=True)\n",
        "callbacks_f = [earlystopper, checkpointer, reduce_lr, CSVLogger(logs_p, append=True, separator=';')]\n",
        "optimizer_f = sgd\n",
        "loss_f = [dice_coef_loss, jacard_coef_loss]\n",
        "metrics_f = [dice_coef, jacard_coef,]\n",
        "#batch_s = 8 * tpu_strategy.num_replicas_in_sync # with TPU\n",
        "batch_s = 8\n",
        "epochs_s = 100\n",
        "\n",
        "#with tpu_strategy.scope():  # with TPU\n",
        "#  model = unet_model(input=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),)\n",
        "model = unet_model(input=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), nn_name='SGD_DICE_JACCARD')\n",
        "model.compile(optimizer=optimizer_f, loss=loss_f, metrics=metrics_f)\n",
        "\n",
        "result = model.fit(x=X_train, y=Y_train.astype(np.float32), validation_data=(X_val, Y_val.astype(np.float32)), batch_size=batch_s, \n",
        "                    epochs=epochs_s, callbacks=callbacks_f, verbose=1,)\n",
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv4tn9INi87D"
      },
      "source": [
        "## U-Net with InceptionResNetV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhCwYryfi87D"
      },
      "outputs": [],
      "source": [
        "# Create U-Net model\n",
        "from keras.applications import InceptionResNetV2\n",
        "from keras.layers import ZeroPadding2D\n",
        "\n",
        "def conv_block(input, num_filters):\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def decoder_block(inputs, skip_features, num_filters):\n",
        "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(inputs)\n",
        "    x = concatenate([x, skip_features])\n",
        "    x = conv_block(x, num_filters)\n",
        "    return x\n",
        "\n",
        "def build_inception_resnetv2_unet(input_shape):\n",
        "    \"\"\" Input \"\"\"\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    \"\"\" Pre-trained InceptionResNetV2 Model \"\"\"\n",
        "    encoder = InceptionResNetV2(include_top=False, weights=\"imagenet\", input_tensor=inputs)\n",
        "\n",
        "    \"\"\" Encoder \"\"\"\n",
        "    s1 = encoder.get_layer(\"input_1\").output           ## (512 x 512)\n",
        "\n",
        "    s2 = encoder.get_layer(\"activation\").output        ## (255 x 255)\n",
        "    s2 = ZeroPadding2D(( (1, 0), (1, 0) ))(s2)         ## (256 x 256)\n",
        "\n",
        "    s3 = encoder.get_layer(\"activation_3\").output      ## (126 x 126)\n",
        "    s3 = ZeroPadding2D((1, 1))(s3)                     ## (128 x 128)\n",
        "\n",
        "    s4 = encoder.get_layer(\"activation_74\").output      ## (61 x 61)\n",
        "    s4 = ZeroPadding2D(( (2, 1),(2, 1) ))(s4)           ## (64 x 64)\n",
        "\n",
        "    \"\"\" Bridge \"\"\"\n",
        "    b1 = encoder.get_layer(\"activation_161\").output     ## (30 x 30)\n",
        "    b1 = ZeroPadding2D((1, 1))(b1)                      ## (32 x 32)\n",
        "\n",
        "    \"\"\" Decoder \"\"\"\n",
        "    d1 = decoder_block(b1, s4, 512)                     ## (64 x 64)\n",
        "    d2 = decoder_block(d1, s3, 256)                     ## (128 x 128)\n",
        "    d3 = decoder_block(d2, s2, 128)                     ## (256 x 256)\n",
        "    d4 = decoder_block(d3, s1, 64)                      ## (512 x 512)\n",
        "\n",
        "    \"\"\" Output \"\"\"\n",
        "    outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n",
        "\n",
        "    model = Model(inputs, outputs, name=\"InceptionResNetV2_U-Net\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFokNOLycs64"
      },
      "outputs": [],
      "source": [
        "model = build_inception_resnetv2_unet(input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uqEgS1Gcs65"
      },
      "source": [
        "### Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5vHBq7ui87D"
      },
      "outputs": [],
      "source": [
        "# Optimizer\n",
        "from keras import optimizers\n",
        "sgd = optimizers.SGD(learning_rate=0.002, decay=0.00003, momentum=0.9)\n",
        "adam = optimizers.Adam(learning_rate=0.002, decay=0.00003)\n",
        "\n",
        "# Loss function\n",
        "from keras.losses import BinaryCrossentropy\n",
        "bce = BinaryCrossentropy()\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return -dice_coef(y_true, y_pred)\n",
        "\n",
        "def jacard_coef_loss(y_true, y_pred):\n",
        "    return -jacard_coef(y_true, y_pred)\n",
        "\n",
        "# Metrics\n",
        "from keras.metrics import Recall, Precision\n",
        "recall = Recall()\n",
        "precision = Precision()\n",
        "\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2.0 * intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1.0)\n",
        "\n",
        "def jacard_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
        "\n",
        "# Fit model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
        "earlystopper = EarlyStopping(patience=5, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(factor=0.1, patience=4, min_lr=0.00001, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ-nH49Hcs65"
      },
      "source": [
        "### Adam, BCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NamQC-qdi87D"
      },
      "outputs": [],
      "source": [
        "# Adam, BCE, batch_size=8\n",
        "logs_p = 'logs/irnv2_unet_64_adam_bce_8.csv'\n",
        "weights_p = 'weights/irnv2_unet_64_adam_bce_8.h5'\n",
        "\n",
        "checkpointer = ModelCheckpoint(weights_p, verbose=1, save_best_only=True)\n",
        "callbacks_f = [earlystopper, checkpointer, reduce_lr, CSVLogger(logs_p, append=True, separator=';')]\n",
        "optimizer_f = adam\n",
        "loss_f = [bce]\n",
        "metrics_f = [recall, precision,]\n",
        "batch_s = 8 * tpu_strategy.num_replicas_in_sync # with TPU\n",
        "#batch_s = 8\n",
        "epochs_s = 100\n",
        "\n",
        "with tpu_strategy.scope():  # with TPU\n",
        "    model = build_inception_resnetv2_unet(input=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),)\n",
        "#model = unet_model(input=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),)\n",
        "model.compile(optimizer=optimizer_f, loss=loss_f, metrics=metrics_f)\n",
        "\n",
        "results = model.fit(x=X_train, y=Y_train.astype(np.float32), validation_data=(X_val, Y_val.astype(np.float32)), batch_size=batch_s, \n",
        "                    epochs=epochs_s, callbacks=callbacks_f, verbose=1,)\n",
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jcd6sBR-cs65"
      },
      "source": [
        "### SGD, BCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quqUi7Xdcs65"
      },
      "outputs": [],
      "source": [
        "# SGD, BCE, batch_size=8\n",
        "logs_p = 'logs/irnv2_unet_64_sgd_bce_8.csv'\n",
        "weights_p = 'weights/irnv2_unet_64_sgd_bce_8.h5'\n",
        "\n",
        "checkpointer = ModelCheckpoint(weights_p, verbose=1, save_best_only=True)\n",
        "callbacks_f = [earlystopper, checkpointer, reduce_lr, CSVLogger(logs_p, append=True, separator=';')]\n",
        "optimizer_f = sgd\n",
        "loss_f = [bce]\n",
        "metrics_f = [recall, precision,]\n",
        "batch_s = 8 * tpu_strategy.num_replicas_in_sync # with TPU\n",
        "#batch_s = 8\n",
        "epochs_s = 100\n",
        "\n",
        "with tpu_strategy.scope():  # with TPU\n",
        "    model = build_inception_resnetv2_unet(input=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),)\n",
        "#model = unet_model(input=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),)\n",
        "model.compile(optimizer=optimizer_f, loss=loss_f, metrics=metrics_f)\n",
        "\n",
        "results = model.fit(x=X_train, y=Y_train.astype(np.float32), validation_data=(X_val, Y_val.astype(np.float32)), batch_size=batch_s, \n",
        "                    epochs=epochs_s, callbacks=callbacks_f, verbose=1,)\n",
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJMf2Lfdcs65"
      },
      "source": [
        "### Adam, DICE + Jaccard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqgKFFbNcs65"
      },
      "outputs": [],
      "source": [
        "# Adam, Dice+Jacard, batch_size=8\n",
        "logs_p = 'logs/irnv2_unet_64_adam_dice+jacard_8.csv'\n",
        "weights_p = 'weights/irnv2_unet_64_adam_dice+jacard_8.h5'\n",
        "\n",
        "checkpointer = ModelCheckpoint(weights_p, verbose=1, save_best_only=True)\n",
        "callbacks_f = [earlystopper, checkpointer, reduce_lr, CSVLogger(logs_p, append=True, separator=';')]\n",
        "optimizer_f = adam\n",
        "loss_f = [dice_coef_loss, jacard_coef_loss]\n",
        "metrics_f = [dice_coef, jacard_coef,]\n",
        "batch_s = 8 * tpu_strategy.num_replicas_in_sync # with TPU\n",
        "#batch_s = 8\n",
        "epochs_s = 100\n",
        "\n",
        "with tpu_strategy.scope():  # with TPU\n",
        "    model = build_inception_resnetv2_unet(input=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),)\n",
        "#model = unet_model(input=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),)\n",
        "model.compile(optimizer=optimizer_f, loss=loss_f, metrics=metrics_f)\n",
        "\n",
        "results = model.fit(x=X_train, y=Y_train.astype(np.float32), validation_data=(X_val, Y_val.astype(np.float32)), batch_size=batch_s, \n",
        "                    epochs=epochs_s, callbacks=callbacks_f, verbose=1,)\n",
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF06xfRucs65"
      },
      "source": [
        "### SGD, DICE + Jaccard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Msvcdaj3cs65"
      },
      "outputs": [],
      "source": [
        "# Adam, Dice+Jacard, batch_size=8\n",
        "logs_p = 'logs/irnv2_unet_64_adam_dice+jacard_8.csv'\n",
        "weights_p = 'weights/irnv2_unet_64_adam_dice+jacard_8.h5'\n",
        "\n",
        "checkpointer = ModelCheckpoint(weights_p, verbose=1, save_best_only=True)\n",
        "callbacks_f = [earlystopper, checkpointer, reduce_lr, CSVLogger(logs_p, append=True, separator=';')]\n",
        "optimizer_f = sgd\n",
        "loss_f = [dice_coef_loss, jacard_coef_loss]\n",
        "metrics_f = [dice_coef, jacard_coef,]\n",
        "batch_s = 8 * tpu_strategy.num_replicas_in_sync # with TPU\n",
        "#batch_s = 8\n",
        "epochs_s = 100\n",
        "\n",
        "with tpu_strategy.scope():  # with TPU\n",
        "    model = build_inception_resnetv2_unet(input=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),)\n",
        "#model = unet_model(input=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),)\n",
        "model.compile(optimizer=optimizer_f, loss=loss_f, metrics=metrics_f)\n",
        "\n",
        "results = model.fit(x=X_train, y=Y_train.astype(np.float32), validation_data=(X_val, Y_val.astype(np.float32)), batch_size=batch_s, \n",
        "                    epochs=epochs_s, callbacks=callbacks_f, verbose=1,)\n",
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI-E1kNMwMuW"
      },
      "source": [
        "## Residual Attention U-Net with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-Samzp6u_Vq"
      },
      "outputs": [],
      "source": [
        "def conv_block(x, filter_size, size, dropout, batch_norm=False):\n",
        "\n",
        "    conv = Conv2D(size, (filter_size, filter_size), padding=\"same\")(x)\n",
        "    if batch_norm is True:\n",
        "        conv = BatchNormalization(axis=3)(conv)\n",
        "    conv = Activation(\"relu\")(conv)\n",
        "\n",
        "    conv = Conv2D(size, (filter_size, filter_size), padding=\"same\")(conv)\n",
        "    if batch_norm is True:\n",
        "        conv = BatchNormalization(axis=3)(conv)\n",
        "    conv = Activation(\"relu\")(conv)\n",
        "    \n",
        "    if dropout > 0:\n",
        "        conv = Dropout(dropout)(conv)\n",
        "\n",
        "    return conv\n",
        "\n",
        "\n",
        "def repeat_elem(tensor, rep):\n",
        "    # lambda function to repeat Repeats the elements of a tensor along an axis\n",
        "    #by a factor of rep.\n",
        "    # If tensor has shape (None, 256,256,3), lambda will return a tensor of shape \n",
        "    #(None, 256,256,6), if specified axis=3 and rep=2.\n",
        "\n",
        "     return Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3),\n",
        "                          arguments={'repnum': rep})(tensor)\n",
        "\n",
        "\n",
        "def res_conv_block(x, filter_size, size, dropout, batch_norm=False):\n",
        "    '''\n",
        "    Residual convolutional layer.\n",
        "    Two variants....\n",
        "    Either put activation function before the addition with shortcut\n",
        "    or after the addition (which would be as proposed in the original resNet).\n",
        "    \n",
        "    1. conv - BN - Activation - conv - BN - Activation \n",
        "                                          - shortcut  - BN - shortcut+BN\n",
        "                                          \n",
        "    2. conv - BN - Activation - conv - BN   \n",
        "                                     - shortcut  - BN - shortcut+BN - Activation                                     \n",
        "    \n",
        "    Check fig 4 in https://arxiv.org/ftp/arxiv/papers/1802/1802.06955.pdf\n",
        "    '''\n",
        "\n",
        "    conv = Conv2D(size, (filter_size, filter_size), padding='same')(x)\n",
        "    if batch_norm is True:\n",
        "        conv = BatchNormalization(axis=3)(conv)\n",
        "    conv = Activation('relu')(conv)\n",
        "    \n",
        "    conv = Conv2D(size, (filter_size, filter_size), padding='same')(conv)\n",
        "    if batch_norm is True:\n",
        "        conv = BatchNormalization(axis=3)(conv)\n",
        "    #conv = layers.Activation('relu')(conv)    #Activation before addition with shortcut\n",
        "    if dropout > 0:\n",
        "        conv = Dropout(dropout)(conv)\n",
        "\n",
        "    shortcut = Conv2D(size, kernel_size=(1, 1), padding='same')(x)\n",
        "    if batch_norm is True:\n",
        "        shortcut = BatchNormalization(axis=3)(shortcut)\n",
        "\n",
        "    res_path = add([shortcut, conv])\n",
        "    res_path = Activation('relu')(res_path)    #Activation after addition with shortcut (Original residual block)\n",
        "    return res_path\n",
        "\n",
        "def gating_signal(input, out_size, batch_norm=False):\n",
        "    \"\"\"\n",
        "    resize the down layer feature map into the same dimension as the up layer feature map\n",
        "    using 1x1 conv\n",
        "    :return: the gating feature map with the same dimension of the up layer feature map\n",
        "    \"\"\"\n",
        "    x = Conv2D(out_size, (1, 1), padding='same')(input)\n",
        "    if batch_norm:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "\n",
        "def attention_block(x, gating, inter_shape):\n",
        "    shape_x = K.int_shape(x)\n",
        "    shape_g = K.int_shape(gating)\n",
        "\n",
        "# Getting the x signal to the same shape as the gating signal\n",
        "    theta_x = Conv2D(inter_shape, (2, 2), strides=(2, 2), padding='same')(x)  # 16\n",
        "    shape_theta_x = K.int_shape(theta_x)\n",
        "\n",
        "# Getting the gating signal to the same number of filters as the inter_shape\n",
        "    phi_g = Conv2D(inter_shape, (1, 1), padding='same')(gating)\n",
        "    upsample_g = Conv2DTranspose(inter_shape, (3, 3),\n",
        "                                 strides=(shape_theta_x[1] // shape_g[1], shape_theta_x[2] // shape_g[2]),\n",
        "                                 padding='same')(phi_g)  # 16\n",
        "\n",
        "    concat_xg = add([upsample_g, theta_x])\n",
        "    act_xg = Activation('relu')(concat_xg)\n",
        "    psi = Conv2D(1, (1, 1), padding='same')(act_xg)\n",
        "    sigmoid_xg = Activation('sigmoid')(psi)\n",
        "    shape_sigmoid = K.int_shape(sigmoid_xg)\n",
        "    upsample_psi = UpSampling2D(size=(shape_x[1] // shape_sigmoid[1], shape_x[2] // shape_sigmoid[2]))(sigmoid_xg)  # 32\n",
        "\n",
        "    upsample_psi = repeat_elem(upsample_psi, shape_x[3])\n",
        "\n",
        "    y = multiply([upsample_psi, x])\n",
        "\n",
        "    result = Conv2D(shape_x[3], (1, 1), padding='same')(y)\n",
        "    result_bn = BatchNormalization()(result)\n",
        "    return result_bn\n",
        "\n",
        "def Attention_ResUNet(input_shape, NUM_CLASSES=1, dropout_rate=0.0, batch_norm=True):\n",
        "    '''\n",
        "    Rsidual UNet, with attention \n",
        "    \n",
        "    '''\n",
        "    # network structure\n",
        "    FILTER_NUM = 64 # number of basic filters for the first layer\n",
        "    FILTER_SIZE = 3 # size of the convolutional filter\n",
        "    UP_SAMP_SIZE = 2 # size of upsampling filters\n",
        "    # input data\n",
        "    # dimension of the image depth\n",
        "    inputs = Input(input_shape, dtype=tf.float32)\n",
        "    axis = 3\n",
        "\n",
        "    # Downsampling layers\n",
        "    # DownRes 1, double residual convolution + pooling\n",
        "    conv_128 = res_conv_block(inputs, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)\n",
        "    pool_64 = MaxPooling2D(pool_size=(2,2))(conv_128)\n",
        "    # DownRes 2\n",
        "    conv_64 = res_conv_block(pool_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)\n",
        "    pool_32 = MaxPooling2D(pool_size=(2,2))(conv_64)\n",
        "    # DownRes 3\n",
        "    conv_32 = res_conv_block(pool_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)\n",
        "    pool_16 = MaxPooling2D(pool_size=(2,2))(conv_32)\n",
        "    # DownRes 4\n",
        "    conv_16 = res_conv_block(pool_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)\n",
        "    pool_8 = MaxPooling2D(pool_size=(2,2))(conv_16)\n",
        "    # DownRes 5, convolution only\n",
        "    conv_8 = res_conv_block(pool_8, FILTER_SIZE, 16*FILTER_NUM, dropout_rate, batch_norm)\n",
        "\n",
        "    # Upsampling layers\n",
        "    # UpRes 6, attention gated concatenation + upsampling + double residual convolution\n",
        "    gating_16 = gating_signal(conv_8, 8*FILTER_NUM, batch_norm)\n",
        "    att_16 = attention_block(conv_16, gating_16, 8*FILTER_NUM)\n",
        "    up_16 = UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(conv_8)\n",
        "    up_16 = concatenate([up_16, att_16], axis=axis)\n",
        "    up_conv_16 = res_conv_block(up_16, FILTER_SIZE, 8*FILTER_NUM, dropout_rate, batch_norm)\n",
        "    # UpRes 7\n",
        "    gating_32 = gating_signal(up_conv_16, 4*FILTER_NUM, batch_norm)\n",
        "    att_32 = attention_block(conv_32, gating_32, 4*FILTER_NUM)\n",
        "    up_32 = UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_16)\n",
        "    up_32 = concatenate([up_32, att_32], axis=axis)\n",
        "    up_conv_32 = res_conv_block(up_32, FILTER_SIZE, 4*FILTER_NUM, dropout_rate, batch_norm)\n",
        "    # UpRes 8\n",
        "    gating_64 = gating_signal(up_conv_32, 2*FILTER_NUM, batch_norm)\n",
        "    att_64 = attention_block(conv_64, gating_64, 2*FILTER_NUM)\n",
        "    up_64 = UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_32)\n",
        "    up_64 = concatenate([up_64, att_64], axis=axis)\n",
        "    up_conv_64 = res_conv_block(up_64, FILTER_SIZE, 2*FILTER_NUM, dropout_rate, batch_norm)\n",
        "    # UpRes 9\n",
        "    gating_128 = gating_signal(up_conv_64, FILTER_NUM, batch_norm)\n",
        "    att_128 = attention_block(conv_128, gating_128, FILTER_NUM)\n",
        "    up_128 = UpSampling2D(size=(UP_SAMP_SIZE, UP_SAMP_SIZE), data_format=\"channels_last\")(up_conv_64)\n",
        "    up_128 = concatenate([up_128, att_128], axis=axis)\n",
        "    up_conv_128 = res_conv_block(up_128, FILTER_SIZE, FILTER_NUM, dropout_rate, batch_norm)\n",
        "\n",
        "    # 1*1 convolutional layers\n",
        "    \n",
        "    conv_final = Conv2D(NUM_CLASSES, kernel_size=(1,1))(up_conv_128)\n",
        "    conv_final = BatchNormalization(axis=axis)(conv_final)\n",
        "    conv_final = Activation('sigmoid')(conv_final)  #Change to softmax for multichannel\n",
        "\n",
        "    # Model integration\n",
        "    model = Model(inputs, conv_final, name=\"AttentionResUNet\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1iLgCjaxH7t",
        "outputId": "0ade0ff8-f474-46a2-8e36-47cee6bc619f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metal device set to: Apple M2\n",
            "\n",
            "systemMemory: 16.00 GB\n",
            "maxCacheSize: 5.33 GB\n",
            "\n",
            "Model: \"AttentionResUNet\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 256, 256, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 256, 256, 64  1792        ['input_1[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 256, 256, 64  256        ['conv2d[0][0]']                 \n",
            " alization)                     )                                                                 \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 256, 256, 64  0           ['batch_normalization[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 256, 256, 64  36928       ['activation[0][0]']             \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 256, 256, 64  256         ['input_1[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 256, 256, 64  256        ['conv2d_1[0][0]']               \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 256, 256, 64  256        ['conv2d_2[0][0]']               \n",
            " rmalization)                   )                                                                 \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 256, 256, 64  0           ['batch_normalization_1[0][0]']  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 256, 256, 64  0           ['batch_normalization_2[0][0]',  \n",
            "                                )                                 'dropout[0][0]']                \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 256, 256, 64  0           ['add[0][0]']                    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 128, 128, 64  0           ['activation_1[0][0]']           \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 128, 128, 12  73856       ['max_pooling2d[0][0]']          \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 128, 128, 12  512        ['conv2d_3[0][0]']               \n",
            " rmalization)                   8)                                                                \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 128, 128, 12  0           ['batch_normalization_3[0][0]']  \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 128, 128, 12  147584      ['activation_2[0][0]']           \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 128, 128, 12  8320        ['max_pooling2d[0][0]']          \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 128, 128, 12  512        ['conv2d_4[0][0]']               \n",
            " rmalization)                   8)                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 128, 128, 12  512        ['conv2d_5[0][0]']               \n",
            " rmalization)                   8)                                                                \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 128, 128, 12  0           ['batch_normalization_4[0][0]']  \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 128, 128, 12  0           ['batch_normalization_5[0][0]',  \n",
            "                                8)                                'dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 128, 128, 12  0           ['add_1[0][0]']                  \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 128)  0          ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 64, 64, 256)  295168      ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 64, 64, 256)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 64, 64, 256)  590080      ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 64, 64, 256)  33024       ['max_pooling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 64, 64, 256)  1024       ['conv2d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 64, 64, 256)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 64, 64, 256)  0           ['batch_normalization_8[0][0]',  \n",
            "                                                                  'dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 64, 64, 256)  0           ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 256)  0          ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 32, 32, 512)  1180160     ['max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 32, 32, 512)  2048       ['conv2d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 32, 32, 512)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 32, 32, 512)  2359808     ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 32, 32, 512)  131584      ['max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 32, 32, 512)  2048       ['conv2d_10[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 32, 32, 512)  2048       ['conv2d_11[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 32, 32, 512)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 32, 32, 512)  0           ['batch_normalization_11[0][0]', \n",
            "                                                                  'dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 32, 32, 512)  0           ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 512)  0          ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 16, 16, 1024  4719616     ['max_pooling2d_3[0][0]']        \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 16, 16, 1024  4096       ['conv2d_12[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 16, 16, 1024  0           ['batch_normalization_12[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 16, 16, 1024  9438208     ['activation_8[0][0]']           \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 16, 16, 1024  525312      ['max_pooling2d_3[0][0]']        \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 16, 16, 1024  4096       ['conv2d_13[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 16, 16, 1024  4096       ['conv2d_14[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 16, 16, 1024  0           ['batch_normalization_13[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 16, 16, 1024  0           ['batch_normalization_14[0][0]', \n",
            "                                )                                 'dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 16, 16, 1024  0           ['add_4[0][0]']                  \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 16, 16, 512)  524800      ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 16, 16, 512)  2048       ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 16, 16, 512)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 16, 16, 512)  262656      ['activation_10[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_transpose (Conv2DTransp  (None, 16, 16, 512)  2359808    ['conv2d_17[0][0]']              \n",
            " ose)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 16, 16, 512)  1049088     ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 16, 16, 512)  0           ['conv2d_transpose[0][0]',       \n",
            "                                                                  'conv2d_16[0][0]']              \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 16, 16, 512)  0           ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 16, 16, 1)    513         ['activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 16, 16, 1)    0           ['conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " up_sampling2d (UpSampling2D)   (None, 32, 32, 1)    0           ['activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 32, 32, 512)  0           ['up_sampling2d[0][0]']          \n",
            "                                                                                                  \n",
            " multiply (Multiply)            (None, 32, 32, 512)  0           ['lambda[0][0]',                 \n",
            "                                                                  'activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 32, 32, 512)  262656      ['multiply[0][0]']               \n",
            "                                                                                                  \n",
            " up_sampling2d_1 (UpSampling2D)  (None, 32, 32, 1024  0          ['activation_9[0][0]']           \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 32, 32, 512)  2048       ['conv2d_19[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 32, 32, 1536  0           ['up_sampling2d_1[0][0]',        \n",
            "                                )                                 'batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 32, 32, 512)  7078400     ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 32, 32, 512)  2048       ['conv2d_20[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 32, 32, 512)  0           ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 32, 32, 512)  2359808     ['activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 32, 32, 512)  786944      ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 32, 32, 512)  2048       ['conv2d_21[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 32, 32, 512)  2048       ['conv2d_22[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 32, 32, 512)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 32, 32, 512)  0           ['batch_normalization_19[0][0]', \n",
            "                                                                  'dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 32, 32, 512)  0           ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 32, 32, 256)  131328      ['activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 32, 32, 256)  1024       ['conv2d_23[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 32, 32, 256)  0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 32, 32, 256)  65792       ['activation_15[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_transpose_1 (Conv2DTran  (None, 32, 32, 256)  590080     ['conv2d_25[0][0]']              \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 32, 32, 256)  262400      ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 32, 32, 256)  0           ['conv2d_transpose_1[0][0]',     \n",
            "                                                                  'conv2d_24[0][0]']              \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 32, 32, 256)  0           ['add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 32, 32, 1)    257         ['activation_16[0][0]']          \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 32, 32, 1)    0           ['conv2d_26[0][0]']              \n",
            "                                                                                                  \n",
            " up_sampling2d_2 (UpSampling2D)  (None, 64, 64, 1)   0           ['activation_17[0][0]']          \n",
            "                                                                                                  \n",
            " lambda_1 (Lambda)              (None, 64, 64, 256)  0           ['up_sampling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " multiply_1 (Multiply)          (None, 64, 64, 256)  0           ['lambda_1[0][0]',               \n",
            "                                                                  'activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 64, 64, 256)  65792       ['multiply_1[0][0]']             \n",
            "                                                                                                  \n",
            " up_sampling2d_3 (UpSampling2D)  (None, 64, 64, 512)  0          ['activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_27[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 64, 64, 768)  0           ['up_sampling2d_3[0][0]',        \n",
            "                                                                  'batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 64, 64, 256)  1769728     ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_28[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, 64, 64, 256)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 64, 64, 256)  590080      ['activation_18[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 64, 64, 256)  196864      ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_29[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 64, 64, 256)  1024       ['conv2d_30[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 64, 64, 256)  0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 64, 64, 256)  0           ['batch_normalization_24[0][0]', \n",
            "                                                                  'dropout_6[0][0]']              \n",
            "                                                                                                  \n",
            " activation_19 (Activation)     (None, 64, 64, 256)  0           ['add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 64, 64, 128)  32896       ['activation_19[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 64, 64, 128)  512        ['conv2d_31[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_20 (Activation)     (None, 64, 64, 128)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 64, 64, 128)  16512       ['activation_20[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_transpose_2 (Conv2DTran  (None, 64, 64, 128)  147584     ['conv2d_33[0][0]']              \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 64, 64, 128)  65664       ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " add_9 (Add)                    (None, 64, 64, 128)  0           ['conv2d_transpose_2[0][0]',     \n",
            "                                                                  'conv2d_32[0][0]']              \n",
            "                                                                                                  \n",
            " activation_21 (Activation)     (None, 64, 64, 128)  0           ['add_9[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 64, 64, 1)    129         ['activation_21[0][0]']          \n",
            "                                                                                                  \n",
            " activation_22 (Activation)     (None, 64, 64, 1)    0           ['conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " up_sampling2d_4 (UpSampling2D)  (None, 128, 128, 1)  0          ['activation_22[0][0]']          \n",
            "                                                                                                  \n",
            " lambda_2 (Lambda)              (None, 128, 128, 12  0           ['up_sampling2d_4[0][0]']        \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " multiply_2 (Multiply)          (None, 128, 128, 12  0           ['lambda_2[0][0]',               \n",
            "                                8)                                'activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 128, 128, 12  16512       ['multiply_2[0][0]']             \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " up_sampling2d_5 (UpSampling2D)  (None, 128, 128, 25  0          ['activation_19[0][0]']          \n",
            "                                6)                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 128, 128, 12  512        ['conv2d_35[0][0]']              \n",
            " ormalization)                  8)                                                                \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 128, 128, 38  0           ['up_sampling2d_5[0][0]',        \n",
            "                                4)                                'batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 128, 128, 12  442496      ['concatenate_2[0][0]']          \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 128, 128, 12  512        ['conv2d_36[0][0]']              \n",
            " ormalization)                  8)                                                                \n",
            "                                                                                                  \n",
            " activation_23 (Activation)     (None, 128, 128, 12  0           ['batch_normalization_27[0][0]'] \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 128, 128, 12  147584      ['activation_23[0][0]']          \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 128, 128, 12  49280       ['concatenate_2[0][0]']          \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 128, 128, 12  512        ['conv2d_37[0][0]']              \n",
            " ormalization)                  8)                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 128, 128, 12  512        ['conv2d_38[0][0]']              \n",
            " ormalization)                  8)                                                                \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 128, 128, 12  0           ['batch_normalization_28[0][0]'] \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " add_10 (Add)                   (None, 128, 128, 12  0           ['batch_normalization_29[0][0]', \n",
            "                                8)                                'dropout_7[0][0]']              \n",
            "                                                                                                  \n",
            " activation_24 (Activation)     (None, 128, 128, 12  0           ['add_10[0][0]']                 \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 128, 128, 64  8256        ['activation_24[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 128, 128, 64  256        ['conv2d_39[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_25 (Activation)     (None, 128, 128, 64  0           ['batch_normalization_30[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 128, 128, 64  4160        ['activation_25[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_transpose_3 (Conv2DTran  (None, 128, 128, 64  36928      ['conv2d_41[0][0]']              \n",
            " spose)                         )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 128, 128, 64  16448       ['activation_1[0][0]']           \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " add_11 (Add)                   (None, 128, 128, 64  0           ['conv2d_transpose_3[0][0]',     \n",
            "                                )                                 'conv2d_40[0][0]']              \n",
            "                                                                                                  \n",
            " activation_26 (Activation)     (None, 128, 128, 64  0           ['add_11[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 128, 128, 1)  65          ['activation_26[0][0]']          \n",
            "                                                                                                  \n",
            " activation_27 (Activation)     (None, 128, 128, 1)  0           ['conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " up_sampling2d_6 (UpSampling2D)  (None, 256, 256, 1)  0          ['activation_27[0][0]']          \n",
            "                                                                                                  \n",
            " lambda_3 (Lambda)              (None, 256, 256, 64  0           ['up_sampling2d_6[0][0]']        \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " multiply_3 (Multiply)          (None, 256, 256, 64  0           ['lambda_3[0][0]',               \n",
            "                                )                                 'activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 256, 256, 64  4160        ['multiply_3[0][0]']             \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " up_sampling2d_7 (UpSampling2D)  (None, 256, 256, 12  0          ['activation_24[0][0]']          \n",
            "                                8)                                                                \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 256, 256, 64  256        ['conv2d_43[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 256, 256, 19  0           ['up_sampling2d_7[0][0]',        \n",
            "                                2)                                'batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 256, 256, 64  110656      ['concatenate_3[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 256, 256, 64  256        ['conv2d_44[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " activation_28 (Activation)     (None, 256, 256, 64  0           ['batch_normalization_32[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 256, 256, 64  36928       ['activation_28[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 256, 256, 64  12352       ['concatenate_3[0][0]']          \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 256, 256, 64  256        ['conv2d_45[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 256, 256, 64  256        ['conv2d_46[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 256, 256, 64  0           ['batch_normalization_33[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " add_12 (Add)                   (None, 256, 256, 64  0           ['batch_normalization_34[0][0]', \n",
            "                                )                                 'dropout_8[0][0]']              \n",
            "                                                                                                  \n",
            " activation_29 (Activation)     (None, 256, 256, 64  0           ['add_12[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 256, 256, 1)  65          ['activation_29[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 256, 256, 1)  4          ['conv2d_47[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_30 (Activation)     (None, 256, 256, 1)  0           ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 39,090,377\n",
            "Trainable params: 39,068,871\n",
            "Non-trainable params: 21,506\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "input_shape = (IMG_WIDTH, IMG_HEIGHT, IMG_CHANNELS)\n",
        "model = Attention_ResUNet(input_shape, NUM_CLASSES=1, dropout_rate=0.1, batch_norm=True)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjW6h0txcs66"
      },
      "source": [
        "### Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fP5D0Kelcs66"
      },
      "outputs": [],
      "source": [
        "# Optimizer\n",
        "from keras import optimizers\n",
        "sgd = optimizers.SGD(learning_rate=0.002, decay=0.00003, momentum=0.9)\n",
        "adam = optimizers.Adam(learning_rate=0.002, decay=0.00003)\n",
        "\n",
        "# Loss function\n",
        "from keras.losses import BinaryCrossentropy\n",
        "bce = BinaryCrossentropy()\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return -dice_coef(y_true, y_pred)\n",
        "\n",
        "def jacard_coef_loss(y_true, y_pred):\n",
        "    return -jacard_coef(y_true, y_pred)\n",
        "\n",
        "# Metrics\n",
        "from keras.metrics import Recall, Precision\n",
        "recall = Recall()\n",
        "precision = Precision()\n",
        "\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2.0 * intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1.0)\n",
        "\n",
        "def jacard_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
        "\n",
        "# Fit model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
        "earlystopper = EarlyStopping(patience=5, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(factor=0.1, patience=4, min_lr=0.00001, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw1SxrWScs66"
      },
      "source": [
        "### Adam, BCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asqkzmULygdN",
        "outputId": "e7a78246-8eef-4721-ab37-eff1f20bf75e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.5484 - recall: 0.9280 - precision: 0.5235\n",
            "Epoch 1: val_loss improved from inf to 1.38251, saving model to weights/ra_unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 258s 2s/step - loss: 0.5484 - recall: 0.9280 - precision: 0.5235 - val_loss: 1.3825 - val_recall: 0.9957 - val_precision: 0.1495 - lr: 0.0020\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - ETA: 0s - loss: 0.4672 - recall: 0.9123 - precision: 0.5836\n",
            "Epoch 2: val_loss improved from 1.38251 to 0.51000, saving model to weights/ra_unet_64_adam_bce_8.h5\n",
            "100/100 [==============================] - 258s 3s/step - loss: 0.4672 - recall: 0.9123 - precision: 0.5836 - val_loss: 0.5100 - val_recall: 0.6983 - val_precision: 0.5474 - lr: 0.0020\n",
            "Epoch 3/100\n"
          ]
        }
      ],
      "source": [
        "# Adam, BCE, batch_size=8\n",
        "logs_p = 'logs/ra_unet_64_adam_bce_8.csv'\n",
        "weights_p = 'weights/ra_unet_64_adam_bce_8.h5'\n",
        "\n",
        "checkpointer = ModelCheckpoint(weights_p, verbose=1, save_best_only=True)\n",
        "callbacks_f = [earlystopper, checkpointer, reduce_lr, CSVLogger(logs_p, append=True, separator=';')]\n",
        "optimizer_f = adam\n",
        "loss_f = [bce]\n",
        "metrics_f = [recall, precision,]\n",
        "#batch_s = 8 * tpu_strategy.num_replicas_in_sync # with TPU\n",
        "batch_s = 8\n",
        "epochs_s = 100\n",
        "\n",
        "#with tpu_strategy.scope():  # with TPU\n",
        "#    model = build_inception_resnetv2_unet(input=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),)\n",
        "model = Attention_ResUNet(input_shape, NUM_CLASSES=1, dropout_rate=0.1, batch_norm=True)\n",
        "model.compile(optimizer=optimizer_f, loss=loss_f, metrics=metrics_f)\n",
        "\n",
        "results = model.fit(x=X_train, y=Y_train, validation_data=(X_val, Y_val), batch_size=batch_s, \n",
        "                    epochs=epochs_s, callbacks=callbacks_f, verbose=1,)\n",
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZf_ZizGcs66"
      },
      "source": [
        "### SGD, BCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "id": "saritjoNcs67",
        "outputId": "55e14dec-353d-45a0-9e47-186619a8e0fb"
      },
      "outputs": [],
      "source": [
        "# Adam, BCE, batch_size=8\n",
        "logs_p = 'logs/ra_unet_64_sgd_bce_8.csv'\n",
        "weights_p = 'weights/ra_unet_64_sgd_bce_8.h5'\n",
        "\n",
        "checkpointer = ModelCheckpoint(weights_p, verbose=1, save_best_only=True)\n",
        "callbacks_f = [earlystopper, checkpointer, reduce_lr, CSVLogger(logs_p, append=True, separator=';')]\n",
        "optimizer_f = sgd\n",
        "loss_f = [bce]\n",
        "metrics_f = [recall, precision,]\n",
        "#batch_s = 8 * tpu_strategy.num_replicas_in_sync # with TPU\n",
        "batch_s = 8\n",
        "epochs_s = 100\n",
        "\n",
        "#with tpu_strategy.scope():  # with TPU\n",
        "#    model = build_inception_resnetv2_unet(input=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),)\n",
        "model = Attention_ResUNet(input_shape, NUM_CLASSES=1, dropout_rate=0.1, batch_norm=True)\n",
        "model.compile(optimizer=optimizer_f, loss=loss_f, metrics=metrics_f)\n",
        "\n",
        "results = model.fit(x=X_train, y=Y_train, validation_data=(X_val, Y_val), batch_size=batch_s, \n",
        "                    epochs=epochs_s, callbacks=callbacks_f, verbose=1,)\n",
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmwVRU2Fcs67"
      },
      "source": [
        "### Adam, DICE + Jaccard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWSXe5N2cs67"
      },
      "outputs": [],
      "source": [
        "# Adam, Dice+Jacard, batch_size=8\n",
        "logs_p = 'logs/ra_unet_64_adam_dice+jacard_8.csv'\n",
        "weights_p = 'weights/ra_unet_64_adam_dice+jacard_8.h5'\n",
        "\n",
        "checkpointer = ModelCheckpoint(weights_p, verbose=1, save_best_only=True)\n",
        "callbacks_f = [earlystopper, checkpointer, reduce_lr, CSVLogger(logs_p, append=True, separator=';')]\n",
        "optimizer_f = adam\n",
        "loss_f = [dice_coef_loss, jacard_coef_loss]\n",
        "metrics_f = [dice_coef, jacard_coef,]\n",
        "#batch_s = 8 * tpu_strategy.num_replicas_in_sync # with TPU\n",
        "batch_s = 8\n",
        "epochs_s = 100\n",
        "\n",
        "#with tpu_strategy.scope():  # with TPU\n",
        "#    model = build_inception_resnetv2_unet(input=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),)\n",
        "model = Attention_ResUNet(input_shape, NUM_CLASSES=1, dropout_rate=0.1, batch_norm=True)\n",
        "model.compile(optimizer=optimizer_f, loss=loss_f, metrics=metrics_f)\n",
        "\n",
        "results = model.fit(x=X_train, y=Y_train.astype(np.float32), validation_data=(X_val, Y_val.astype(np.float32)), batch_size=batch_s, \n",
        "                    epochs=epochs_s, callbacks=callbacks_f, verbose=1,)\n",
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjRvT-tlcs67"
      },
      "source": [
        "### SGD, DICE + Jaccard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyGmNF6Bcs67"
      },
      "outputs": [],
      "source": [
        "# Adam, Dice+Jacard, batch_size=8\n",
        "logs_p = 'logs/ra_unet_64_sgd_dice+jacard_8.csv'\n",
        "weights_p = 'weights/ra_unet_64_sgd_dice+jacard_8.h5'\n",
        "\n",
        "checkpointer = ModelCheckpoint(weights_p, verbose=1, save_best_only=True)\n",
        "callbacks_f = [earlystopper, checkpointer, reduce_lr, CSVLogger(logs_p, append=True, separator=';')]\n",
        "optimizer_f = sgd\n",
        "loss_f = [dice_coef_loss, jacard_coef_loss]\n",
        "metrics_f = [dice_coef, jacard_coef,]\n",
        "#batch_s = 8 * tpu_strategy.num_replicas_in_sync # with TPU\n",
        "batch_s = 8\n",
        "epochs_s = 100\n",
        "\n",
        "#with tpu_strategy.scope():  # with TPU\n",
        "#    model = build_inception_resnetv2_unet(input=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),)\n",
        "model = Attention_ResUNet(input_shape, NUM_CLASSES=1, dropout_rate=0.1, batch_norm=True)\n",
        "model.compile(optimizer=optimizer_f, loss=loss_f, metrics=metrics_f)\n",
        "\n",
        "results = model.fit(x=X_train, y=Y_train.astype(np.float32), validation_data=(X_val, Y_val.astype(np.float32)), batch_size=batch_s, \n",
        "                    epochs=epochs_s, callbacks=callbacks_f, verbose=1,)\n",
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDoc2ZYTa4ZG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
